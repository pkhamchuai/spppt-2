{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import cv2\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "from skimage.metrics import structural_similarity as ssim\n",
    "\n",
    "# Suppress the specific warning\n",
    "import csv\n",
    "from datetime import datetime\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torch\n",
    "from torchvision import transforms\n",
    "from torch import nn, optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils import data\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.transforms import ToTensor\n",
    "\n",
    "from utils.utils0 import *\n",
    "from utils.utils1 import *\n",
    "from utils.utils1 import ModelParams, DL_affine_plot, loss_extra\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f'Device: {device}')\n",
    "\n",
    "# Stub to warn about opencv version.\n",
    "if int(cv2.__version__[0]) < 3: # pragma: no cover\n",
    "  print('Warning: OpenCV 3 is not installed')\n",
    "\n",
    "image_size = 256\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cases, model parameters\n",
    "- Supervised DL w/ groundtruth affine transformation parameters (MSE params, MSE, NCC images)\n",
    "    - Synthetic eye\n",
    "    - Synthetic shape\n",
    "- Unsupervised DL (MSE, NCC images)\n",
    "    - Actual eye data\n",
    "    - Synthetic eye\n",
    "    - Synthetic shape\n",
    "- Data\n",
    "    - only images\n",
    "    - only heatmaps\n",
    "    - images & heatmaps\n",
    "- Loss function\n",
    "    - MSE affine parameters\n",
    "    - MSE, NCC images\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model name:  dataset1_sup1_image1_heatmaps0_loss_image0\n",
      "Model code:  11100_0.001_0_10_1\n",
      "Model params:  {'dataset': 1, 'sup': 1, 'image': 1, 'heatmaps': 0, 'loss_image_case': 0, 'loss_image': MSELoss(), 'loss_affine': <utils.utils1.loss_affine object at 0x7f4c44e512b0>, 'learning_rate': 0.001, 'decay_rate': 0.96, 'start_epoch': 0, 'num_epochs': 10, 'batch_size': 1, 'model_name': 'dataset1_sup1_image1_heatmaps0_loss_image0'}\n",
      "\n",
      "Model name:  dataset1_sup1_image1_heatmaps0_loss_image0\n",
      "Model code:  11100_0.001_0_10_1\n",
      "Dataset used:  Synthetic eye\n",
      "Supervised or unsupervised model:  Supervised\n",
      "Image type:  Image used\n",
      "Heatmaps used:  Heatmaps not used\n",
      "Loss function case:  0\n",
      "Loss function for image:  MSELoss()\n",
      "Loss function for affine:  <utils.utils1.loss_affine object at 0x7f4c44e512b0>\n",
      "Learning rate:  0.001\n",
      "Decay rate:  0.96\n",
      "Start epoch:  0\n",
      "Number of epochs:  10\n",
      "Batch size:  1\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model_params = ModelParams(sup=1, dataset=1, image=1, heatmaps=0, \n",
    "                           loss_image=0, num_epochs=10, learning_rate=1e-3)\n",
    "model_params.print_explanation()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Models\n",
    "## SuperPoint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ImgReg Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from networks import affine_network_simple as an\n",
    "\n",
    "class SP_DHR_Net(nn.Module):\n",
    "    def __init__(self, model_params):\n",
    "        super(SP_AffineNet, self).__init__()\n",
    "        self.superpoint = SuperPointFrontend('utils/superpoint_v1.pth', nms_dist=4,\n",
    "                          conf_thresh=0.015, nn_thresh=0.7, cuda=True)\n",
    "        self.affineNet = an.load_network(device)\n",
    "        self.nn_thresh = 0.7\n",
    "        self.model_params = model_params\n",
    "        print(\"\\nRunning new version (not run SP on source image)\")\n",
    "\n",
    "    def forward(self, source_image, target_image):\n",
    "        # source_image = source_image.to(device)\n",
    "        # target_image = target_image.to(device)\n",
    "\n",
    "        # print('source_image: ', source_image.shape)\n",
    "        # print('target_image: ', target_image.shape)\n",
    "        points1, desc1, heatmap1 = self.superpoint(source_image[0, 0, :, :].cpu().numpy())\n",
    "        points2, desc2, heatmap2 = self.superpoint(target_image[0, 0, :, :].cpu().numpy())\n",
    "\n",
    "        if self.model_params.heatmaps == 0:\n",
    "            affine_params = self.affineNet(source_image, target_image)\n",
    "        elif self.model_params.heatmaps == 1:\n",
    "            print(\"This part is not yet implemented.\")\n",
    "            # affine_params = self.affineNet(source_image, target_image, heatmap1, heatmap2)\n",
    "\n",
    "        # transform the source image using the affine parameters\n",
    "        # using F.affine_grid and F.grid_sample\n",
    "        transformed_source_affine = tensor_affine_transform(source_image, affine_params)\n",
    "        points1_2, desc1_2, heatmap1_2 = self.superpoint(transformed_source_affine[0, 0, :, :].detach().cpu().numpy())\n",
    "\n",
    "        # match the points between the two images\n",
    "        tracker = PointTracker(5, nn_thresh=0.7)\n",
    "        try:\n",
    "            matches = tracker.nn_match_two_way(desc1, desc2, nn_thresh=self.nn_thresh)\n",
    "        except:\n",
    "            # print('No matches found')\n",
    "            # TODO: find a better way to do this\n",
    "            try:\n",
    "                while matches.shape[1] < 3 and self.nn_thresh > 0.1:\n",
    "                    self.nn_thresh = self.nn_thresh - 0.1\n",
    "                    matches = tracker.nn_match_two_way(desc1, desc2, nn_thresh=self.nn_thresh)\n",
    "            except:\n",
    "                return transformed_source_affine, affine_params, [], [], [], [], [], [], []\n",
    "\n",
    "        # take the elements from points1 and points2 using the matches as indices\n",
    "        matches1 = np.array(points1[:2, matches[0, :].astype(int)])\n",
    "        matches2 = np.array(points2[:2, matches[1, :].astype(int)])\n",
    "        # matches1_2 = np.array(points1_2[:2, matches[0, :].astype(int)])\n",
    "        # print('matches1', matches1)\n",
    "        # print('matches2', matches2)\n",
    "        # print('matches1_2', matches1_2)\n",
    "\n",
    "        # try:\n",
    "        #     matches1_2 = points1_2[:2, matches[0, :].astype(int)]\n",
    "        # except:\n",
    "        # print(affine_params.cpu().detach().shape, transformed_source_affine.shape)\n",
    "        matches1_2 = transform_points_DVF(torch.tensor(matches1), \n",
    "                        affine_params.cpu().detach(), transformed_source_affine)\n",
    "\n",
    "        # transform the points using the affine parameters\n",
    "        # matches1_transformed = transform_points(matches1.T[None, :, :], affine_params.cpu().detach())\n",
    "        return transformed_source_affine, affine_params, matches1, matches2, matches1_2, \\\n",
    "            desc1_2, desc2, heatmap1_2, heatmap2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SP ImgReg model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.datagen import datagen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test datagen for all datasets and training and testing\n",
    "# for dataset in range(4): # don't forget to change this back to 2\n",
    "#     for is_train in [True, False]:\n",
    "#         for sup in [False, True]:\n",
    "#             print(f'dataset: {dataset}, is_train: {is_train}, sup: {sup}')\n",
    "#             dataloader = datagen(dataset, is_train, sup)\n",
    "            \n",
    "#             if sup==1 and dataset==0:\n",
    "#                 print('skipping')\n",
    "#                 pass\n",
    "#             else:\n",
    "#                 try:\n",
    "#                     print('index, source_img.shape,       target_img.shape')\n",
    "#                     for i, (source_img, target_img) in enumerate(dataloader):\n",
    "#                         print(i, source_img.shape, target_img.shape)\n",
    "#                         if i == 2:\n",
    "#                             break\n",
    "#                 except ValueError:\n",
    "#                     print('index, source_img.shape,       target_img.shape,            affine_params.shape')\n",
    "#                     for i, batch in enumerate(dataloader):\n",
    "#                         print(i, batch[0].shape, batch[1].shape, batch[2].shape)\n",
    "#                         if i == 5:\n",
    "#                             break\n",
    "#             print('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training\n",
    "## Dataset initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set:  [torch.Size([1, 1, 256, 256]), torch.Size([1, 1, 256, 256]), torch.Size([1, 2, 3])]\n",
      "Test set:  [torch.Size([1, 1, 256, 256]), torch.Size([1, 1, 256, 256]), torch.Size([1, 2, 3])]\n"
     ]
    }
   ],
   "source": [
    "train_dataset = datagen(model_params.dataset, True, model_params.sup)\n",
    "test_dataset = datagen(model_params.dataset, False, model_params.sup)\n",
    "\n",
    "# Get sample batch\n",
    "print('Train set: ', [x.shape for x in next(iter(train_dataset))])\n",
    "print('Test set: ', [x.shape for x in next(iter(test_dataset))])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model initialize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset1_sup1_image1_heatmaps0_loss_image0\n",
      "\n",
      "Model name:  dataset1_sup1_image1_heatmaps0_loss_image0\n",
      "Model code:  11100_0.001_0_10_1\n",
      "Dataset used:  Synthetic eye\n",
      "Supervised or unsupervised model:  Supervised\n",
      "Image type:  Image used\n",
      "Heatmaps used:  Heatmaps not used\n",
      "Loss function case:  0\n",
      "Loss function for image:  MSELoss()\n",
      "Loss function for affine:  <utils.utils1.loss_affine object at 0x7f4c44e512b0>\n",
      "Learning rate:  0.001\n",
      "Decay rate:  0.96\n",
      "Start epoch:  0\n",
      "Number of epochs:  10\n",
      "Batch size:  1\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# print case\n",
    "print(model_params)\n",
    "model_params.print_explanation()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'SP_AffineNet' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/home/pkhamchuai/codes/EyeImgReg-check-training/DL_template-DHR.ipynb Cell 16\u001b[0m line \u001b[0;36m1\n\u001b[0;32m----> <a href='vscode-notebook-cell://wsl%2Bubuntu/home/pkhamchuai/codes/EyeImgReg-check-training/DL_template-DHR.ipynb#X21sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m model \u001b[39m=\u001b[39m SP_DHR_Net(model_params)\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu/home/pkhamchuai/codes/EyeImgReg-check-training/DL_template-DHR.ipynb#X21sdnNjb2RlLXJlbW90ZQ%3D%3D?line=1'>2</a>\u001b[0m \u001b[39mprint\u001b[39m(model)\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu/home/pkhamchuai/codes/EyeImgReg-check-training/DL_template-DHR.ipynb#X21sdnNjb2RlLXJlbW90ZQ%3D%3D?line=3'>4</a>\u001b[0m parameters \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39mparameters()\n",
      "\u001b[1;32m/home/pkhamchuai/codes/EyeImgReg-check-training/DL_template-DHR.ipynb Cell 16\u001b[0m line \u001b[0;36m5\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu/home/pkhamchuai/codes/EyeImgReg-check-training/DL_template-DHR.ipynb#X21sdnNjb2RlLXJlbW90ZQ%3D%3D?line=3'>4</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\u001b[39mself\u001b[39m, model_params):\n\u001b[0;32m----> <a href='vscode-notebook-cell://wsl%2Bubuntu/home/pkhamchuai/codes/EyeImgReg-check-training/DL_template-DHR.ipynb#X21sdnNjb2RlLXJlbW90ZQ%3D%3D?line=4'>5</a>\u001b[0m     \u001b[39msuper\u001b[39m(SP_AffineNet, \u001b[39mself\u001b[39m)\u001b[39m.\u001b[39m\u001b[39m__init__\u001b[39m()\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu/home/pkhamchuai/codes/EyeImgReg-check-training/DL_template-DHR.ipynb#X21sdnNjb2RlLXJlbW90ZQ%3D%3D?line=5'>6</a>\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msuperpoint \u001b[39m=\u001b[39m SuperPointFrontend(\u001b[39m'\u001b[39m\u001b[39mutils/superpoint_v1.pth\u001b[39m\u001b[39m'\u001b[39m, nms_dist\u001b[39m=\u001b[39m\u001b[39m4\u001b[39m,\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu/home/pkhamchuai/codes/EyeImgReg-check-training/DL_template-DHR.ipynb#X21sdnNjb2RlLXJlbW90ZQ%3D%3D?line=6'>7</a>\u001b[0m                       conf_thresh\u001b[39m=\u001b[39m\u001b[39m0.015\u001b[39m, nn_thresh\u001b[39m=\u001b[39m\u001b[39m0.7\u001b[39m, cuda\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu/home/pkhamchuai/codes/EyeImgReg-check-training/DL_template-DHR.ipynb#X21sdnNjb2RlLXJlbW90ZQ%3D%3D?line=7'>8</a>\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39maffineNet \u001b[39m=\u001b[39m an\u001b[39m.\u001b[39mload_network(device)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'SP_AffineNet' is not defined"
     ]
    }
   ],
   "source": [
    "model = SP_DHR_Net(model_params)\n",
    "print(model)\n",
    "\n",
    "parameters = model.parameters()\n",
    "optimizer = optim.Adam(parameters, model_params.learning_rate, weight_decay=0.0001)\n",
    "scheduler = optim.lr_scheduler.LambdaLR(optimizer, lambda epoch: model_params.decay_rate ** epoch)\n",
    "# model_path = 'trained_models/10102_0.001_0_20_1_20230930-091532.pth'\n",
    "\n",
    "# if a model is loaded, the training will continue from the epoch it was saved at\n",
    "try:\n",
    "    model.load_state_dict(torch.load(model_path))\n",
    "    model_params.start_epoch = int(model_path.split('/')[-1].split('_')[3])\n",
    "    print(f'Loaded model from {model_path}\\nstarting at epoch {model_params.start_epoch}')\n",
    "    if model_params.start_epoch >= model_params.num_epochs:\n",
    "            model_params.num_epochs += model_params.start_epoch\n",
    "except:\n",
    "    model_params.start_epoch = 0\n",
    "    print('No model loaded, starting from scratch')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define training function\n",
    "def train(model, model_params, timestamp):\n",
    "    # Define loss function based on supervised or unsupervised learning\n",
    "    criterion = model_params.loss_image\n",
    "    extra = loss_extra()\n",
    "\n",
    "    if model_params.sup:\n",
    "        criterion_affine = nn.MSELoss()\n",
    "        # TODO: add loss for points1_affine and points2, Euclidean distance\n",
    "\n",
    "    # Define optimizer\n",
    "    optimizer = optim.Adam(model.parameters(), lr=model_params.learning_rate)\n",
    "\n",
    "    # Create empty list to store epoch number, train loss and validation loss\n",
    "    epoch_loss_list = []\n",
    "    running_loss_list = []\n",
    "    \n",
    "    # Create output directory\n",
    "    output_dir = f\"output/{model_params.get_model_code()}_{timestamp}\"\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    # Train model\n",
    "    for epoch in range(model_params.start_epoch, model_params.num_epochs):\n",
    "        # Set model to training mode\n",
    "        model.train()\n",
    "\n",
    "        # Zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        running_loss = 0.0\n",
    "        train_bar = tqdm(train_dataset, desc=f'Training Epoch {epoch+1}/{model_params.num_epochs}')\n",
    "        for i, data in enumerate(train_bar):\n",
    "            # Get images and affine parameters\n",
    "            if model_params.sup:\n",
    "                source_image, target_image, affine_params_true = data\n",
    "            else:\n",
    "                source_image, target_image = data\n",
    "                affine_params_true = None\n",
    "            source_image = source_image.to(device)\n",
    "            target_image = target_image.to(device)\n",
    "\n",
    "            # Forward + backward + optimize\n",
    "            outputs = model(source_image, target_image)\n",
    "            # for i in range(len(outputs)):\n",
    "            #         print(i, outputs[i].shape)\n",
    "            # 0 torch.Size([1, 1, 256, 256])\n",
    "            # 1 torch.Size([1, 2, 3])\n",
    "            # 2 (2, 4)\n",
    "            # 3 (2, 4)\n",
    "            # 4 (1, 4, 2)\n",
    "            # 5 (256, 9)\n",
    "            # 6 (256, 16)\n",
    "            # 7 (256, 256)\n",
    "            # 8 (256, 256)\n",
    "            transformed_source_affine = outputs[0] # image\n",
    "            affine_params_predicted = outputs[1] # affine parameters\n",
    "            points1 = outputs[2]\n",
    "            points2 = outputs[3]\n",
    "            points1_affine = np.array(outputs[4])\n",
    "\n",
    "            # print(f\"affine_params_true: {affine_params_true}\")\n",
    "            # print(f\"affine_params_predicted: {affine_params_predicted}\\n\")\n",
    "\n",
    "            try:\n",
    "                points1_affine = points1_affine.reshape(points1_affine.shape[2], points1_affine.shape[1])\n",
    "            except:\n",
    "                pass\n",
    "            desc1 = outputs[5]\n",
    "            desc2 = outputs[6]\n",
    "            heatmap1 = outputs[7]\n",
    "            heatmap2 = outputs[8]\n",
    "\n",
    "            loss = criterion(transformed_source_affine, target_image)\n",
    "            loss += extra(affine_params_predicted)\n",
    "            if model_params.sup:\n",
    "                loss_affine = criterion_affine(affine_params_true.view(1, 2, 3), affine_params_predicted.cpu())\n",
    "                # TODO: add loss for points1_affine and points2, Euclidean distance\n",
    "                # loss_points = criterion_points(points1_affine, points2)\n",
    "                loss += loss_affine\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "            \n",
    "            # Plot images if i < 5\n",
    "            if i < 5:\n",
    "                DL_affine_plot(f\"epoch{epoch+1}_train\", output_dir,\n",
    "                    f\"{i}\", \"_\", source_image[0, 0, :, :].detach().cpu().numpy(), \n",
    "                    target_image[0, 0, :, :].detach().cpu().numpy(), \n",
    "                    transformed_source_affine[0, 0, :, :].detach().cpu().numpy(),\n",
    "                    points1, points2, points1_affine, desc1, desc2, affine_params_true=affine_params_true,\n",
    "                    affine_params_predict=affine_params_predicted, heatmap1=heatmap1, heatmap2=heatmap2, plot=True)\n",
    "\n",
    "            # Print statistics\n",
    "            running_loss += loss.item()\n",
    "            running_loss_list.append([epoch+((i+1)/len(train_dataset)), loss.item()])\n",
    "            train_bar.set_postfix({'loss': running_loss / (i+1)})\n",
    "        print(f'Training Epoch {epoch+1}/{model_params.num_epochs} loss: {running_loss / len(train_dataset)}')\n",
    "\n",
    "        # Validate model\n",
    "        validation_loss = 0.0\n",
    "        model.eval()\n",
    "        # with torch.no_grad():\n",
    "        for i, data in enumerate(test_dataset, 0):\n",
    "            # Get images and affine parameters\n",
    "            if model_params.sup:\n",
    "                source_image, target_image, affine_params_true = data\n",
    "            else:\n",
    "                source_image, target_image = data\n",
    "                affine_params_true = None\n",
    "            source_image = source_image.to(device)\n",
    "            target_image = target_image.to(device)\n",
    "\n",
    "            # Forward pass\n",
    "            outputs = model(source_image, target_image)\n",
    "            # for i in range(len(outputs)):\n",
    "            #     print(i, outputs[i].shape)\n",
    "            transformed_source_affine = outputs[0]\n",
    "            affine_params_predicted = outputs[1]\n",
    "            points1 = outputs[2]\n",
    "            points2 = outputs[3]\n",
    "            points1_affine = np.array(outputs[4])\n",
    "            try:\n",
    "                points1_affine = points1_affine.reshape(points1_affine.shape[2], points1_affine.shape[1])\n",
    "            except:\n",
    "                pass\n",
    "            desc1 = outputs[5]\n",
    "            desc2 = outputs[6]\n",
    "            heatmap1 = outputs[7]\n",
    "            heatmap2 = outputs[8]\n",
    "\n",
    "            loss = criterion(transformed_source_affine, target_image)\n",
    "            loss += extra(affine_params_predicted)\n",
    "            if model_params.sup:\n",
    "                loss_affine = criterion_affine(affine_params_true.view(1, 2, 3), affine_params_predicted.cpu())\n",
    "                # TODO: add loss for points1_affine and points2, Euclidean distance\n",
    "                # loss_points = criterion_points(points1_affine, points2)\n",
    "                loss += loss_affine\n",
    "\n",
    "            # Add to validation loss\n",
    "            validation_loss += loss.item()\n",
    "\n",
    "            # Plot images if i < 5\n",
    "            if i < 5:\n",
    "                DL_affine_plot(f\"epoch{epoch+1}_valid\", output_dir,\n",
    "                    f\"{i}\", \"_\", source_image[0, 0, :, :].detach().cpu().numpy(), \n",
    "                    target_image[0, 0, :, :].detach().cpu().numpy(), \n",
    "                    transformed_source_affine[0, 0, :, :].detach().cpu().numpy(),\n",
    "                    points1, points2, points1_affine, desc1, desc2, affine_params_true=affine_params_true,\n",
    "                    affine_params_predict=affine_params_predicted, heatmap1=heatmap1, heatmap2=heatmap2, plot=True)\n",
    "\n",
    "        # Print validation statistics\n",
    "        validation_loss /= len(test_dataset)\n",
    "        print(f'Validation Epoch {epoch+1}/{model_params.num_epochs} loss: {validation_loss}')\n",
    "\n",
    "        # Append epoch number, train loss and validation loss to epoch_loss_list\n",
    "        epoch_loss_list.append([epoch+1, running_loss / len(train_dataset), validation_loss])\n",
    "\n",
    "        \n",
    "        # Extract epoch number, train loss and validation loss from epoch_loss_list\n",
    "        epoch = [x[0] for x in epoch_loss_list]\n",
    "        train_loss = [x[1] for x in epoch_loss_list]\n",
    "        val_loss = [x[2] for x in epoch_loss_list]\n",
    "        step = [x[0] for x in running_loss_list]\n",
    "        running_train_loss = [x[1] for x in running_loss_list]\n",
    "\n",
    "        save_plot_name = f\"{output_dir}/loss_{model_params.get_model_code()}_epoch{model_params.num_epochs}_{timestamp}.png\"\n",
    "\n",
    "        # Plot train loss and validation loss against epoch number\n",
    "        plt.figure()\n",
    "        plt.plot(step, running_train_loss, label='Running Train Loss', alpha=0.3)\n",
    "        plt.plot(epoch, train_loss, label='Train Loss')\n",
    "        plt.plot(epoch, val_loss, label='Validation Loss')\n",
    "        plt.title('Train and Validation Loss')\n",
    "        plt.legend()\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.ylabel('Loss')\n",
    "        plt.yscale('log')\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(save_plot_name)\n",
    "        plt.close()\n",
    "        \n",
    "    print('Finished Training')\n",
    "\n",
    "    # delete all txt files in output_dir\n",
    "    for file in os.listdir(output_dir):\n",
    "        if file.endswith(\".txt\"):\n",
    "            os.remove(os.path.join(output_dir, file))\n",
    "\n",
    "    # Return epoch_loss_list\n",
    "    return epoch_loss_list\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## working code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1/10:   0%|          | 0/360 [00:00<?, ?it/s]/home/pkhamchuai/codes/spppt-1/utils/utils1.py:712: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  torch.tensor(M).view(1, 2, 3))\n",
      "/home/pkhamchuai/miniconda3/envs/spppt/lib/python3.8/site-packages/torch/nn/functional.py:4298: UserWarning: Default grid_sample and affine_grid behavior has changed to align_corners=False since 1.3.0. Please specify align_corners=True if the old behavior is desired. See the documentation of grid_sample for details.\n",
      "  warnings.warn(\n",
      "/home/pkhamchuai/miniconda3/envs/spppt/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:139: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\"Detected call of `lr_scheduler.step()` before `optimizer.step()`. \"\n",
      "Training Epoch 1/10:   1%|          | 2/360 [00:03<08:56,  1.50s/it, loss=0.611]  /home/pkhamchuai/codes/spppt-1/utils/utils1.py:250: RuntimeWarning: invalid value encountered in divide\n",
      "  image = ((image - image.min()) / (image.max() - image.min()) * 255).astype(np.uint8)\n",
      "/home/pkhamchuai/codes/spppt-1/utils/utils1.py:250: RuntimeWarning: invalid value encountered in cast\n",
      "  image = ((image - image.min()) / (image.max() - image.min()) * 255).astype(np.uint8)\n",
      "Training Epoch 1/10: 100%|██████████| 360/360 [00:27<00:00, 13.03it/s, loss=1.16e+7]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch 1/10 loss: 11598290.223731168\n",
      "Validation Epoch 1/10 loss: 239009.353515625\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2/10: 100%|██████████| 360/360 [00:27<00:00, 12.92it/s, loss=2.22e+9]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch 2/10 loss: 2224853848.2828126\n",
      "Validation Epoch 2/10 loss: 32503087.55\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3/10: 100%|██████████| 360/360 [00:27<00:00, 13.24it/s, loss=1.94e+10]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch 3/10 loss: 19364786733.877777\n",
      "Validation Epoch 3/10 loss: 195047966.4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 4/10: 100%|██████████| 360/360 [00:28<00:00, 12.80it/s, loss=1.02e+10]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch 4/10 loss: 10173419981.022223\n",
      "Validation Epoch 4/10 loss: 135035695.2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 5/10:  35%|███▌      | 126/360 [00:13<00:25,  9.31it/s, loss=6.46e+9]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/home/pkhamchuai/codes/spppt-1/DL_template3.ipynb Cell 20\u001b[0m line \u001b[0;36m2\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu/home/pkhamchuai/codes/spppt-1/DL_template3.ipynb#X26sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m timestamp \u001b[39m=\u001b[39m datetime\u001b[39m.\u001b[39mnow()\u001b[39m.\u001b[39mstrftime(\u001b[39m\"\u001b[39m\u001b[39m%\u001b[39m\u001b[39mY\u001b[39m\u001b[39m%\u001b[39m\u001b[39mm\u001b[39m\u001b[39m%d\u001b[39;00m\u001b[39m-\u001b[39m\u001b[39m%\u001b[39m\u001b[39mH\u001b[39m\u001b[39m%\u001b[39m\u001b[39mM\u001b[39m\u001b[39m%\u001b[39m\u001b[39mS\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m----> <a href='vscode-notebook-cell://wsl%2Bubuntu/home/pkhamchuai/codes/spppt-1/DL_template3.ipynb#X26sdnNjb2RlLXJlbW90ZQ%3D%3D?line=1'>2</a>\u001b[0m loss_list \u001b[39m=\u001b[39m train(model, model_params, timestamp)\n",
      "\u001b[1;32m/home/pkhamchuai/codes/spppt-1/DL_template3.ipynb Cell 20\u001b[0m line \u001b[0;36m8\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu/home/pkhamchuai/codes/spppt-1/DL_template3.ipynb#X26sdnNjb2RlLXJlbW90ZQ%3D%3D?line=76'>77</a>\u001b[0m     \u001b[39m# TODO: add loss for points1_affine and points2, Euclidean distance\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu/home/pkhamchuai/codes/spppt-1/DL_template3.ipynb#X26sdnNjb2RlLXJlbW90ZQ%3D%3D?line=77'>78</a>\u001b[0m     \u001b[39m# loss_points = criterion_points(points1_affine, points2)\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu/home/pkhamchuai/codes/spppt-1/DL_template3.ipynb#X26sdnNjb2RlLXJlbW90ZQ%3D%3D?line=78'>79</a>\u001b[0m     loss \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m loss_affine\n\u001b[0;32m---> <a href='vscode-notebook-cell://wsl%2Bubuntu/home/pkhamchuai/codes/spppt-1/DL_template3.ipynb#X26sdnNjb2RlLXJlbW90ZQ%3D%3D?line=79'>80</a>\u001b[0m loss\u001b[39m.\u001b[39;49mbackward()\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu/home/pkhamchuai/codes/spppt-1/DL_template3.ipynb#X26sdnNjb2RlLXJlbW90ZQ%3D%3D?line=80'>81</a>\u001b[0m optimizer\u001b[39m.\u001b[39mstep()\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu/home/pkhamchuai/codes/spppt-1/DL_template3.ipynb#X26sdnNjb2RlLXJlbW90ZQ%3D%3D?line=81'>82</a>\u001b[0m scheduler\u001b[39m.\u001b[39mstep()\n",
      "File \u001b[0;32m~/miniconda3/envs/spppt/lib/python3.8/site-packages/torch/_tensor.py:487\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    477\u001b[0m \u001b[39mif\u001b[39;00m has_torch_function_unary(\u001b[39mself\u001b[39m):\n\u001b[1;32m    478\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    479\u001b[0m         Tensor\u001b[39m.\u001b[39mbackward,\n\u001b[1;32m    480\u001b[0m         (\u001b[39mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    485\u001b[0m         inputs\u001b[39m=\u001b[39minputs,\n\u001b[1;32m    486\u001b[0m     )\n\u001b[0;32m--> 487\u001b[0m torch\u001b[39m.\u001b[39;49mautograd\u001b[39m.\u001b[39;49mbackward(\n\u001b[1;32m    488\u001b[0m     \u001b[39mself\u001b[39;49m, gradient, retain_graph, create_graph, inputs\u001b[39m=\u001b[39;49minputs\n\u001b[1;32m    489\u001b[0m )\n",
      "File \u001b[0;32m~/miniconda3/envs/spppt/lib/python3.8/site-packages/torch/autograd/__init__.py:200\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    195\u001b[0m     retain_graph \u001b[39m=\u001b[39m create_graph\n\u001b[1;32m    197\u001b[0m \u001b[39m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[1;32m    198\u001b[0m \u001b[39m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    199\u001b[0m \u001b[39m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 200\u001b[0m Variable\u001b[39m.\u001b[39;49m_execution_engine\u001b[39m.\u001b[39;49mrun_backward(  \u001b[39m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    201\u001b[0m     tensors, grad_tensors_, retain_graph, create_graph, inputs,\n\u001b[1;32m    202\u001b[0m     allow_unreachable\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, accumulate_grad\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "timestamp = datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "loss_list = train(model, model_params, timestamp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training output:\n",
      "[1, 0.24539990491337246, 0.22000780366361142]\n",
      "[2, 0.24089633435424831, 0.21959682665765284]\n",
      "[3, 0.24925384295897351, 0.21985937096178532]\n",
      "[4, 0.26845063306391237, 0.21944834664463997]\n",
      "[5, 0.2517348497692082, 0.21950501017272472]\n",
      "[6, 0.24793884151925644, 0.21988297253847122]\n",
      "[7, 0.25682318293386036, 0.21949527338147162]\n",
      "[8, 0.24465472961051596, 0.21963326036930084]\n",
      "[9, 0.247489680411915, 0.21973580308258533]\n",
      "[10, 0.23891730540328557, 0.21962998770177364]\n"
     ]
    }
   ],
   "source": [
    "print(\"Training output:\")\n",
    "for i in range(len(loss_list)):\n",
    "    print(loss_list[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model saving"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trained_models/model3_21101_0.01_0_10_1_20231018-225701.pth\n"
     ]
    }
   ],
   "source": [
    "model_save_path = \"trained_models/\"\n",
    "model_name_to_save = model_save_path + f\"model3_{model_params.get_model_code()}_{timestamp}.pth\"\n",
    "print(model_name_to_save)\n",
    "torch.save(model.state_dict(), model_name_to_save)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test model (loading and inference)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save results and export metrics to csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = SPmodel = SP_AffineNet().to(device)\n",
    "# print(model)\n",
    "\n",
    "# parameters = model.parameters()\n",
    "# optimizer = optim.Adam(parameters, model_params.learning_rate)\n",
    "# scheduler = optim.lr_scheduler.LambdaLR(optimizer, lambda epoch: model_params.decay_rate ** epoch)\n",
    "\n",
    "# model.load_state_dict(torch.load(model_name_to_save))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing:: 100%|██████████| 40/40 [00:53<00:00,  1.35s/it]\n"
     ]
    }
   ],
   "source": [
    "def test(model, model_params, timestamp):\n",
    "    # Set model to training mode\n",
    "    model.eval()\n",
    "\n",
    "    # Create output directory\n",
    "    output_dir = f\"output/{model_params.get_model_code()}_{timestamp}_test\"\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    # Validate model\n",
    "    # validation_loss = 0.0\n",
    "\n",
    "    # create a csv file to store the metrics\n",
    "    csv_file = f\"{output_dir}/metrics.csv\"\n",
    "    with open(csv_file, 'w', newline='') as file:\n",
    "        writer = csv.writer(file)\n",
    "        # matches1_transformed.shape[-1], mse_before, mse12, tre_before, tre12, \\\n",
    "        # mse12_image, ssim12_image, \n",
    "        writer.writerow([\"index\", \"mse_before\", \"mse12\", \"tre_before\", \"tre12\", \"mse12_image_before\", \"mse12_image\", \"ssim12_image_before\", \"ssim12_image\"])\n",
    "\n",
    "    with torch.no_grad():\n",
    "        testbar = tqdm(test_dataset, desc=f'Testing:')\n",
    "        for i, data in enumerate(testbar, 0):\n",
    "            # Get images and affine parameters\n",
    "            if model_params.sup:\n",
    "                source_image, target_image, affine_params_true = data\n",
    "            else:\n",
    "                source_image, target_image = data\n",
    "            source_image = source_image.to(device)\n",
    "            target_image = target_image.to(device)\n",
    "\n",
    "            # Forward pass\n",
    "            outputs = model(source_image, target_image)\n",
    "            # for i in range(len(outputs)):\n",
    "            #     print(i, outputs[i].shape)\n",
    "            transformed_source_affine = outputs[0]\n",
    "            affine_params_predicted = outputs[1]\n",
    "            points1 = outputs[2]\n",
    "            points2 = outputs[3]\n",
    "            points1_affine = np.array(outputs[4])\n",
    "            try:\n",
    "                points1_affine = points1_affine.reshape(points1_affine.shape[2], points1_affine.shape[1])\n",
    "            except:\n",
    "                pass\n",
    "            desc1 = outputs[5]\n",
    "            desc2 = outputs[6]\n",
    "            heatmap1 = outputs[7]\n",
    "            heatmap2 = outputs[8]\n",
    "\n",
    "            if i < 50:\n",
    "                plot_ = True\n",
    "            else:\n",
    "                plot_ = False\n",
    "\n",
    "            results = DL_affine_plot(f\"{i+1}\", output_dir,\n",
    "                f\"{i}\", \"_\", source_image[0, 0, :, :].cpu().numpy(), target_image[0, 0, :, :].cpu().numpy(), \\\n",
    "                transformed_source_affine[0, 0, :, :].cpu().numpy(), \\\n",
    "                points1, points2, points1_affine, desc1, desc2, affine_params_true=affine_params_true,\n",
    "                affine_params_predict=affine_params_predicted, heatmap1=heatmap1, heatmap2=heatmap2, plot=plot_)\n",
    "\n",
    "            # calculate metrics\n",
    "            # matches1_transformed = results[0]\n",
    "            mse_before = results[1]\n",
    "            mse12 = results[2]\n",
    "            tre_before = results[3]\n",
    "            tre12 = results[4]\n",
    "            mse12_image_before = results[5]\n",
    "            mse12_image = results[6]\n",
    "            ssim12_image_before = results[7]\n",
    "            ssim12_image = results[8]\n",
    "\n",
    "            # write metrics to csv file\n",
    "            with open(csv_file, 'a', newline='') as file:\n",
    "                writer = csv.writer(file) # TODO: might need to export true & predicted affine parameters too\n",
    "                writer.writerow([i, mse_before, mse12, tre_before, tre12, mse12_image_before, mse12_image, ssim12_image_before, ssim12_image])\n",
    "\n",
    "    # delete all txt files in output_dir\n",
    "    for file in os.listdir(output_dir):\n",
    "        if file.endswith(\".txt\"):\n",
    "            os.remove(os.path.join(output_dir, file))\n",
    "\n",
    "# timestamp = datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "metrics = test(model, model_params, timestamp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spppt",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
