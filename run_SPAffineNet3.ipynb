{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import cv2\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "from skimage.metrics import structural_similarity as ssim\n",
    "\n",
    "# Suppress the specific warning\n",
    "import csv\n",
    "from datetime import datetime\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torch\n",
    "from torchvision import transforms\n",
    "from torch import nn, optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils import data\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.transforms import ToTensor\n",
    "\n",
    "from utils.utils0 import *\n",
    "from utils.utils1 import *\n",
    "from utils.utils1 import ModelParams, DL_affine_plot, loss_extra\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f'Device: {device}')\n",
    "\n",
    "# Stub to warn about opencv version.\n",
    "if int(cv2.__version__[0]) < 3: # pragma: no cover\n",
    "  print('Warning: OpenCV 3 is not installed')\n",
    "\n",
    "image_size = 256\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cases, model parameters\n",
    "- Supervised DL w/ groundtruth affine transformation parameters (MSE params, MSE, NCC images)\n",
    "    - Synthetic eye\n",
    "    - Synthetic shape\n",
    "- Unsupervised DL (MSE, NCC images)\n",
    "    - Actual eye data\n",
    "    - Synthetic eye\n",
    "    - Synthetic shape\n",
    "- Data\n",
    "    - only images\n",
    "    - only heatmaps\n",
    "    - images & heatmaps\n",
    "- Loss function\n",
    "    - MSE affine parameters\n",
    "    - MSE, NCC images\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model name:  dataset1_sup0_image1_heatmaps0_loss_image0\n",
      "Model code:  10100_0.001_0_5_1\n",
      "Model params:  {'dataset': 1, 'sup': 0, 'image': 1, 'heatmaps': 0, 'loss_image_case': 0, 'loss_image': MSELoss(), 'loss_affine': None, 'learning_rate': 0.001, 'decay_rate': 0.96, 'start_epoch': 0, 'num_epochs': 5, 'batch_size': 1, 'model_name': 'dataset1_sup0_image1_heatmaps0_loss_image0'}\n",
      "\n",
      "Model name:  dataset1_sup0_image1_heatmaps0_loss_image0\n",
      "Model code:  10100_0.001_0_5_1\n",
      "Dataset used:  Synthetic eye\n",
      "Supervised or unsupervised model:  Unsupervised\n",
      "Image type:  Image used\n",
      "Heatmaps used:  Heatmaps not used\n",
      "Loss function case:  0\n",
      "Loss function for image:  MSELoss()\n",
      "Loss function for affine:  None\n",
      "Learning rate:  0.001\n",
      "Decay rate:  0.96\n",
      "Start epoch:  0\n",
      "Number of epochs:  5\n",
      "Batch size:  1\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model_params = ModelParams(sup=0, dataset=1, image=1, heatmaps=0, \n",
    "                           loss_image=0, num_epochs=5, learning_rate=1e-3)\n",
    "model_params.print_explanation()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Models\n",
    "## SuperPoint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ImgReg Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.SPaffineNet3 import SP_AffineNet3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SP ImgReg model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.datagen import datagen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test datagen for all datasets and training and testing\n",
    "# for dataset in range(4): # don't forget to change this back to 2\n",
    "#     for is_train in [True, False]:\n",
    "#         for sup in [False, True]:\n",
    "#             print(f'dataset: {dataset}, is_train: {is_train}, sup: {sup}')\n",
    "#             dataloader = datagen(dataset, is_train, sup)\n",
    "            \n",
    "#             if sup==1 and dataset==0:\n",
    "#                 print('skipping')\n",
    "#                 pass\n",
    "#             else:\n",
    "#                 try:\n",
    "#                     print('index, source_img.shape,       target_img.shape')\n",
    "#                     for i, (source_img, target_img) in enumerate(dataloader):\n",
    "#                         print(i, source_img.shape, target_img.shape)\n",
    "#                         if i == 2:\n",
    "#                             break\n",
    "#                 except ValueError:\n",
    "#                     print('index, source_img.shape,       target_img.shape,            affine_params.shape')\n",
    "#                     for i, batch in enumerate(dataloader):\n",
    "#                         print(i, batch[0].shape, batch[1].shape, batch[2].shape)\n",
    "#                         if i == 5:\n",
    "#                             break\n",
    "#             print('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training\n",
    "## Dataset initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set:  [torch.Size([1, 1, 256, 256]), torch.Size([1, 1, 256, 256])]\n",
      "Test set:  [torch.Size([1, 1, 256, 256]), torch.Size([1, 1, 256, 256])]\n"
     ]
    }
   ],
   "source": [
    "train_dataset = datagen(model_params.dataset, True, model_params.sup)\n",
    "test_dataset = datagen(model_params.dataset, False, model_params.sup)\n",
    "\n",
    "# Get sample batch\n",
    "print('Train set: ', [x.shape for x in next(iter(train_dataset))])\n",
    "print('Test set: ', [x.shape for x in next(iter(test_dataset))])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model initialize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset1_sup0_image1_heatmaps0_loss_image0\n",
      "\n",
      "Model name:  dataset1_sup0_image1_heatmaps0_loss_image0\n",
      "Model code:  10100_0.001_0_5_1\n",
      "Dataset used:  Synthetic eye\n",
      "Supervised or unsupervised model:  Unsupervised\n",
      "Image type:  Image used\n",
      "Heatmaps used:  Heatmaps not used\n",
      "Loss function case:  0\n",
      "Loss function for image:  MSELoss()\n",
      "Loss function for affine:  None\n",
      "Learning rate:  0.001\n",
      "Decay rate:  0.96\n",
      "Start epoch:  0\n",
      "Number of epochs:  5\n",
      "Batch size:  1\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# print case\n",
    "print(model_params)\n",
    "model_params.print_explanation()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Running new version (not run SP on source image)\n",
      "-------------------------------------------------------------------------------\n",
      "           Layer (type)            Input Shape         Param #     Tr. Param #\n",
      "===============================================================================\n",
      "               Conv2d-1       [1, 1, 256, 256]             640             640\n",
      "            LeakyReLU-2      [1, 64, 256, 256]               0               0\n",
      "            GroupNorm-3      [1, 64, 256, 256]             128             128\n",
      "               Conv2d-4      [1, 64, 256, 256]          16,448          16,448\n",
      "               Conv2d-5      [1, 64, 128, 128]          73,856          73,856\n",
      "            GroupNorm-6     [1, 128, 128, 128]             256             256\n",
      "               Conv2d-7     [1, 128, 128, 128]          65,664          65,664\n",
      "               Conv2d-8       [1, 128, 64, 64]         295,168         295,168\n",
      "            GroupNorm-9       [1, 256, 64, 64]             512             512\n",
      "              Conv2d-10       [1, 256, 64, 64]         262,400         262,400\n",
      "   AdaptiveAvgPool2d-11       [1, 256, 32, 32]               0               0\n",
      "              Linear-12                  [512]          32,832          32,832\n",
      "             Dropout-13                   [64]               0               0\n",
      "              Linear-14                   [64]             390             390\n",
      "===============================================================================\n",
      "Total params: 748,294\n",
      "Trainable params: 748,294\n",
      "Non-trainable params: 0\n",
      "-------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "===================================== Hierarchical Summary =====================================\n",
      "\n",
      "AffineNet3(\n",
      "  (conv1): Conv2d(1, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)), 640 params\n",
      "  (conv2): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)), 73,856 params\n",
      "  (conv3): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)), 295,168 params\n",
      "  (conv1s): Conv2d(64, 64, kernel_size=(2, 2), stride=(2, 2)), 16,448 params\n",
      "  (conv2s): Conv2d(128, 128, kernel_size=(2, 2), stride=(2, 2)), 65,664 params\n",
      "  (conv3s): Conv2d(256, 256, kernel_size=(2, 2), stride=(2, 2)), 262,400 params\n",
      "  (fc1): Linear(in_features=512, out_features=64, bias=True), 32,832 params\n",
      "  (fc3): Linear(in_features=64, out_features=6, bias=True), 390 params\n",
      "  (dropout): Dropout(p=0.9, inplace=False), 0 params\n",
      "  (aPooling): AdaptiveAvgPool2d(output_size=(1, 1)), 0 params\n",
      "  (ReLU): LeakyReLU(negative_slope=0.01), 0 params\n",
      "  (Act1): GroupNorm(32, 64, eps=1e-05, affine=True), 128 params\n",
      "  (Act2): GroupNorm(64, 128, eps=1e-05, affine=True), 256 params\n",
      "  (Act3): GroupNorm(128, 256, eps=1e-05, affine=True), 512 params\n",
      "), 748,294 params\n",
      "\n",
      "\n",
      "================================================================================================\n",
      "\n",
      "SP_AffineNet3(\n",
      "  (affineNet): AffineNet3(\n",
      "    (conv1): Conv2d(1, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (conv2): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (conv3): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (conv1s): Conv2d(64, 64, kernel_size=(2, 2), stride=(2, 2))\n",
      "    (conv2s): Conv2d(128, 128, kernel_size=(2, 2), stride=(2, 2))\n",
      "    (conv3s): Conv2d(256, 256, kernel_size=(2, 2), stride=(2, 2))\n",
      "    (fc1): Linear(in_features=512, out_features=64, bias=True)\n",
      "    (fc3): Linear(in_features=64, out_features=6, bias=True)\n",
      "    (dropout): Dropout(p=0.9, inplace=False)\n",
      "    (aPooling): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "    (ReLU): LeakyReLU(negative_slope=0.01)\n",
      "    (Act1): GroupNorm(32, 64, eps=1e-05, affine=True)\n",
      "    (Act2): GroupNorm(64, 128, eps=1e-05, affine=True)\n",
      "    (Act3): GroupNorm(128, 256, eps=1e-05, affine=True)\n",
      "  )\n",
      ")\n",
      "No model loaded, starting from scratch\n"
     ]
    }
   ],
   "source": [
    "model = SP_AffineNet3(model_params).to(device)\n",
    "print(model)\n",
    "\n",
    "parameters = model.parameters()\n",
    "optimizer = optim.Adam(parameters, model_params.learning_rate, weight_decay=0.0001)\n",
    "scheduler = optim.lr_scheduler.LambdaLR(optimizer, lambda epoch: model_params.decay_rate ** epoch)\n",
    "# model_path = 'trained_models/10102_0.001_0_20_1_20230930-091532.pth'\n",
    "\n",
    "# if a model is loaded, the training will continue from the epoch it was saved at\n",
    "try:\n",
    "    model.load_state_dict(torch.load(model_path))\n",
    "    model_params.start_epoch = int(model_path.split('/')[-1].split('_')[3])\n",
    "    print(f'Loaded model from {model_path}\\nstarting at epoch {model_params.start_epoch}')\n",
    "    if model_params.start_epoch >= model_params.num_epochs:\n",
    "            model_params.num_epochs += model_params.start_epoch\n",
    "except:\n",
    "    model_params.start_epoch = 0\n",
    "    print('No model loaded, starting from scratch')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define training function\n",
    "def train(model, model_params, timestamp):\n",
    "    # Define loss function based on supervised or unsupervised learning\n",
    "    criterion = model_params.loss_image\n",
    "    extra = loss_extra()\n",
    "\n",
    "    if model_params.sup:\n",
    "        criterion_affine = nn.MSELoss()\n",
    "        # TODO: add loss for points1_affine and points2, Euclidean distance\n",
    "\n",
    "    # Define optimizer\n",
    "    optimizer = optim.Adam(model.parameters(), lr=model_params.learning_rate)\n",
    "\n",
    "    # Create empty list to store epoch number, train loss and validation loss\n",
    "    epoch_loss_list = []\n",
    "    running_loss_list = []\n",
    "    \n",
    "    # Create output directory\n",
    "    output_dir = f\"output/{model_params.get_model_code()}_{timestamp}\"\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    # Train model\n",
    "    for epoch in range(model_params.start_epoch, model_params.num_epochs):\n",
    "        # Set model to training mode\n",
    "        model.train()\n",
    "\n",
    "        # Zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        running_loss = 0.0\n",
    "        train_bar = tqdm(train_dataset, desc=f'Training Epoch {epoch+1}/{model_params.num_epochs}')\n",
    "        for i, data in enumerate(train_bar):\n",
    "            # Get images and affine parameters\n",
    "            if model_params.sup:\n",
    "                source_image, target_image, affine_params_true = data\n",
    "            else:\n",
    "                source_image, target_image = data\n",
    "                affine_params_true = None\n",
    "            source_image = source_image.to(device)\n",
    "            target_image = target_image.to(device)\n",
    "\n",
    "            # Forward + backward + optimize\n",
    "            outputs = model(source_image, target_image)\n",
    "            # for i in range(len(outputs)):\n",
    "            #         print(i, outputs[i].shape)\n",
    "            # 0 torch.Size([1, 1, 256, 256])\n",
    "            # 1 torch.Size([1, 2, 3])\n",
    "            # 2 (2, 4)\n",
    "            # 3 (2, 4)\n",
    "            # 4 (1, 4, 2)\n",
    "            # 5 (256, 9)\n",
    "            # 6 (256, 16)\n",
    "            # 7 (256, 256)\n",
    "            # 8 (256, 256)\n",
    "            transformed_source_affine = outputs[0] # image\n",
    "            affine_params_predicted = outputs[1] # affine parameters\n",
    "            points1 = outputs[2]\n",
    "            points2 = outputs[3]\n",
    "            points1_affine = np.array(outputs[4])\n",
    "\n",
    "            # print(f\"affine_params_true: {affine_params_true}\")\n",
    "            # print(f\"affine_params_predicted: {affine_params_predicted}\\n\")\n",
    "\n",
    "            try:\n",
    "                points1_affine = points1_affine.reshape(points1_affine.shape[2], points1_affine.shape[1])\n",
    "            except:\n",
    "                pass\n",
    "            desc1 = outputs[5]\n",
    "            desc2 = outputs[6]\n",
    "            heatmap1 = outputs[7]\n",
    "            heatmap2 = outputs[8]\n",
    "\n",
    "            loss = criterion(transformed_source_affine, target_image)\n",
    "            loss += extra(affine_params_predicted)\n",
    "            if model_params.sup:\n",
    "                loss_affine = criterion_affine(affine_params_true.view(1, 2, 3), affine_params_predicted.cpu())\n",
    "                # TODO: add loss for points1_affine and points2, Euclidean distance\n",
    "                # loss_points = criterion_points(points1_affine, points2)\n",
    "                loss += loss_affine\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "            \n",
    "            # Plot images if i < 5\n",
    "            if i % 100 == 0:\n",
    "                DL_affine_plot(f\"epoch{epoch+1}_train\", output_dir,\n",
    "                    f\"{i}\", \"_\", source_image[0, 0, :, :].detach().cpu().numpy(), \n",
    "                    target_image[0, 0, :, :].detach().cpu().numpy(), \n",
    "                    transformed_source_affine[0, 0, :, :].detach().cpu().numpy(),\n",
    "                    points1, points2, points1_affine, desc1, desc2, affine_params_true=affine_params_true,\n",
    "                    affine_params_predict=affine_params_predicted, heatmap1=heatmap1, heatmap2=heatmap2, plot=True)\n",
    "\n",
    "            # Print statistics\n",
    "            running_loss += loss.item()\n",
    "            running_loss_list.append([epoch+((i+1)/len(train_dataset)), loss.item()])\n",
    "            train_bar.set_postfix({'loss': running_loss / (i+1)})\n",
    "        print(f'Training Epoch {epoch+1}/{model_params.num_epochs} loss: {running_loss / len(train_dataset)}')\n",
    "\n",
    "        # Validate model\n",
    "        validation_loss = 0.0\n",
    "        model.eval()\n",
    "        # with torch.no_grad():\n",
    "        for i, data in enumerate(test_dataset, 0):\n",
    "            # Get images and affine parameters\n",
    "            if model_params.sup:\n",
    "                source_image, target_image, affine_params_true = data\n",
    "            else:\n",
    "                source_image, target_image = data\n",
    "                affine_params_true = None\n",
    "            source_image = source_image.to(device)\n",
    "            target_image = target_image.to(device)\n",
    "\n",
    "            # Forward pass\n",
    "            outputs = model(source_image, target_image)\n",
    "            # for i in range(len(outputs)):\n",
    "            #     print(i, outputs[i].shape)\n",
    "            transformed_source_affine = outputs[0]\n",
    "            affine_params_predicted = outputs[1]\n",
    "            points1 = outputs[2]\n",
    "            points2 = outputs[3]\n",
    "            points1_affine = np.array(outputs[4])\n",
    "            try:\n",
    "                points1_affine = points1_affine.reshape(points1_affine.shape[2], points1_affine.shape[1])\n",
    "            except:\n",
    "                pass\n",
    "            desc1 = outputs[5]\n",
    "            desc2 = outputs[6]\n",
    "            heatmap1 = outputs[7]\n",
    "            heatmap2 = outputs[8]\n",
    "\n",
    "            loss = criterion(transformed_source_affine, target_image)\n",
    "            loss += extra(affine_params_predicted)\n",
    "            if model_params.sup:\n",
    "                loss_affine = criterion_affine(affine_params_true.view(1, 2, 3), affine_params_predicted.cpu())\n",
    "                # TODO: add loss for points1_affine and points2, Euclidean distance\n",
    "                # loss_points = criterion_points(points1_affine, points2)\n",
    "                loss += loss_affine\n",
    "\n",
    "            # Add to validation loss\n",
    "            validation_loss += loss.item()\n",
    "\n",
    "            if i % 50 == 0:\n",
    "                DL_affine_plot(f\"epoch{epoch+1}_valid\", output_dir,\n",
    "                    f\"{i}\", \"_\", source_image[0, 0, :, :].detach().cpu().numpy(), \n",
    "                    target_image[0, 0, :, :].detach().cpu().numpy(), \n",
    "                    transformed_source_affine[0, 0, :, :].detach().cpu().numpy(),\n",
    "                    points1, points2, points1_affine, desc1, desc2, affine_params_true=affine_params_true,\n",
    "                    affine_params_predict=affine_params_predicted, heatmap1=heatmap1, heatmap2=heatmap2, plot=True)\n",
    "\n",
    "        # Print validation statistics\n",
    "        validation_loss /= len(test_dataset)\n",
    "        print(f'Validation Epoch {epoch+1}/{model_params.num_epochs} loss: {validation_loss}')\n",
    "\n",
    "        # Append epoch number, train loss and validation loss to epoch_loss_list\n",
    "        epoch_loss_list.append([epoch+1, running_loss / len(train_dataset), validation_loss])\n",
    "\n",
    "        \n",
    "        # Extract epoch number, train loss and validation loss from epoch_loss_list\n",
    "        epoch = [x[0] for x in epoch_loss_list]\n",
    "        train_loss = [x[1] for x in epoch_loss_list]\n",
    "        val_loss = [x[2] for x in epoch_loss_list]\n",
    "        step = [x[0] for x in running_loss_list]\n",
    "        running_train_loss = [x[1] for x in running_loss_list]\n",
    "\n",
    "        save_plot_name = f\"{output_dir}/loss_{model_params.get_model_code()}_epoch{model_params.num_epochs}_{timestamp}.png\"\n",
    "\n",
    "        # Plot train loss and validation loss against epoch number\n",
    "        plt.figure()\n",
    "        plt.plot(step, running_train_loss, label='Running Train Loss', alpha=0.3)\n",
    "        plt.plot(epoch, train_loss, label='Train Loss')\n",
    "        plt.plot(epoch, val_loss, label='Validation Loss')\n",
    "        plt.title('Train and Validation Loss')\n",
    "        plt.legend()\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.ylabel('Loss')\n",
    "        plt.yscale('log')\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(save_plot_name)\n",
    "        plt.close()\n",
    "        \n",
    "    print('Finished Training')\n",
    "\n",
    "    # delete all txt files in output_dir\n",
    "    for file in os.listdir(output_dir):\n",
    "        if file.endswith(\".txt\"):\n",
    "            os.remove(os.path.join(output_dir, file))\n",
    "\n",
    "    # Return epoch_loss_list\n",
    "    return epoch_loss_list\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## working code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1/5:   0%|          | 0/461 [00:00<?, ?it/s]/home/pkhamchuai/miniconda3/envs/spppt/lib/python3.8/site-packages/torch/nn/functional.py:4358: UserWarning: Default grid_sample and affine_grid behavior has changed to align_corners=False since 1.3.0. Please specify align_corners=True if the old behavior is desired. See the documentation of grid_sample for details.\n",
      "  warnings.warn(\n",
      "/home/pkhamchuai/miniconda3/envs/spppt/lib/python3.8/site-packages/torch/functional.py:504: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at ../aten/src/ATen/native/TensorShape.cpp:3526.)\n",
      "  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n",
      "/home/pkhamchuai/miniconda3/envs/spppt/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:136: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\"Detected call of `lr_scheduler.step()` before `optimizer.step()`. \"\n",
      "Training Epoch 1/5:  65%|██████▌   | 300/461 [00:57<00:27,  5.93it/s, loss=3.61e+3]/home/pkhamchuai/codes/EyeImgReg-check-training/utils/utils1.py:277: RuntimeWarning: invalid value encountered in divide\n",
      "  image = ((image - image.min()) / (image.max() - image.min()) * 255).astype(np.uint8)\n",
      "/home/pkhamchuai/codes/EyeImgReg-check-training/utils/utils1.py:277: RuntimeWarning: invalid value encountered in cast\n",
      "  image = ((image - image.min()) / (image.max() - image.min()) * 255).astype(np.uint8)\n",
      "Training Epoch 1/5: 100%|██████████| 461/461 [01:31<00:00,  5.02it/s, loss=4.24e+3]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch 1/5 loss: 4238.731186757789\n",
      "Validation Epoch 1/5 loss: 5073.507404852351\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2/5: 100%|██████████| 461/461 [01:28<00:00,  5.19it/s, loss=5.74e+3]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch 2/5 loss: 5739.813438970935\n",
      "Validation Epoch 2/5 loss: 6000.112492832569\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3/5: 100%|██████████| 461/461 [01:36<00:00,  4.78it/s, loss=5.73e+3]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch 3/5 loss: 5726.798460328923\n",
      "Validation Epoch 3/5 loss: 6000.112313646789\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 4/5: 100%|██████████| 461/461 [01:29<00:00,  5.15it/s, loss=5.75e+3]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch 4/5 loss: 5746.321364394626\n",
      "Validation Epoch 4/5 loss: 5935.892009210149\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 5/5: 100%|██████████| 461/461 [01:26<00:00,  5.35it/s, loss=5.77e+3]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch 5/5 loss: 5772.350731252234\n",
      "Validation Epoch 5/5 loss: 6000.113760571961\n",
      "Finished Training\n"
     ]
    }
   ],
   "source": [
    "timestamp = datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "loss_list = train(model, model_params, timestamp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training output:\n",
      "[1, 4238.731186757789, 5073.507404852351]\n",
      "[2, 5739.813438970935, 6000.112492832569]\n",
      "[3, 5726.798460328923, 6000.112313646789]\n",
      "[4, 5746.321364394626, 5935.892009210149]\n",
      "[5, 5772.350731252234, 6000.113760571961]\n"
     ]
    }
   ],
   "source": [
    "print(\"Training output:\")\n",
    "for i in range(len(loss_list)):\n",
    "    print(loss_list[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model saving"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trained_models/model3_10100_0.001_0_5_1_20231103-094248.pth\n"
     ]
    }
   ],
   "source": [
    "model_save_path = \"trained_models/\"\n",
    "model_name_to_save = model_save_path + f\"model3_{model_params.get_model_code()}_{timestamp}.pth\"\n",
    "print(model_name_to_save)\n",
    "torch.save(model.state_dict(), model_name_to_save)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test model (loading and inference)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save results and export metrics to csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = SPmodel = SP_AffineNet().to(device)\n",
    "# print(model)\n",
    "\n",
    "# parameters = model.parameters()\n",
    "# optimizer = optim.Adam(parameters, model_params.learning_rate)\n",
    "# scheduler = optim.lr_scheduler.LambdaLR(optimizer, lambda epoch: model_params.decay_rate ** epoch)\n",
    "\n",
    "# model.load_state_dict(torch.load(model_name_to_save))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing::   0%|          | 0/109 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "UnboundLocalError",
     "evalue": "local variable 'affine_params_true' referenced before assignment",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUnboundLocalError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 82\u001b[0m\n\u001b[1;32m     79\u001b[0m             os\u001b[38;5;241m.\u001b[39mremove(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(output_dir, file))\n\u001b[1;32m     81\u001b[0m \u001b[38;5;66;03m# timestamp = datetime.now().strftime(\"%Y%m%d-%H%M%S\")\u001b[39;00m\n\u001b[0;32m---> 82\u001b[0m metrics \u001b[38;5;241m=\u001b[39m \u001b[43mtest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel_params\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimestamp\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[14], line 57\u001b[0m, in \u001b[0;36mtest\u001b[0;34m(model, model_params, timestamp)\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     52\u001b[0m     plot_ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m     54\u001b[0m results \u001b[38;5;241m=\u001b[39m DL_affine_plot(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, output_dir,\n\u001b[1;32m     55\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_\u001b[39m\u001b[38;5;124m\"\u001b[39m, source_image[\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m0\u001b[39m, :, :]\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mnumpy(), target_image[\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m0\u001b[39m, :, :]\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mnumpy(), \\\n\u001b[1;32m     56\u001b[0m     transformed_source_affine[\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m0\u001b[39m, :, :]\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mnumpy(), \\\n\u001b[0;32m---> 57\u001b[0m     points1, points2, points1_affine, desc1, desc2, affine_params_true\u001b[38;5;241m=\u001b[39m\u001b[43maffine_params_true\u001b[49m,\n\u001b[1;32m     58\u001b[0m     affine_params_predict\u001b[38;5;241m=\u001b[39maffine_params_predicted, heatmap1\u001b[38;5;241m=\u001b[39mheatmap1, heatmap2\u001b[38;5;241m=\u001b[39mheatmap2, plot\u001b[38;5;241m=\u001b[39mplot_)\n\u001b[1;32m     60\u001b[0m \u001b[38;5;66;03m# calculate metrics\u001b[39;00m\n\u001b[1;32m     61\u001b[0m \u001b[38;5;66;03m# matches1_transformed = results[0]\u001b[39;00m\n\u001b[1;32m     62\u001b[0m mse_before \u001b[38;5;241m=\u001b[39m results[\u001b[38;5;241m1\u001b[39m]\n",
      "\u001b[0;31mUnboundLocalError\u001b[0m: local variable 'affine_params_true' referenced before assignment"
     ]
    }
   ],
   "source": [
    "def test(model, model_params, timestamp):\n",
    "    # Set model to training mode\n",
    "    model.eval()\n",
    "\n",
    "    # Create output directory\n",
    "    output_dir = f\"output/{model_params.get_model_code()}_{timestamp}_test\"\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    # Validate model\n",
    "    # validation_loss = 0.0\n",
    "\n",
    "    # create a csv file to store the metrics\n",
    "    csv_file = f\"{output_dir}/metrics.csv\"\n",
    "    with open(csv_file, 'w', newline='') as file:\n",
    "        writer = csv.writer(file)\n",
    "        # matches1_transformed.shape[-1], mse_before, mse12, tre_before, tre12, \\\n",
    "        # mse12_image, ssim12_image, \n",
    "        writer.writerow([\"index\", \"mse_before\", \"mse12\", \"tre_before\", \"tre12\", \"mse12_image_before\", \"mse12_image\", \"ssim12_image_before\", \"ssim12_image\"])\n",
    "\n",
    "    with torch.no_grad():\n",
    "        testbar = tqdm(test_dataset, desc=f'Testing:')\n",
    "        for i, data in enumerate(testbar, 0):\n",
    "            # Get images and affine parameters\n",
    "            if model_params.sup:\n",
    "                source_image, target_image, affine_params_true = data\n",
    "            else:\n",
    "                source_image, target_image = data\n",
    "            source_image = source_image.to(device)\n",
    "            target_image = target_image.to(device)\n",
    "\n",
    "            # Forward pass\n",
    "            outputs = model(source_image, target_image)\n",
    "            # for i in range(len(outputs)):\n",
    "            #     print(i, outputs[i].shape)\n",
    "            transformed_source_affine = outputs[0]\n",
    "            affine_params_predicted = outputs[1]\n",
    "            points1 = outputs[2]\n",
    "            points2 = outputs[3]\n",
    "            points1_affine = np.array(outputs[4])\n",
    "            try:\n",
    "                points1_affine = points1_affine.reshape(points1_affine.shape[2], points1_affine.shape[1])\n",
    "            except:\n",
    "                pass\n",
    "            desc1 = outputs[5]\n",
    "            desc2 = outputs[6]\n",
    "            heatmap1 = outputs[7]\n",
    "            heatmap2 = outputs[8]\n",
    "\n",
    "            if i < 50:\n",
    "                plot_ = True\n",
    "            else:\n",
    "                plot_ = False\n",
    "\n",
    "            results = DL_affine_plot(f\"{i+1}\", output_dir,\n",
    "                f\"{i}\", \"_\", source_image[0, 0, :, :].cpu().numpy(), target_image[0, 0, :, :].cpu().numpy(), \\\n",
    "                transformed_source_affine[0, 0, :, :].cpu().numpy(), \\\n",
    "                points1, points2, points1_affine, desc1, desc2, affine_params_true=affine_params_true,\n",
    "                affine_params_predict=affine_params_predicted, heatmap1=heatmap1, heatmap2=heatmap2, plot=plot_)\n",
    "\n",
    "            # calculate metrics\n",
    "            # matches1_transformed = results[0]\n",
    "            mse_before = results[1]\n",
    "            mse12 = results[2]\n",
    "            tre_before = results[3]\n",
    "            tre12 = results[4]\n",
    "            mse12_image_before = results[5]\n",
    "            mse12_image = results[6]\n",
    "            ssim12_image_before = results[7]\n",
    "            ssim12_image = results[8]\n",
    "\n",
    "            # write metrics to csv file\n",
    "            with open(csv_file, 'a', newline='') as file:\n",
    "                writer = csv.writer(file) # TODO: might need to export true & predicted affine parameters too\n",
    "                writer.writerow([i, mse_before, mse12, tre_before, tre12, mse12_image_before, mse12_image, ssim12_image_before, ssim12_image])\n",
    "\n",
    "    # delete all txt files in output_dir\n",
    "    for file in os.listdir(output_dir):\n",
    "        if file.endswith(\".txt\"):\n",
    "            os.remove(os.path.join(output_dir, file))\n",
    "\n",
    "# timestamp = datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "metrics = test(model, model_params, timestamp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spppt",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
