{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import cv2\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "from skimage.metrics import structural_similarity as ssim\n",
    "\n",
    "# Suppress the specific warning\n",
    "import csv\n",
    "from datetime import datetime\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torch\n",
    "from torchvision import transforms\n",
    "from torch import nn, optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils import data\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.transforms import ToTensor\n",
    "\n",
    "from utils.utils0 import *\n",
    "from utils.utils1 import *\n",
    "from utils.utils1 import ModelParams, DL_affine_plot, loss_extra\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f'Device: {device}')\n",
    "\n",
    "# Stub to warn about opencv version.\n",
    "if int(cv2.__version__[0]) < 3: # pragma: no cover\n",
    "  print('Warning: OpenCV 3 is not installed')\n",
    "\n",
    "image_size = 256\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cases, model parameters\n",
    "- Supervised DL w/ groundtruth affine transformation parameters (MSE params, MSE, NCC images)\n",
    "    - Synthetic eye\n",
    "    - Synthetic shape\n",
    "- Unsupervised DL (MSE, NCC images)\n",
    "    - Actual eye data\n",
    "    - Synthetic eye\n",
    "    - Synthetic shape\n",
    "- Data\n",
    "    - only images\n",
    "    - only heatmaps\n",
    "    - images & heatmaps\n",
    "- Loss function\n",
    "    - MSE affine parameters\n",
    "    - MSE, NCC images\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model name:  dataset1_sup1_image1_heatmaps0_loss_image0\n",
      "Model code:  11100_0.001_0_20_1\n",
      "Model params:  {'dataset': 1, 'sup': 1, 'image': 1, 'heatmaps': 0, 'loss_image_case': 0, 'loss_image': MSELoss(), 'loss_affine': <utils.utils1.loss_affine object at 0x7fe53458ac10>, 'learning_rate': 0.001, 'decay_rate': 0.96, 'start_epoch': 0, 'num_epochs': 20, 'batch_size': 1, 'model_name': 'dataset1_sup1_image1_heatmaps0_loss_image0'}\n",
      "\n",
      "Model name:  dataset1_sup1_image1_heatmaps0_loss_image0\n",
      "Model code:  11100_0.001_0_20_1\n",
      "Dataset used:  Synthetic eye\n",
      "Supervised or unsupervised model:  Supervised\n",
      "Image type:  Image used\n",
      "Heatmaps used:  Heatmaps not used\n",
      "Loss function case:  0\n",
      "Loss function for image:  MSELoss()\n",
      "Loss function for affine:  <utils.utils1.loss_affine object at 0x7fe53458ac10>\n",
      "Learning rate:  0.001\n",
      "Decay rate:  0.96\n",
      "Start epoch:  0\n",
      "Number of epochs:  20\n",
      "Batch size:  1\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model_params = ModelParams(sup=1, dataset=1, image=1, heatmaps=0, \n",
    "                           loss_image=0, num_epochs=20, learning_rate=1e-3)\n",
    "model_params.print_explanation()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Models\n",
    "## SuperPoint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ImgReg Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from networks import affine_network_simple as an\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "from utils.SuperPoint import SuperPointFrontend\n",
    "from utils.utils0 import *\n",
    "from utils.utils1 import *\n",
    "from utils.utils1 import transform_points_DVF\n",
    "\n",
    "class SP_DHR_Net(nn.Module):\n",
    "    def __init__(self, model_params):\n",
    "        super(SP_DHR_Net, self).__init__()\n",
    "        self.superpoint = SuperPointFrontend('utils/superpoint_v1.pth', nms_dist=4,\n",
    "                          conf_thresh=0.015, nn_thresh=0.7, cuda=True)\n",
    "        self.affineNet = an.load_network(device)\n",
    "        self.nn_thresh = 0.7\n",
    "        self.model_params = model_params\n",
    "        print(\"\\nRunning new version (not run SP on source image)\")\n",
    "\n",
    "    def forward(self, source_image, target_image):\n",
    "        # source_image = source_image.to(device)\n",
    "        # target_image = target_image.to(device)\n",
    "\n",
    "        # print('source_image: ', source_image.shape)\n",
    "        # print('target_image: ', target_image.shape)\n",
    "        points1, desc1, heatmap1 = self.superpoint(source_image[0, 0, :, :].cpu().numpy())\n",
    "        points2, desc2, heatmap2 = self.superpoint(target_image[0, 0, :, :].cpu().numpy())\n",
    "\n",
    "        if self.model_params.heatmaps == 0:\n",
    "            affine_params = self.affineNet(source_image, target_image)\n",
    "        elif self.model_params.heatmaps == 1:\n",
    "            print(\"This part is not yet implemented.\")\n",
    "            # affine_params = self.affineNet(source_image, target_image, heatmap1, heatmap2)\n",
    "\n",
    "        # transform the source image using the affine parameters\n",
    "        # using F.affine_grid and F.grid_sample\n",
    "        transformed_source_affine = tensor_affine_transform(source_image, affine_params)\n",
    "        points1_2, desc1_2, heatmap1_2 = self.superpoint(transformed_source_affine[0, 0, :, :].detach().cpu().numpy())\n",
    "\n",
    "        # match the points between the two images\n",
    "        tracker = PointTracker(5, nn_thresh=0.7)\n",
    "        try:\n",
    "            matches = tracker.nn_match_two_way(desc1, desc2, nn_thresh=self.nn_thresh)\n",
    "        except:\n",
    "            # print('No matches found')\n",
    "            # TODO: find a better way to do this\n",
    "            try:\n",
    "                while matches.shape[1] < 3 and self.nn_thresh > 0.1:\n",
    "                    self.nn_thresh = self.nn_thresh - 0.1\n",
    "                    matches = tracker.nn_match_two_way(desc1, desc2, nn_thresh=self.nn_thresh)\n",
    "            except:\n",
    "                return transformed_source_affine, affine_params, [], [], [], [], [], [], []\n",
    "\n",
    "        # take the elements from points1 and points2 using the matches as indices\n",
    "        matches1 = np.array(points1[:2, matches[0, :].astype(int)])\n",
    "        matches2 = np.array(points2[:2, matches[1, :].astype(int)])\n",
    "        # matches1_2 = np.array(points1_2[:2, matches[0, :].astype(int)])\n",
    "        # print('matches1', matches1)\n",
    "        # print('matches2', matches2)\n",
    "        # print('matches1_2', matches1_2)\n",
    "\n",
    "        # try:\n",
    "        #     matches1_2 = points1_2[:2, matches[0, :].astype(int)]\n",
    "        # except:\n",
    "        # print(affine_params.cpu().detach().shape, transformed_source_affine.shape)\n",
    "        matches1_2 = transform_points_DVF(torch.tensor(matches1), \n",
    "                        affine_params.cpu().detach(), transformed_source_affine)\n",
    "\n",
    "        # transform the points using the affine parameters\n",
    "        # matches1_transformed = transform_points(matches1.T[None, :, :], affine_params.cpu().detach())\n",
    "        return transformed_source_affine, affine_params, matches1, matches2, matches1_2, \\\n",
    "            desc1_2, desc2, heatmap1_2, heatmap2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SP ImgReg model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.datagen import datagen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test datagen for all datasets and training and testing\n",
    "# for dataset in range(4): # don't forget to change this back to 2\n",
    "#     for is_train in [True, False]:\n",
    "#         for sup in [False, True]:\n",
    "#             print(f'dataset: {dataset}, is_train: {is_train}, sup: {sup}')\n",
    "#             dataloader = datagen(dataset, is_train, sup)\n",
    "            \n",
    "#             if sup==1 and dataset==0:\n",
    "#                 print('skipping')\n",
    "#                 pass\n",
    "#             else:\n",
    "#                 try:\n",
    "#                     print('index, source_img.shape,       target_img.shape')\n",
    "#                     for i, (source_img, target_img) in enumerate(dataloader):\n",
    "#                         print(i, source_img.shape, target_img.shape)\n",
    "#                         if i == 2:\n",
    "#                             break\n",
    "#                 except ValueError:\n",
    "#                     print('index, source_img.shape,       target_img.shape,            affine_params.shape')\n",
    "#                     for i, batch in enumerate(dataloader):\n",
    "#                         print(i, batch[0].shape, batch[1].shape, batch[2].shape)\n",
    "#                         if i == 5:\n",
    "#                             break\n",
    "#             print('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training\n",
    "## Dataset initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set:  [torch.Size([1, 1, 256, 256]), torch.Size([1, 1, 256, 256]), torch.Size([1, 2, 3])]\n",
      "Test set:  [torch.Size([1, 1, 256, 256]), torch.Size([1, 1, 256, 256]), torch.Size([1, 2, 3])]\n"
     ]
    }
   ],
   "source": [
    "train_dataset = datagen(model_params.dataset, True, model_params.sup)\n",
    "test_dataset = datagen(model_params.dataset, False, model_params.sup)\n",
    "\n",
    "# Get sample batch\n",
    "print('Train set: ', [x.shape for x in next(iter(train_dataset))])\n",
    "print('Test set: ', [x.shape for x in next(iter(test_dataset))])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model initialize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset1_sup1_image1_heatmaps0_loss_image0\n",
      "\n",
      "Model name:  dataset1_sup1_image1_heatmaps0_loss_image0\n",
      "Model code:  11100_0.001_0_20_1\n",
      "Dataset used:  Synthetic eye\n",
      "Supervised or unsupervised model:  Supervised\n",
      "Image type:  Image used\n",
      "Heatmaps used:  Heatmaps not used\n",
      "Loss function case:  0\n",
      "Loss function for image:  MSELoss()\n",
      "Loss function for affine:  <utils.utils1.loss_affine object at 0x7fe53458ac10>\n",
      "Learning rate:  0.001\n",
      "Decay rate:  0.96\n",
      "Start epoch:  0\n",
      "Number of epochs:  20\n",
      "Batch size:  1\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# print case\n",
    "print(model_params)\n",
    "model_params.print_explanation()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Running new version (not run SP on source image)\n",
      "SP_DHR_Net(\n",
      "  (affineNet): Affine_Network(\n",
      "    (feature_extractor): Feature_Extractor(\n",
      "      (input_layer): Sequential(\n",
      "        (0): Conv2d(2, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3))\n",
      "      )\n",
      "      (layer_1): Forward_Layer(\n",
      "        (pool_layer): Sequential(\n",
      "          (0): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(3, 3))\n",
      "        )\n",
      "        (layer): Sequential(\n",
      "          (0): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(3, 3))\n",
      "          (1): GroupNorm(128, 128, eps=1e-05, affine=True)\n",
      "          (2): PReLU(num_parameters=1)\n",
      "          (3): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "          (4): GroupNorm(128, 128, eps=1e-05, affine=True)\n",
      "          (5): PReLU(num_parameters=1)\n",
      "        )\n",
      "      )\n",
      "      (layer_2): Forward_Layer(\n",
      "        (layer): Sequential(\n",
      "          (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "          (1): GroupNorm(128, 128, eps=1e-05, affine=True)\n",
      "          (2): PReLU(num_parameters=1)\n",
      "          (3): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "          (4): GroupNorm(128, 128, eps=1e-05, affine=True)\n",
      "          (5): PReLU(num_parameters=1)\n",
      "        )\n",
      "      )\n",
      "      (layer_3): Forward_Layer(\n",
      "        (pool_layer): Sequential(\n",
      "          (0): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(3, 3))\n",
      "        )\n",
      "        (layer): Sequential(\n",
      "          (0): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(3, 3))\n",
      "          (1): GroupNorm(256, 256, eps=1e-05, affine=True)\n",
      "          (2): PReLU(num_parameters=1)\n",
      "          (3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "          (4): GroupNorm(256, 256, eps=1e-05, affine=True)\n",
      "          (5): PReLU(num_parameters=1)\n",
      "        )\n",
      "      )\n",
      "      (layer_4): Forward_Layer(\n",
      "        (layer): Sequential(\n",
      "          (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "          (1): GroupNorm(256, 256, eps=1e-05, affine=True)\n",
      "          (2): PReLU(num_parameters=1)\n",
      "          (3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "          (4): GroupNorm(256, 256, eps=1e-05, affine=True)\n",
      "          (5): PReLU(num_parameters=1)\n",
      "        )\n",
      "      )\n",
      "      (layer_5): Forward_Layer(\n",
      "        (pool_layer): Sequential(\n",
      "          (0): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(3, 3))\n",
      "        )\n",
      "        (layer): Sequential(\n",
      "          (0): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(3, 3))\n",
      "          (1): GroupNorm(512, 512, eps=1e-05, affine=True)\n",
      "          (2): PReLU(num_parameters=1)\n",
      "          (3): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "          (4): GroupNorm(512, 512, eps=1e-05, affine=True)\n",
      "          (5): PReLU(num_parameters=1)\n",
      "        )\n",
      "      )\n",
      "      (layer_6): Forward_Layer(\n",
      "        (pool_layer): Sequential(\n",
      "          (0): Conv2d(512, 1024, kernel_size=(3, 3), stride=(2, 2), padding=(3, 3))\n",
      "        )\n",
      "        (layer): Sequential(\n",
      "          (0): Conv2d(512, 1024, kernel_size=(3, 3), stride=(2, 2), padding=(3, 3))\n",
      "          (1): GroupNorm(1024, 1024, eps=1e-05, affine=True)\n",
      "          (2): PReLU(num_parameters=1)\n",
      "          (3): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "          (4): GroupNorm(1024, 1024, eps=1e-05, affine=True)\n",
      "          (5): PReLU(num_parameters=1)\n",
      "        )\n",
      "      )\n",
      "      (last_layer): Sequential(\n",
      "        (0): Conv2d(1024, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
      "        (1): GroupNorm(512, 512, eps=1e-05, affine=True)\n",
      "        (2): PReLU(num_parameters=1)\n",
      "        (3): Conv2d(512, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
      "        (4): GroupNorm(256, 256, eps=1e-05, affine=True)\n",
      "        (5): PReLU(num_parameters=1)\n",
      "        (6): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "      )\n",
      "    )\n",
      "    (regression_network): Regression_Network(\n",
      "      (fc): Sequential(\n",
      "        (0): Linear(in_features=256, out_features=6, bias=True)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      ")\n",
      "No model loaded, starting from scratch\n"
     ]
    }
   ],
   "source": [
    "model = SP_DHR_Net(model_params)\n",
    "print(model)\n",
    "\n",
    "parameters = model.parameters()\n",
    "optimizer = optim.Adam(parameters, model_params.learning_rate, weight_decay=0.0001)\n",
    "scheduler = optim.lr_scheduler.LambdaLR(optimizer, lambda epoch: model_params.decay_rate ** epoch)\n",
    "# model_path = 'trained_models/10102_0.001_0_20_1_20230930-091532.pth'\n",
    "\n",
    "# if a model is loaded, the training will continue from the epoch it was saved at\n",
    "try:\n",
    "    model.load_state_dict(torch.load(model_path))\n",
    "    model_params.start_epoch = int(model_path.split('/')[-1].split('_')[3])\n",
    "    print(f'Loaded model from {model_path}\\nstarting at epoch {model_params.start_epoch}')\n",
    "    if model_params.start_epoch >= model_params.num_epochs:\n",
    "            model_params.num_epochs += model_params.start_epoch\n",
    "except:\n",
    "    model_params.start_epoch = 0\n",
    "    print('No model loaded, starting from scratch')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define training function\n",
    "def train(model, model_params, timestamp):\n",
    "    # Define loss function based on supervised or unsupervised learning\n",
    "    criterion = model_params.loss_image\n",
    "    extra = loss_extra()\n",
    "\n",
    "    if model_params.sup:\n",
    "        criterion_affine = nn.MSELoss()\n",
    "        # TODO: add loss for points1_affine and points2, Euclidean distance\n",
    "\n",
    "    # Define optimizer\n",
    "    optimizer = optim.Adam(model.parameters(), lr=model_params.learning_rate)\n",
    "\n",
    "    # Create empty list to store epoch number, train loss and validation loss\n",
    "    epoch_loss_list = []\n",
    "    running_loss_list = []\n",
    "    \n",
    "    # Create output directory\n",
    "    output_dir = f\"output/{model_params.get_model_code()}_{timestamp}\"\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    # Train model\n",
    "    for epoch in range(model_params.start_epoch, model_params.num_epochs):\n",
    "        # Set model to training mode\n",
    "        model.train()\n",
    "\n",
    "        # Zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        running_loss = 0.0\n",
    "        train_bar = tqdm(train_dataset, desc=f'Training Epoch {epoch+1}/{model_params.num_epochs}')\n",
    "        for i, data in enumerate(train_bar):\n",
    "            # Get images and affine parameters\n",
    "            if model_params.sup:\n",
    "                source_image, target_image, affine_params_true = data\n",
    "            else:\n",
    "                source_image, target_image = data\n",
    "                affine_params_true = None\n",
    "            source_image = source_image.to(device)\n",
    "            target_image = target_image.to(device)\n",
    "\n",
    "            # Forward + backward + optimize\n",
    "            outputs = model(source_image, target_image)\n",
    "            # for i in range(len(outputs)):\n",
    "            #         print(i, outputs[i].shape)\n",
    "            # 0 torch.Size([1, 1, 256, 256])\n",
    "            # 1 torch.Size([1, 2, 3])\n",
    "            # 2 (2, 4)\n",
    "            # 3 (2, 4)\n",
    "            # 4 (1, 4, 2)\n",
    "            # 5 (256, 9)\n",
    "            # 6 (256, 16)\n",
    "            # 7 (256, 256)\n",
    "            # 8 (256, 256)\n",
    "            transformed_source_affine = outputs[0] # image\n",
    "            affine_params_predicted = outputs[1] # affine parameters\n",
    "            points1 = outputs[2]\n",
    "            points2 = outputs[3]\n",
    "            points1_affine = np.array(outputs[4])\n",
    "\n",
    "            # print(f\"affine_params_true: {affine_params_true}\")\n",
    "            # print(f\"affine_params_predicted: {affine_params_predicted}\\n\")\n",
    "\n",
    "            try:\n",
    "                points1_affine = points1_affine.reshape(points1_affine.shape[2], points1_affine.shape[1])\n",
    "            except:\n",
    "                pass\n",
    "            desc1 = outputs[5]\n",
    "            desc2 = outputs[6]\n",
    "            heatmap1 = outputs[7]\n",
    "            heatmap2 = outputs[8]\n",
    "\n",
    "            loss = criterion(transformed_source_affine, target_image)\n",
    "            loss += extra(affine_params_predicted)\n",
    "            if model_params.sup:\n",
    "                loss_affine = criterion_affine(affine_params_true.view(1, 2, 3), affine_params_predicted.cpu())\n",
    "                # TODO: add loss for points1_affine and points2, Euclidean distance\n",
    "                # loss_points = criterion_points(points1_affine, points2)\n",
    "                loss += loss_affine\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "            \n",
    "            # Plot images if i < 5\n",
    "            if i < 5:\n",
    "                DL_affine_plot(f\"epoch{epoch+1}_train\", output_dir,\n",
    "                    f\"{i}\", \"_\", source_image[0, 0, :, :].detach().cpu().numpy(), \n",
    "                    target_image[0, 0, :, :].detach().cpu().numpy(), \n",
    "                    transformed_source_affine[0, 0, :, :].detach().cpu().numpy(),\n",
    "                    points1, points2, points1_affine, desc1, desc2, affine_params_true=affine_params_true,\n",
    "                    affine_params_predict=affine_params_predicted, heatmap1=heatmap1, heatmap2=heatmap2, plot=True)\n",
    "\n",
    "            # Print statistics\n",
    "            running_loss += loss.item()\n",
    "            running_loss_list.append([epoch+((i+1)/len(train_dataset)), loss.item()])\n",
    "            train_bar.set_postfix({'loss': running_loss / (i+1)})\n",
    "        print(f'Training Epoch {epoch+1}/{model_params.num_epochs} loss: {running_loss / len(train_dataset)}')\n",
    "\n",
    "        # Validate model\n",
    "        validation_loss = 0.0\n",
    "        model.eval()\n",
    "        # with torch.no_grad():\n",
    "        for i, data in enumerate(test_dataset, 0):\n",
    "            # Get images and affine parameters\n",
    "            if model_params.sup:\n",
    "                source_image, target_image, affine_params_true = data\n",
    "            else:\n",
    "                source_image, target_image = data\n",
    "                affine_params_true = None\n",
    "            source_image = source_image.to(device)\n",
    "            target_image = target_image.to(device)\n",
    "\n",
    "            # Forward pass\n",
    "            outputs = model(source_image, target_image)\n",
    "            # for i in range(len(outputs)):\n",
    "            #     print(i, outputs[i].shape)\n",
    "            transformed_source_affine = outputs[0]\n",
    "            affine_params_predicted = outputs[1]\n",
    "            points1 = outputs[2]\n",
    "            points2 = outputs[3]\n",
    "            points1_affine = np.array(outputs[4])\n",
    "            try:\n",
    "                points1_affine = points1_affine.reshape(points1_affine.shape[2], points1_affine.shape[1])\n",
    "            except:\n",
    "                pass\n",
    "            desc1 = outputs[5]\n",
    "            desc2 = outputs[6]\n",
    "            heatmap1 = outputs[7]\n",
    "            heatmap2 = outputs[8]\n",
    "\n",
    "            loss = criterion(transformed_source_affine, target_image)\n",
    "            loss += extra(affine_params_predicted)\n",
    "            if model_params.sup:\n",
    "                loss_affine = criterion_affine(affine_params_true.view(1, 2, 3), affine_params_predicted.cpu())\n",
    "                # TODO: add loss for points1_affine and points2, Euclidean distance\n",
    "                # loss_points = criterion_points(points1_affine, points2)\n",
    "                loss += loss_affine\n",
    "\n",
    "            # Add to validation loss\n",
    "            validation_loss += loss.item()\n",
    "\n",
    "            # Plot images if i < 5\n",
    "            if i < 5:\n",
    "                DL_affine_plot(f\"epoch{epoch+1}_valid\", output_dir,\n",
    "                    f\"{i}\", \"_\", source_image[0, 0, :, :].detach().cpu().numpy(), \n",
    "                    target_image[0, 0, :, :].detach().cpu().numpy(), \n",
    "                    transformed_source_affine[0, 0, :, :].detach().cpu().numpy(),\n",
    "                    points1, points2, points1_affine, desc1, desc2, affine_params_true=affine_params_true,\n",
    "                    affine_params_predict=affine_params_predicted, heatmap1=heatmap1, heatmap2=heatmap2, plot=True)\n",
    "\n",
    "        # Print validation statistics\n",
    "        validation_loss /= len(test_dataset)\n",
    "        print(f'Validation Epoch {epoch+1}/{model_params.num_epochs} loss: {validation_loss}')\n",
    "\n",
    "        # Append epoch number, train loss and validation loss to epoch_loss_list\n",
    "        epoch_loss_list.append([epoch+1, running_loss / len(train_dataset), validation_loss])\n",
    "\n",
    "        \n",
    "        # Extract epoch number, train loss and validation loss from epoch_loss_list\n",
    "        epoch = [x[0] for x in epoch_loss_list]\n",
    "        train_loss = [x[1] for x in epoch_loss_list]\n",
    "        val_loss = [x[2] for x in epoch_loss_list]\n",
    "        step = [x[0] for x in running_loss_list]\n",
    "        running_train_loss = [x[1] for x in running_loss_list]\n",
    "\n",
    "        save_plot_name = f\"{output_dir}/loss_{model_params.get_model_code()}_epoch{model_params.num_epochs}_{timestamp}.png\"\n",
    "\n",
    "        # Plot train loss and validation loss against epoch number\n",
    "        plt.figure()\n",
    "        plt.plot(step, running_train_loss, label='Running Train Loss', alpha=0.3)\n",
    "        plt.plot(epoch, train_loss, label='Train Loss')\n",
    "        plt.plot(epoch, val_loss, label='Validation Loss')\n",
    "        plt.title('Train and Validation Loss')\n",
    "        plt.legend()\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.ylabel('Loss')\n",
    "        plt.yscale('log')\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(save_plot_name)\n",
    "        plt.close()\n",
    "        \n",
    "    print('Finished Training')\n",
    "\n",
    "    # delete all txt files in output_dir\n",
    "    for file in os.listdir(output_dir):\n",
    "        if file.endswith(\".txt\"):\n",
    "            os.remove(os.path.join(output_dir, file))\n",
    "\n",
    "    # Return epoch_loss_list\n",
    "    return epoch_loss_list\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## working code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1/20:   0%|          | 0/461 [00:00<?, ?it/s]/home/pkhamchuai/codes/EyeImgReg-check-training/utils/utils1.py:712: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  torch.tensor(M).view(1, 2, 3))\n",
      "/home/pkhamchuai/miniconda3/envs/spppt/lib/python3.8/site-packages/torch/nn/functional.py:4298: UserWarning: Default grid_sample and affine_grid behavior has changed to align_corners=False since 1.3.0. Please specify align_corners=True if the old behavior is desired. See the documentation of grid_sample for details.\n",
      "  warnings.warn(\n",
      "/home/pkhamchuai/miniconda3/envs/spppt/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:139: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\"Detected call of `lr_scheduler.step()` before `optimizer.step()`. \"\n",
      "Training Epoch 1/20: 100%|██████████| 461/461 [01:30<00:00,  5.07it/s, loss=2.2]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch 1/20 loss: 2.1975847451084993\n",
      "Validation Epoch 1/20 loss: 7.263564122926205\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2/20: 100%|██████████| 461/461 [01:39<00:00,  4.64it/s, loss=3.89]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch 2/20 loss: 3.888145002595062\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/pkhamchuai/codes/EyeImgReg-check-training/utils/utils1.py:250: RuntimeWarning: invalid value encountered in divide\n",
      "  image = ((image - image.min()) / (image.max() - image.min()) * 255).astype(np.uint8)\n",
      "/home/pkhamchuai/codes/EyeImgReg-check-training/utils/utils1.py:250: RuntimeWarning: invalid value encountered in cast\n",
      "  image = ((image - image.min()) / (image.max() - image.min()) * 255).astype(np.uint8)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Epoch 2/20 loss: 9.310233995455121\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3/20: 100%|██████████| 461/461 [01:34<00:00,  4.87it/s, loss=3.93]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch 3/20 loss: 3.9253992739571926\n",
      "Validation Epoch 3/20 loss: 1.4278312768411199\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 4/20: 100%|██████████| 461/461 [01:34<00:00,  4.88it/s, loss=0.656]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch 4/20 loss: 0.6556272684978302\n",
      "Validation Epoch 4/20 loss: 0.7295820838814482\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 5/20: 100%|██████████| 461/461 [01:30<00:00,  5.08it/s, loss=0.5]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch 5/20 loss: 0.4999029438562869\n",
      "Validation Epoch 5/20 loss: 0.156081740325744\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 6/20: 100%|██████████| 461/461 [01:30<00:00,  5.10it/s, loss=0.109] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch 6/20 loss: 0.10937255937222254\n",
      "Validation Epoch 6/20 loss: 0.11381915471422563\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 7/20: 100%|██████████| 461/461 [01:33<00:00,  4.94it/s, loss=0.135]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch 7/20 loss: 0.1354616981205771\n",
      "Validation Epoch 7/20 loss: 0.04687200412325083\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 8/20: 100%|██████████| 461/461 [01:35<00:00,  4.84it/s, loss=0.0955]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch 8/20 loss: 0.09547796265430215\n",
      "Validation Epoch 8/20 loss: 0.2183945373110815\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 9/20: 100%|██████████| 461/461 [01:30<00:00,  5.11it/s, loss=0.123]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch 9/20 loss: 0.12337391245544539\n",
      "Validation Epoch 9/20 loss: 0.14618844600445632\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 10/20: 100%|██████████| 461/461 [01:43<00:00,  4.45it/s, loss=0.0734]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch 10/20 loss: 0.07338362073812658\n",
      "Validation Epoch 10/20 loss: 0.08963110998546311\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 11/20: 100%|██████████| 461/461 [01:59<00:00,  3.85it/s, loss=0.0725]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch 11/20 loss: 0.07253742514182197\n",
      "Validation Epoch 11/20 loss: 0.06919823586940765\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 12/20: 100%|██████████| 461/461 [01:44<00:00,  4.42it/s, loss=0.0474]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch 12/20 loss: 0.04735311784492624\n",
      "Validation Epoch 12/20 loss: 0.04652125000475197\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 13/20: 100%|██████████| 461/461 [01:35<00:00,  4.83it/s, loss=0.0522]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch 13/20 loss: 0.05222686697524889\n",
      "Validation Epoch 13/20 loss: 0.07957459142038581\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 14/20: 100%|██████████| 461/461 [01:38<00:00,  4.67it/s, loss=0.0558]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch 14/20 loss: 0.05582663718279685\n",
      "Validation Epoch 14/20 loss: 0.06745984827364804\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 15/20: 100%|██████████| 461/461 [01:33<00:00,  4.92it/s, loss=0.0602]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch 15/20 loss: 0.0602284325104926\n",
      "Validation Epoch 15/20 loss: 0.1385147816612633\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 16/20: 100%|██████████| 461/461 [01:34<00:00,  4.87it/s, loss=0.158] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch 16/20 loss: 0.15809513810302844\n",
      "Validation Epoch 16/20 loss: 0.10265482517830822\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 17/20: 100%|██████████| 461/461 [01:32<00:00,  4.97it/s, loss=0.0617]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch 17/20 loss: 0.06171040538367118\n",
      "Validation Epoch 17/20 loss: 0.0917477202046355\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 18/20: 100%|██████████| 461/461 [01:35<00:00,  4.84it/s, loss=0.0642]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch 18/20 loss: 0.06420281573568437\n",
      "Validation Epoch 18/20 loss: 0.13654189276585885\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 19/20: 100%|██████████| 461/461 [01:39<00:00,  4.63it/s, loss=0.228]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch 19/20 loss: 0.2283831620528292\n",
      "Validation Epoch 19/20 loss: 0.6673955567386172\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 20/20: 100%|██████████| 461/461 [01:33<00:00,  4.92it/s, loss=1.25] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch 20/20 loss: 1.2478737919359042\n",
      "Validation Epoch 20/20 loss: 1.3351452131883814\n",
      "Finished Training\n"
     ]
    }
   ],
   "source": [
    "timestamp = datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "loss_list = train(model, model_params, timestamp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training output:\n",
      "[1, 2.1975847451084993, 7.263564122926205]\n",
      "[2, 3.888145002595062, 9.310233995455121]\n",
      "[3, 3.9253992739571926, 1.4278312768411199]\n",
      "[4, 0.6556272684978302, 0.7295820838814482]\n",
      "[5, 0.4999029438562869, 0.156081740325744]\n",
      "[6, 0.10937255937222254, 0.11381915471422563]\n",
      "[7, 0.1354616981205771, 0.04687200412325083]\n",
      "[8, 0.09547796265430215, 0.2183945373110815]\n",
      "[9, 0.12337391245544539, 0.14618844600445632]\n",
      "[10, 0.07338362073812658, 0.08963110998546311]\n",
      "[11, 0.07253742514182197, 0.06919823586940765]\n",
      "[12, 0.04735311784492624, 0.04652125000475197]\n",
      "[13, 0.05222686697524889, 0.07957459142038581]\n",
      "[14, 0.05582663718279685, 0.06745984827364804]\n",
      "[15, 0.0602284325104926, 0.1385147816612633]\n",
      "[16, 0.15809513810302844, 0.10265482517830822]\n",
      "[17, 0.06171040538367118, 0.0917477202046355]\n",
      "[18, 0.06420281573568437, 0.13654189276585885]\n",
      "[19, 0.2283831620528292, 0.6673955567386172]\n",
      "[20, 1.2478737919359042, 1.3351452131883814]\n"
     ]
    }
   ],
   "source": [
    "print(\"Training output:\")\n",
    "for i in range(len(loss_list)):\n",
    "    print(loss_list[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model saving"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trained_models/DHR_11100_0.001_0_20_1_20231030-141020.pth\n"
     ]
    }
   ],
   "source": [
    "model_save_path = \"trained_models/\"\n",
    "model_name_to_save = model_save_path + f\"DHR_{model_params.get_model_code()}_{timestamp}.pth\"\n",
    "print(model_name_to_save)\n",
    "torch.save(model.state_dict(), model_name_to_save)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test model (loading and inference)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save results and export metrics to csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = SPmodel = SP_AffineNet().to(device)\n",
    "# print(model)\n",
    "\n",
    "# parameters = model.parameters()\n",
    "# optimizer = optim.Adam(parameters, model_params.learning_rate)\n",
    "# scheduler = optim.lr_scheduler.LambdaLR(optimizer, lambda epoch: model_params.decay_rate ** epoch)\n",
    "\n",
    "# model.load_state_dict(torch.load(model_name_to_save))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing::   0%|          | 0/109 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing:: 100%|██████████| 109/109 [01:36<00:00,  1.12it/s]\n"
     ]
    }
   ],
   "source": [
    "def test(model, model_params, timestamp):\n",
    "    # Set model to training mode\n",
    "    model.eval()\n",
    "\n",
    "    # Create output directory\n",
    "    output_dir = f\"output/{model_params.get_model_code()}_{timestamp}_test\"\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    # Validate model\n",
    "    # validation_loss = 0.0\n",
    "\n",
    "    # create a csv file to store the metrics\n",
    "    csv_file = f\"{output_dir}/metrics.csv\"\n",
    "    with open(csv_file, 'w', newline='') as file:\n",
    "        writer = csv.writer(file)\n",
    "        # matches1_transformed.shape[-1], mse_before, mse12, tre_before, tre12, \\\n",
    "        # mse12_image, ssim12_image, \n",
    "        writer.writerow([\"index\", \"mse_before\", \"mse12\", \"tre_before\", \"tre12\", \"mse12_image_before\", \"mse12_image\", \"ssim12_image_before\", \"ssim12_image\"])\n",
    "\n",
    "    with torch.no_grad():\n",
    "        testbar = tqdm(test_dataset, desc=f'Testing:')\n",
    "        for i, data in enumerate(testbar, 0):\n",
    "            # Get images and affine parameters\n",
    "            if model_params.sup:\n",
    "                source_image, target_image, affine_params_true = data\n",
    "            else:\n",
    "                source_image, target_image = data\n",
    "            source_image = source_image.to(device)\n",
    "            target_image = target_image.to(device)\n",
    "\n",
    "            # Forward pass\n",
    "            outputs = model(source_image, target_image)\n",
    "            # for i in range(len(outputs)):\n",
    "            #     print(i, outputs[i].shape)\n",
    "            transformed_source_affine = outputs[0]\n",
    "            affine_params_predicted = outputs[1]\n",
    "            points1 = outputs[2]\n",
    "            points2 = outputs[3]\n",
    "            points1_affine = np.array(outputs[4])\n",
    "            try:\n",
    "                points1_affine = points1_affine.reshape(points1_affine.shape[2], points1_affine.shape[1])\n",
    "            except:\n",
    "                pass\n",
    "            desc1 = outputs[5]\n",
    "            desc2 = outputs[6]\n",
    "            heatmap1 = outputs[7]\n",
    "            heatmap2 = outputs[8]\n",
    "\n",
    "            if i < 50:\n",
    "                plot_ = True\n",
    "            else:\n",
    "                plot_ = False\n",
    "\n",
    "            results = DL_affine_plot(f\"{i+1}\", output_dir,\n",
    "                f\"{i}\", \"_\", source_image[0, 0, :, :].cpu().numpy(), target_image[0, 0, :, :].cpu().numpy(), \\\n",
    "                transformed_source_affine[0, 0, :, :].cpu().numpy(), \\\n",
    "                points1, points2, points1_affine, desc1, desc2, affine_params_true=affine_params_true,\n",
    "                affine_params_predict=affine_params_predicted, heatmap1=heatmap1, heatmap2=heatmap2, plot=plot_)\n",
    "\n",
    "            # calculate metrics\n",
    "            # matches1_transformed = results[0]\n",
    "            mse_before = results[1]\n",
    "            mse12 = results[2]\n",
    "            tre_before = results[3]\n",
    "            tre12 = results[4]\n",
    "            mse12_image_before = results[5]\n",
    "            mse12_image = results[6]\n",
    "            ssim12_image_before = results[7]\n",
    "            ssim12_image = results[8]\n",
    "\n",
    "            # write metrics to csv file\n",
    "            with open(csv_file, 'a', newline='') as file:\n",
    "                writer = csv.writer(file) # TODO: might need to export true & predicted affine parameters too\n",
    "                writer.writerow([i, mse_before, mse12, tre_before, tre12, mse12_image_before, mse12_image, ssim12_image_before, ssim12_image])\n",
    "\n",
    "    # delete all txt files in output_dir\n",
    "    for file in os.listdir(output_dir):\n",
    "        if file.endswith(\".txt\"):\n",
    "            os.remove(os.path.join(output_dir, file))\n",
    "\n",
    "# timestamp = datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "metrics = test(model, model_params, timestamp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spppt",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
