{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import cv2\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "from skimage.metrics import structural_similarity as ssim\n",
    "from skimage.measure import ransac\n",
    "from skimage.transform import FundamentalMatrixTransform, AffineTransform\n",
    "\n",
    "# Suppress the specific warning\n",
    "import csv\n",
    "from datetime import datetime\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torch\n",
    "from torchvision import transforms\n",
    "from torch import nn, optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils import data\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.transforms import ToTensor\n",
    "\n",
    "from utils.utils0 import *\n",
    "from utils.utils1 import *\n",
    "from utils.utils1 import ModelParams, DL_affine_plot, loss_extra\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f'Device: {device}')\n",
    "\n",
    "# Stub to warn about opencv version.\n",
    "if int(cv2.__version__[0]) < 3: # pragma: no cover\n",
    "  print('Warning: OpenCV 3 is not installed')\n",
    "\n",
    "image_size = 256\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cases, model parameters\n",
    "- Supervised DL w/ groundtruth affine transformation parameters (MSE params, MSE, NCC images)\n",
    "    - Synthetic eye\n",
    "    - Synthetic shape\n",
    "- Unsupervised DL (MSE, NCC images)\n",
    "    - Actual eye data\n",
    "    - Synthetic eye\n",
    "    - Synthetic shape\n",
    "- Data\n",
    "    - only images\n",
    "    - only heatmaps\n",
    "    - images & heatmaps\n",
    "- Loss function\n",
    "    - MSE affine parameters\n",
    "    - MSE, NCC images\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model name:  dataset2_sup0_image0_heatmaps0_loss_image1\n",
      "Model code:  20001_0.001_0_10_1\n",
      "Model params:  {'dataset': 2, 'sup': 0, 'image': 0, 'heatmaps': 0, 'loss_image_case': 1, 'loss_image': NCC(), 'loss_affine': None, 'learning_rate': 0.001, 'decay_rate': 0.96, 'start_epoch': 0, 'num_epochs': 10, 'batch_size': 1, 'model_name': 'dataset2_sup0_image0_heatmaps0_loss_image1'}\n",
      "\n",
      "Model name:  dataset2_sup0_image0_heatmaps0_loss_image1\n",
      "Model code:  20001_0.001_0_10_1\n",
      "Dataset used:  Synthetic shape\n",
      "Supervised or unsupervised model:  Unsupervised\n",
      "Image type:  Image not used\n",
      "Heatmaps used:  Heatmaps not used\n",
      "Loss function case:  1\n",
      "Loss function for image:  NCC()\n",
      "Loss function for affine:  None\n",
      "Learning rate:  0.001\n",
      "Decay rate:  0.96\n",
      "Start epoch:  0\n",
      "Number of epochs:  10\n",
      "Batch size:  1\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model_params = ModelParams(sup=0, dataset=2, image=0, heatmaps=0, \n",
    "                           loss_image=1, num_epochs=10, learning_rate=1e-3)\n",
    "model_params.print_explanation()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Models\n",
    "## SuperPoint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ImgReg Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.SPaffineNet import SP_AffineNet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SP ImgReg model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.datagen import datagen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset: 0, is_train: True, sup: False\n",
      "Training eye dataset\n",
      "Number of training data:  500\n",
      "index, source_img.shape,       target_img.shape\n",
      "0 torch.Size([1, 1, 256, 256]) torch.Size([1, 1, 256, 256])\n",
      "1 torch.Size([1, 1, 256, 256]) torch.Size([1, 1, 256, 256])\n",
      "2 torch.Size([1, 1, 256, 256]) torch.Size([1, 1, 256, 256])\n",
      "\n",
      "\n",
      "dataset: 0, is_train: True, sup: True\n",
      "Training eye dataset\n",
      "Number of training data:  500\n",
      "skipping\n",
      "\n",
      "\n",
      "dataset: 0, is_train: False, sup: False\n",
      "Test eye dataset\n",
      "Number of testing data:  100\n",
      "index, source_img.shape,       target_img.shape\n",
      "0 torch.Size([1, 1, 256, 256]) torch.Size([1, 1, 256, 256])\n",
      "1 torch.Size([1, 1, 256, 256]) torch.Size([1, 1, 256, 256])\n",
      "2 torch.Size([1, 1, 256, 256]) torch.Size([1, 1, 256, 256])\n",
      "\n",
      "\n",
      "dataset: 0, is_train: False, sup: True\n",
      "Test eye dataset\n",
      "Number of testing data:  100\n",
      "skipping\n",
      "\n",
      "\n",
      "dataset: 1, is_train: True, sup: False\n",
      "index, source_img.shape,       target_img.shape\n",
      "0 torch.Size([1, 1, 256, 256]) torch.Size([1, 1, 256, 256])\n",
      "1 torch.Size([1, 1, 256, 256]) torch.Size([1, 1, 256, 256])\n",
      "2 torch.Size([1, 1, 256, 256]) torch.Size([1, 1, 256, 256])\n",
      "\n",
      "\n",
      "dataset: 1, is_train: True, sup: True\n",
      "index, source_img.shape,       target_img.shape\n",
      "index, source_img.shape,       target_img.shape,            affine_params.shape\n",
      "0 torch.Size([1, 1, 256, 256]) torch.Size([1, 1, 256, 256]) torch.Size([1, 2, 3])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 torch.Size([1, 1, 256, 256]) torch.Size([1, 1, 256, 256]) torch.Size([1, 2, 3])\n",
      "2 torch.Size([1, 1, 256, 256]) torch.Size([1, 1, 256, 256]) torch.Size([1, 2, 3])\n",
      "3 torch.Size([1, 1, 256, 256]) torch.Size([1, 1, 256, 256]) torch.Size([1, 2, 3])\n",
      "4 torch.Size([1, 1, 256, 256]) torch.Size([1, 1, 256, 256]) torch.Size([1, 2, 3])\n",
      "5 torch.Size([1, 1, 256, 256]) torch.Size([1, 1, 256, 256]) torch.Size([1, 2, 3])\n",
      "\n",
      "\n",
      "dataset: 1, is_train: False, sup: False\n",
      "index, source_img.shape,       target_img.shape\n",
      "0 torch.Size([1, 1, 256, 256]) torch.Size([1, 1, 256, 256])\n",
      "1 torch.Size([1, 1, 256, 256]) torch.Size([1, 1, 256, 256])\n",
      "2 torch.Size([1, 1, 256, 256]) torch.Size([1, 1, 256, 256])\n",
      "\n",
      "\n",
      "dataset: 1, is_train: False, sup: True\n",
      "index, source_img.shape,       target_img.shape\n",
      "index, source_img.shape,       target_img.shape,            affine_params.shape\n",
      "0 torch.Size([1, 1, 256, 256]) torch.Size([1, 1, 256, 256]) torch.Size([1, 2, 3])\n",
      "1 torch.Size([1, 1, 256, 256]) torch.Size([1, 1, 256, 256]) torch.Size([1, 2, 3])\n",
      "2 torch.Size([1, 1, 256, 256]) torch.Size([1, 1, 256, 256]) torch.Size([1, 2, 3])\n",
      "3 torch.Size([1, 1, 256, 256]) torch.Size([1, 1, 256, 256]) torch.Size([1, 2, 3])\n",
      "4 torch.Size([1, 1, 256, 256]) torch.Size([1, 1, 256, 256]) torch.Size([1, 2, 3])\n",
      "5 torch.Size([1, 1, 256, 256]) torch.Size([1, 1, 256, 256]) torch.Size([1, 2, 3])\n",
      "\n",
      "\n",
      "dataset: 2, is_train: True, sup: False\n",
      "index, source_img.shape,       target_img.shape\n",
      "0 torch.Size([1, 1, 256, 256]) torch.Size([1, 1, 256, 256])\n",
      "1 torch.Size([1, 1, 256, 256]) torch.Size([1, 1, 256, 256])\n",
      "2 torch.Size([1, 1, 256, 256]) torch.Size([1, 1, 256, 256])\n",
      "\n",
      "\n",
      "dataset: 2, is_train: True, sup: True\n",
      "index, source_img.shape,       target_img.shape\n",
      "index, source_img.shape,       target_img.shape,            affine_params.shape\n",
      "0 torch.Size([1, 1, 256, 256]) torch.Size([1, 1, 256, 256]) torch.Size([1, 2, 3])\n",
      "1 torch.Size([1, 1, 256, 256]) torch.Size([1, 1, 256, 256]) torch.Size([1, 2, 3])\n",
      "2 torch.Size([1, 1, 256, 256]) torch.Size([1, 1, 256, 256]) torch.Size([1, 2, 3])\n",
      "3 torch.Size([1, 1, 256, 256]) torch.Size([1, 1, 256, 256]) torch.Size([1, 2, 3])\n",
      "4 torch.Size([1, 1, 256, 256]) torch.Size([1, 1, 256, 256]) torch.Size([1, 2, 3])\n",
      "5 torch.Size([1, 1, 256, 256]) torch.Size([1, 1, 256, 256]) torch.Size([1, 2, 3])\n",
      "\n",
      "\n",
      "dataset: 2, is_train: False, sup: False\n",
      "index, source_img.shape,       target_img.shape\n",
      "0 torch.Size([1, 1, 256, 256]) torch.Size([1, 1, 256, 256])\n",
      "1 torch.Size([1, 1, 256, 256]) torch.Size([1, 1, 256, 256])\n",
      "2 torch.Size([1, 1, 256, 256]) torch.Size([1, 1, 256, 256])\n",
      "\n",
      "\n",
      "dataset: 2, is_train: False, sup: True\n",
      "index, source_img.shape,       target_img.shape\n",
      "index, source_img.shape,       target_img.shape,            affine_params.shape\n",
      "0 torch.Size([1, 1, 256, 256]) torch.Size([1, 1, 256, 256]) torch.Size([1, 2, 3])\n",
      "1 torch.Size([1, 1, 256, 256]) torch.Size([1, 1, 256, 256]) torch.Size([1, 2, 3])\n",
      "2 torch.Size([1, 1, 256, 256]) torch.Size([1, 1, 256, 256]) torch.Size([1, 2, 3])\n",
      "3 torch.Size([1, 1, 256, 256]) torch.Size([1, 1, 256, 256]) torch.Size([1, 2, 3])\n",
      "4 torch.Size([1, 1, 256, 256]) torch.Size([1, 1, 256, 256]) torch.Size([1, 2, 3])\n",
      "5 torch.Size([1, 1, 256, 256]) torch.Size([1, 1, 256, 256]) torch.Size([1, 2, 3])\n",
      "\n",
      "\n",
      "dataset: 3, is_train: True, sup: False\n",
      "index, source_img.shape,       target_img.shape\n",
      "0 torch.Size([1, 1, 28, 28]) torch.Size([1])\n",
      "1 torch.Size([1, 1, 28, 28]) torch.Size([1])\n",
      "2 torch.Size([1, 1, 28, 28]) torch.Size([1])\n",
      "\n",
      "\n",
      "dataset: 3, is_train: True, sup: True\n",
      "index, source_img.shape,       target_img.shape\n",
      "0 torch.Size([1, 1, 28, 28]) torch.Size([1])\n",
      "1 torch.Size([1, 1, 28, 28]) torch.Size([1])\n",
      "2 torch.Size([1, 1, 28, 28]) torch.Size([1])\n",
      "\n",
      "\n",
      "dataset: 3, is_train: False, sup: False\n",
      "index, source_img.shape,       target_img.shape\n",
      "0 torch.Size([1, 1, 28, 28]) torch.Size([1])\n",
      "1 torch.Size([1, 1, 28, 28]) torch.Size([1])\n",
      "2 torch.Size([1, 1, 28, 28]) torch.Size([1])\n",
      "\n",
      "\n",
      "dataset: 3, is_train: False, sup: True\n",
      "index, source_img.shape,       target_img.shape\n",
      "0 torch.Size([1, 1, 28, 28]) torch.Size([1])\n",
      "1 torch.Size([1, 1, 28, 28]) torch.Size([1])\n",
      "2 torch.Size([1, 1, 28, 28]) torch.Size([1])\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# test datagen for all datasets and training and testing\n",
    "for dataset in range(4): # don't forget to change this back to 2\n",
    "    for is_train in [True, False]:\n",
    "        for sup in [False, True]:\n",
    "            print(f'dataset: {dataset}, is_train: {is_train}, sup: {sup}')\n",
    "            dataloader = datagen(dataset, is_train, sup)\n",
    "            \n",
    "            if sup==1 and dataset==0:\n",
    "                print('skipping')\n",
    "                pass\n",
    "            else:\n",
    "                try:\n",
    "                    print('index, source_img.shape,       target_img.shape')\n",
    "                    for i, (source_img, target_img) in enumerate(dataloader):\n",
    "                        print(i, source_img.shape, target_img.shape)\n",
    "                        if i == 2:\n",
    "                            break\n",
    "                except ValueError:\n",
    "                    print('index, source_img.shape,       target_img.shape,            affine_params.shape')\n",
    "                    for i, batch in enumerate(dataloader):\n",
    "                        print(i, batch[0].shape, batch[1].shape, batch[2].shape)\n",
    "                        if i == 5:\n",
    "                            break\n",
    "            print('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training\n",
    "## Dataset initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set:  [torch.Size([1, 1, 256, 256]), torch.Size([1, 1, 256, 256])]\n",
      "Test set:  [torch.Size([1, 1, 256, 256]), torch.Size([1, 1, 256, 256])]\n"
     ]
    }
   ],
   "source": [
    "train_dataset = datagen(model_params.dataset, True, model_params.sup)\n",
    "test_dataset = datagen(model_params.dataset, False, model_params.sup)\n",
    "\n",
    "# Get sample batch\n",
    "print('Train set: ', [x.shape for x in next(iter(train_dataset))])\n",
    "print('Test set: ', [x.shape for x in next(iter(test_dataset))])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model initialize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset2_sup0_image0_heatmaps0_loss_image1\n",
      "\n",
      "Model name:  dataset2_sup0_image0_heatmaps0_loss_image1\n",
      "Model code:  20001_0.001_0_10_1\n",
      "Dataset used:  Synthetic shape\n",
      "Supervised or unsupervised model:  Unsupervised\n",
      "Image type:  Image not used\n",
      "Heatmaps used:  Heatmaps not used\n",
      "Loss function case:  1\n",
      "Loss function for image:  NCC()\n",
      "Loss function for affine:  None\n",
      "Learning rate:  0.001\n",
      "Decay rate:  0.96\n",
      "Start epoch:  0\n",
      "Number of epochs:  10\n",
      "Batch size:  1\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# print case\n",
    "print(model_params)\n",
    "model_params.print_explanation()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Running new version (not run SP on source image)\n",
      "SP_AffineNet(\n",
      "  (affineNet): AffineNet(\n",
      "    (conv1): Conv2d(1, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (conv2): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (conv3): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (conv4): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (conv5): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (conv1s): Conv2d(64, 64, kernel_size=(2, 2), stride=(2, 2))\n",
      "    (conv2s): Conv2d(128, 128, kernel_size=(2, 2), stride=(2, 2))\n",
      "    (conv3s): Conv2d(256, 256, kernel_size=(2, 2), stride=(2, 2))\n",
      "    (conv4s): Conv2d(512, 512, kernel_size=(2, 2), stride=(2, 2))\n",
      "    (fc1): Linear(in_features=1024, out_features=128, bias=True)\n",
      "    (fc2): Linear(in_features=128, out_features=64, bias=True)\n",
      "    (fc3): Linear(in_features=64, out_features=32, bias=True)\n",
      "    (fc4): Linear(in_features=32, out_features=6, bias=True)\n",
      "    (dropout): Dropout(p=0.7, inplace=False)\n",
      "    (aPooling): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "    (ReLU): LeakyReLU(negative_slope=0.01)\n",
      "    (Act1): GroupNorm(32, 64, eps=1e-05, affine=True)\n",
      "    (Act2): GroupNorm(64, 128, eps=1e-05, affine=True)\n",
      "    (Act3): GroupNorm(128, 256, eps=1e-05, affine=True)\n",
      "    (Act4): GroupNorm(256, 512, eps=1e-05, affine=True)\n",
      "    (Act5): GroupNorm(256, 512, eps=1e-05, affine=True)\n",
      "  )\n",
      ")\n",
      "Loaded model from trained_models/10102_0.001_0_20_1_20230930-091532.pth\n",
      "starting at epoch 20\n"
     ]
    }
   ],
   "source": [
    "model = SP_AffineNet(model_params).to(device)\n",
    "print(model)\n",
    "\n",
    "parameters = model.parameters()\n",
    "optimizer = optim.Adam(parameters, model_params.learning_rate)\n",
    "scheduler = optim.lr_scheduler.LambdaLR(optimizer, lambda epoch: model_params.decay_rate ** epoch)\n",
    "model_path = 'trained_models/10102_0.001_0_20_1_20230930-091532.pth'\n",
    "\n",
    "# if a model is loaded, the training will continue from the epoch it was saved at\n",
    "try:\n",
    "    model.load_state_dict(torch.load(model_path))\n",
    "    model_params.start_epoch = int(model_path.split('/')[-1].split('_')[3])\n",
    "    print(f'Loaded model from {model_path}\\nstarting at epoch {model_params.start_epoch}')\n",
    "    if model_params.start_epoch >= model_params.num_epochs:\n",
    "            model_params.num_epochs += model_params.start_epoch\n",
    "except:\n",
    "    model_params.start_epoch = 0\n",
    "    print('No model loaded, starting from scratch')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define training function\n",
    "def train(model, model_params, timestamp):\n",
    "    # Define loss function based on supervised or unsupervised learning\n",
    "    criterion = model_params.loss_image\n",
    "    # extra = loss_extra()\n",
    "\n",
    "    if model_params.sup:\n",
    "        criterion_affine = nn.MSELoss()\n",
    "        # TODO: add loss for points1_affine and points2, Euclidean distance\n",
    "\n",
    "    # Define optimizer\n",
    "    optimizer = optim.Adam(model.parameters(), lr=model_params.learning_rate)\n",
    "\n",
    "    # Create empty list to store epoch number, train loss and validation loss\n",
    "    epoch_loss_list = []\n",
    "    running_loss_list = []\n",
    "    \n",
    "    # Create output directory\n",
    "    output_dir = f\"output/{model_params.get_model_code()}_{timestamp}\"\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    # Train model\n",
    "    for epoch in range(model_params.start_epoch, model_params.num_epochs):\n",
    "        # Set model to training mode\n",
    "        model.train()\n",
    "\n",
    "        # Zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        running_loss = 0.0\n",
    "        train_bar = tqdm(train_dataset, desc=f'Training Epoch {epoch+1}/{model_params.num_epochs}')\n",
    "        for i, data in enumerate(train_bar):\n",
    "            # Get images and affine parameters\n",
    "            if model_params.sup:\n",
    "                source_image, target_image, affine_params_true = data\n",
    "            else:\n",
    "                source_image, target_image = data\n",
    "                affine_params_true = None\n",
    "            source_image = source_image.to(device)\n",
    "            target_image = target_image.to(device)\n",
    "\n",
    "            # Forward + backward + optimize\n",
    "            outputs = model(source_image, target_image)\n",
    "            # for i in range(len(outputs)):\n",
    "            #         print(i, outputs[i].shape)\n",
    "            # 0 torch.Size([1, 1, 256, 256])\n",
    "            # 1 torch.Size([1, 2, 3])\n",
    "            # 2 (2, 4)\n",
    "            # 3 (2, 4)\n",
    "            # 4 (1, 4, 2)\n",
    "            # 5 (256, 9)\n",
    "            # 6 (256, 16)\n",
    "            # 7 (256, 256)\n",
    "            # 8 (256, 256)\n",
    "            transformed_source_affine = outputs[0] # image\n",
    "            affine_params_predicted = outputs[1] # affine parameters\n",
    "            points1 = outputs[2]\n",
    "            points2 = outputs[3]\n",
    "            points1_affine = np.array(outputs[4])\n",
    "\n",
    "            # print(f\"affine_params_true: {affine_params_true}\")\n",
    "            # print(f\"affine_params_predicted: {affine_params_predicted}\\n\")\n",
    "\n",
    "            try:\n",
    "                points1_affine = points1_affine.reshape(points1_affine.shape[2], points1_affine.shape[1])\n",
    "            except:\n",
    "                pass\n",
    "            desc1 = outputs[5]\n",
    "            desc2 = outputs[6]\n",
    "            heatmap1 = outputs[7]\n",
    "            heatmap2 = outputs[8]\n",
    "\n",
    "            loss = criterion(transformed_source_affine, target_image)\n",
    "            # loss += extra(affine_params_predicted)\n",
    "            if model_params.sup:\n",
    "                loss_affine = criterion_affine(affine_params_true.view(1, 2, 3), affine_params_predicted.cpu())\n",
    "                # TODO: add loss for points1_affine and points2, Euclidean distance\n",
    "                # loss_points = criterion_points(points1_affine, points2)\n",
    "                loss += loss_affine\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "            \n",
    "            # Plot images if i < 5\n",
    "            if i < 5:\n",
    "                DL_affine_plot(f\"epoch{epoch+1}_train\", output_dir,\n",
    "                    f\"{i}\", \"_\", source_image[0, 0, :, :].detach().cpu().numpy(), \n",
    "                    target_image[0, 0, :, :].detach().cpu().numpy(), \n",
    "                    transformed_source_affine[0, 0, :, :].detach().cpu().numpy(),\n",
    "                    points1, points2, points1_affine, desc1, desc2, affine_params_true=affine_params_true,\n",
    "                    affine_params_predict=affine_params_predicted, heatmap1=heatmap1, heatmap2=heatmap2, plot=True)\n",
    "\n",
    "            # Print statistics\n",
    "            running_loss += loss.item()\n",
    "            running_loss_list.append([epoch+((i+1)/len(train_dataset)), loss.item()])\n",
    "            train_bar.set_postfix({'loss': running_loss / (i+1)})\n",
    "        print(f'Training Epoch {epoch+1}/{model_params.num_epochs} loss: {running_loss / len(train_dataset)}')\n",
    "\n",
    "        # Validate model\n",
    "        validation_loss = 0.0\n",
    "        model.eval()\n",
    "        # with torch.no_grad():\n",
    "        for i, data in enumerate(test_dataset, 0):\n",
    "            # Get images and affine parameters\n",
    "            if model_params.sup:\n",
    "                source_image, target_image, affine_params_true = data\n",
    "            else:\n",
    "                source_image, target_image = data\n",
    "                affine_params_true = None\n",
    "            source_image = source_image.to(device)\n",
    "            target_image = target_image.to(device)\n",
    "\n",
    "            # Forward pass\n",
    "            outputs = model(source_image, target_image)\n",
    "            # for i in range(len(outputs)):\n",
    "            #     print(i, outputs[i].shape)\n",
    "            transformed_source_affine = outputs[0]\n",
    "            affine_params_predicted = outputs[1]\n",
    "            points1 = outputs[2]\n",
    "            points2 = outputs[3]\n",
    "            points1_affine = np.array(outputs[4])\n",
    "            try:\n",
    "                points1_affine = points1_affine.reshape(points1_affine.shape[2], points1_affine.shape[1])\n",
    "            except:\n",
    "                pass\n",
    "            desc1 = outputs[5]\n",
    "            desc2 = outputs[6]\n",
    "            heatmap1 = outputs[7]\n",
    "            heatmap2 = outputs[8]\n",
    "\n",
    "            loss = criterion(transformed_source_affine, target_image)\n",
    "            # loss += extra(affine_params_predicted)\n",
    "            if model_params.sup:\n",
    "                loss_affine = criterion_affine(affine_params_true.view(1, 2, 3), affine_params_predicted.cpu())\n",
    "                # TODO: add loss for points1_affine and points2, Euclidean distance\n",
    "                # loss_points = criterion_points(points1_affine, points2)\n",
    "                loss += loss_affine\n",
    "\n",
    "            # Add to validation loss\n",
    "            validation_loss += loss.item()\n",
    "\n",
    "            # Plot images if i < 5\n",
    "            if i < 5:\n",
    "                DL_affine_plot(f\"epoch{epoch+1}_valid\", output_dir,\n",
    "                    f\"{i}\", \"_\", source_image[0, 0, :, :].detach().cpu().numpy(), \n",
    "                    target_image[0, 0, :, :].detach().cpu().numpy(), \n",
    "                    transformed_source_affine[0, 0, :, :].detach().cpu().numpy(),\n",
    "                    points1, points2, points1_affine, desc1, desc2, affine_params_true=affine_params_true,\n",
    "                    affine_params_predict=affine_params_predicted, heatmap1=heatmap1, heatmap2=heatmap2, plot=True)\n",
    "\n",
    "        # Print validation statistics\n",
    "        validation_loss /= len(test_dataset)\n",
    "        print(f'Validation Epoch {epoch+1}/{model_params.num_epochs} loss: {validation_loss}')\n",
    "\n",
    "        # Append epoch number, train loss and validation loss to epoch_loss_list\n",
    "        epoch_loss_list.append([epoch+1, running_loss / len(train_dataset), validation_loss])\n",
    "\n",
    "        \n",
    "        # Extract epoch number, train loss and validation loss from epoch_loss_list\n",
    "        epoch = [x[0] for x in epoch_loss_list]\n",
    "        train_loss = [x[1] for x in epoch_loss_list]\n",
    "        val_loss = [x[2] for x in epoch_loss_list]\n",
    "        step = [x[0] for x in running_loss_list]\n",
    "        running_train_loss = [x[1] for x in running_loss_list]\n",
    "\n",
    "        save_plot_name = f\"{output_dir}/loss_{model_params.get_model_code()}_epoch{model_params.num_epochs}_{timestamp}.png\"\n",
    "\n",
    "        # Plot train loss and validation loss against epoch number\n",
    "        plt.figure()\n",
    "        plt.plot(step, running_train_loss, label='Running Train Loss', alpha=0.3)\n",
    "        plt.plot(epoch, train_loss, label='Train Loss')\n",
    "        plt.plot(epoch, val_loss, label='Validation Loss')\n",
    "        plt.title('Train and Validation Loss')\n",
    "        plt.legend()\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.ylabel('Loss')\n",
    "        plt.yscale('log')\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(save_plot_name)\n",
    "        plt.close()\n",
    "        \n",
    "    print('Finished Training')\n",
    "\n",
    "    # delete all txt files in output_dir\n",
    "    for file in os.listdir(output_dir):\n",
    "        if file.endswith(\".txt\"):\n",
    "            os.remove(os.path.join(output_dir, file))\n",
    "\n",
    "    # Return epoch_loss_list\n",
    "    return epoch_loss_list\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## working code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 21/30:   0%|          | 0/360 [00:00<?, ?it/s]/home/pkhamchuai/codes/spppt-1/utils/utils1.py:712: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  torch.tensor(M).view(1, 2, 3))\n",
      "/home/pkhamchuai/miniconda3/envs/spppt/lib/python3.8/site-packages/torch/nn/functional.py:4298: UserWarning: Default grid_sample and affine_grid behavior has changed to align_corners=False since 1.3.0. Please specify align_corners=True if the old behavior is desired. See the documentation of grid_sample for details.\n",
      "  warnings.warn(\n",
      "/home/pkhamchuai/miniconda3/envs/spppt/lib/python3.8/site-packages/torch/functional.py:504: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at ../aten/src/ATen/native/TensorShape.cpp:3483.)\n",
      "  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n",
      "/home/pkhamchuai/miniconda3/envs/spppt/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:139: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\"Detected call of `lr_scheduler.step()` before `optimizer.step()`. \"\n",
      "Training Epoch 21/30: 100%|██████████| 360/360 [00:53<00:00,  6.73it/s, loss=-.0267]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch 21/30 loss: -0.02668601806728677\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/pkhamchuai/codes/spppt-1/utils/utils1.py:250: RuntimeWarning: invalid value encountered in divide\n",
      "  image = ((image - image.min()) / (image.max() - image.min()) * 255).astype(np.uint8)\n",
      "/home/pkhamchuai/codes/spppt-1/utils/utils1.py:250: RuntimeWarning: invalid value encountered in cast\n",
      "  image = ((image - image.min()) / (image.max() - image.min()) * 255).astype(np.uint8)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Epoch 21/30 loss: 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 22/30: 100%|██████████| 360/360 [00:49<00:00,  7.25it/s, loss=-2.14e-5]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch 22/30 loss: -2.1448024522720113e-05\n",
      "Validation Epoch 22/30 loss: 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 23/30: 100%|██████████| 360/360 [00:49<00:00,  7.23it/s, loss=2.07e-6]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch 23/30 loss: 2.066797848480443e-06\n",
      "Validation Epoch 23/30 loss: 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 24/30:   3%|▎         | 12/360 [00:08<03:57,  1.47it/s, loss=0]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/home/pkhamchuai/codes/spppt-1/DL_template5.ipynb Cell 21\u001b[0m line \u001b[0;36m2\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu/home/pkhamchuai/codes/spppt-1/DL_template5.ipynb#X26sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m timestamp \u001b[39m=\u001b[39m datetime\u001b[39m.\u001b[39mnow()\u001b[39m.\u001b[39mstrftime(\u001b[39m\"\u001b[39m\u001b[39m%\u001b[39m\u001b[39mY\u001b[39m\u001b[39m%\u001b[39m\u001b[39mm\u001b[39m\u001b[39m%d\u001b[39;00m\u001b[39m-\u001b[39m\u001b[39m%\u001b[39m\u001b[39mH\u001b[39m\u001b[39m%\u001b[39m\u001b[39mM\u001b[39m\u001b[39m%\u001b[39m\u001b[39mS\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m----> <a href='vscode-notebook-cell://wsl%2Bubuntu/home/pkhamchuai/codes/spppt-1/DL_template5.ipynb#X26sdnNjb2RlLXJlbW90ZQ%3D%3D?line=1'>2</a>\u001b[0m loss_list \u001b[39m=\u001b[39m train(model, model_params, timestamp)\n",
      "\u001b[1;32m/home/pkhamchuai/codes/spppt-1/DL_template5.ipynb Cell 21\u001b[0m line \u001b[0;36m4\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu/home/pkhamchuai/codes/spppt-1/DL_template5.ipynb#X26sdnNjb2RlLXJlbW90ZQ%3D%3D?line=39'>40</a>\u001b[0m target_image \u001b[39m=\u001b[39m target_image\u001b[39m.\u001b[39mto(device)\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu/home/pkhamchuai/codes/spppt-1/DL_template5.ipynb#X26sdnNjb2RlLXJlbW90ZQ%3D%3D?line=41'>42</a>\u001b[0m \u001b[39m# Forward + backward + optimize\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell://wsl%2Bubuntu/home/pkhamchuai/codes/spppt-1/DL_template5.ipynb#X26sdnNjb2RlLXJlbW90ZQ%3D%3D?line=42'>43</a>\u001b[0m outputs \u001b[39m=\u001b[39m model(source_image, target_image)\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu/home/pkhamchuai/codes/spppt-1/DL_template5.ipynb#X26sdnNjb2RlLXJlbW90ZQ%3D%3D?line=43'>44</a>\u001b[0m \u001b[39m# for i in range(len(outputs)):\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu/home/pkhamchuai/codes/spppt-1/DL_template5.ipynb#X26sdnNjb2RlLXJlbW90ZQ%3D%3D?line=44'>45</a>\u001b[0m \u001b[39m#         print(i, outputs[i].shape)\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu/home/pkhamchuai/codes/spppt-1/DL_template5.ipynb#X26sdnNjb2RlLXJlbW90ZQ%3D%3D?line=45'>46</a>\u001b[0m \u001b[39m# 0 torch.Size([1, 1, 256, 256])\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu/home/pkhamchuai/codes/spppt-1/DL_template5.ipynb#X26sdnNjb2RlLXJlbW90ZQ%3D%3D?line=52'>53</a>\u001b[0m \u001b[39m# 7 (256, 256)\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu/home/pkhamchuai/codes/spppt-1/DL_template5.ipynb#X26sdnNjb2RlLXJlbW90ZQ%3D%3D?line=53'>54</a>\u001b[0m \u001b[39m# 8 (256, 256)\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu/home/pkhamchuai/codes/spppt-1/DL_template5.ipynb#X26sdnNjb2RlLXJlbW90ZQ%3D%3D?line=54'>55</a>\u001b[0m transformed_source_affine \u001b[39m=\u001b[39m outputs[\u001b[39m0\u001b[39m] \u001b[39m# image\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/spppt/lib/python3.8/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/codes/spppt-1/utils/SPaffineNet.py:69\u001b[0m, in \u001b[0;36mSP_AffineNet.forward\u001b[0;34m(self, source_image, target_image)\u001b[0m\n\u001b[1;32m     59\u001b[0m matches2 \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39marray(points2[:\u001b[39m2\u001b[39m, matches[\u001b[39m1\u001b[39m, :]\u001b[39m.\u001b[39mastype(\u001b[39mint\u001b[39m)])\n\u001b[1;32m     60\u001b[0m \u001b[39m# matches1_2 = np.array(points1_2[:2, matches[0, :].astype(int)])\u001b[39;00m\n\u001b[1;32m     61\u001b[0m \u001b[39m# print('matches1', matches1)\u001b[39;00m\n\u001b[1;32m     62\u001b[0m \u001b[39m# print('matches2', matches2)\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     67\u001b[0m \u001b[39m# except:\u001b[39;00m\n\u001b[1;32m     68\u001b[0m \u001b[39m# print(affine_params.cpu().detach().shape, transformed_source_affine.shape)\u001b[39;00m\n\u001b[0;32m---> 69\u001b[0m matches1_2 \u001b[39m=\u001b[39m transform_points_DVF(torch\u001b[39m.\u001b[39;49mtensor(matches1), \n\u001b[1;32m     70\u001b[0m                 affine_params\u001b[39m.\u001b[39;49mcpu()\u001b[39m.\u001b[39;49mdetach(), transformed_source_affine)\n\u001b[1;32m     72\u001b[0m \u001b[39m# transform the points using the affine parameters\u001b[39;00m\n\u001b[1;32m     73\u001b[0m \u001b[39m# matches1_transformed = transform_points(matches1.T[None, :, :], affine_params.cpu().detach())\u001b[39;00m\n\u001b[1;32m     74\u001b[0m \u001b[39mreturn\u001b[39;00m transformed_source_affine, affine_params, matches1, matches2, matches1_2, \\\n\u001b[1;32m     75\u001b[0m     desc1_2, desc2, heatmap1_2, heatmap2\n",
      "File \u001b[0;32m~/codes/spppt-1/utils/utils1.py:710\u001b[0m, in \u001b[0;36mtransform_points_DVF\u001b[0;34m(points, M, image)\u001b[0m\n\u001b[1;32m    705\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mtransform_points_DVF\u001b[39m(points, M, image):\n\u001b[1;32m    706\u001b[0m     \u001b[39m# transform points using displacement field\u001b[39;00m\n\u001b[1;32m    707\u001b[0m     \u001b[39m# DVF.shape = (2, H, W)\u001b[39;00m\n\u001b[1;32m    708\u001b[0m     \u001b[39m# points.shape = (2, N)\u001b[39;00m\n\u001b[1;32m    709\u001b[0m     displacement_field \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mzeros(image\u001b[39m.\u001b[39mshape[\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m], image\u001b[39m.\u001b[39mshape[\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m])\n\u001b[0;32m--> 710\u001b[0m     DVF \u001b[39m=\u001b[39m transform_to_displacement_field(\n\u001b[1;32m    711\u001b[0m         displacement_field\u001b[39m.\u001b[39;49mview(\u001b[39m1\u001b[39;49m, \u001b[39m1\u001b[39;49m, displacement_field\u001b[39m.\u001b[39;49msize(\u001b[39m0\u001b[39;49m), displacement_field\u001b[39m.\u001b[39;49msize(\u001b[39m1\u001b[39;49m)), \n\u001b[1;32m    712\u001b[0m         torch\u001b[39m.\u001b[39;49mtensor(M)\u001b[39m.\u001b[39;49mview(\u001b[39m1\u001b[39;49m, \u001b[39m2\u001b[39;49m, \u001b[39m3\u001b[39;49m))\n\u001b[1;32m    713\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(DVF, torch\u001b[39m.\u001b[39mTensor):\n\u001b[1;32m    714\u001b[0m         DVF \u001b[39m=\u001b[39m DVF\u001b[39m.\u001b[39mnumpy()\n",
      "File \u001b[0;32m~/codes/spppt-1/utils/utils0.py:476\u001b[0m, in \u001b[0;36mtransform_to_displacement_field\u001b[0;34m(tensor, tensor_transform, device)\u001b[0m\n\u001b[1;32m    474\u001b[0m gy \u001b[39m=\u001b[39m gy\u001b[39m.\u001b[39mtype(torch\u001b[39m.\u001b[39mFloatTensor)\u001b[39m.\u001b[39mto(device)\n\u001b[1;32m    475\u001b[0m gx \u001b[39m=\u001b[39m gx\u001b[39m.\u001b[39mtype(torch\u001b[39m.\u001b[39mFloatTensor)\u001b[39m.\u001b[39mto(device)\n\u001b[0;32m--> 476\u001b[0m grid_x \u001b[39m=\u001b[39m (gx \u001b[39m/\u001b[39;49m (x_size \u001b[39m-\u001b[39;49m \u001b[39m1\u001b[39;49m) \u001b[39m-\u001b[39;49m \u001b[39m0.5\u001b[39;49m) \u001b[39m*\u001b[39m \u001b[39m2\u001b[39m\n\u001b[1;32m    477\u001b[0m grid_y \u001b[39m=\u001b[39m (gy \u001b[39m/\u001b[39m (y_size \u001b[39m-\u001b[39m \u001b[39m1\u001b[39m) \u001b[39m-\u001b[39m \u001b[39m0.5\u001b[39m) \u001b[39m*\u001b[39m \u001b[39m2\u001b[39m\n\u001b[1;32m    478\u001b[0m u_x \u001b[39m=\u001b[39m deformation_field[\u001b[39m0\u001b[39m, :, :, \u001b[39m0\u001b[39m] \u001b[39m-\u001b[39m grid_x\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "timestamp = datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "loss_list = train(model, model_params, timestamp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training output:\n",
      "[21, -0.5568007857900941, -0.6026321995537728]\n",
      "[22, -0.5562881386217972, -0.5906273022526876]\n",
      "[23, -0.5571336048280096, -0.6006967229652218]\n",
      "[24, -0.5556762931040592, -0.5950118253007531]\n",
      "[25, -0.5550380393810984, -0.5957967696711421]\n"
     ]
    }
   ],
   "source": [
    "print(\"Training output:\")\n",
    "for i in range(len(loss_list)):\n",
    "    print(loss_list[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model saving"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trained_models/21101_0.001_20_25_1_20231018-220831.pth\n"
     ]
    }
   ],
   "source": [
    "model_save_path = \"trained_models/\"\n",
    "model_name_to_save = model_save_path + f\"model5_{model_params.get_model_code()}_{timestamp}.pth\"\n",
    "print(model_name_to_save)\n",
    "torch.save(model.state_dict(), model_name_to_save)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test model (loading and inference)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save results and export metrics to csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = SPmodel = SP_AffineNet().to(device)\n",
    "# print(model)\n",
    "\n",
    "# parameters = model.parameters()\n",
    "# optimizer = optim.Adam(parameters, model_params.learning_rate)\n",
    "# scheduler = optim.lr_scheduler.LambdaLR(optimizer, lambda epoch: model_params.decay_rate ** epoch)\n",
    "\n",
    "# model.load_state_dict(torch.load(model_name_to_save))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing::   0%|          | 0/40 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "DL_affine_plot() got an unexpected keyword argument 'affine_params'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/home/pkhamchuai/codes/spppt-1/DL_template.ipynb Cell 28\u001b[0m line \u001b[0;36m8\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu/home/pkhamchuai/codes/spppt-1/DL_template.ipynb#X42sdnNjb2RlLXJlbW90ZQ%3D%3D?line=78'>79</a>\u001b[0m             os\u001b[39m.\u001b[39mremove(os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mjoin(output_dir, file))\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu/home/pkhamchuai/codes/spppt-1/DL_template.ipynb#X42sdnNjb2RlLXJlbW90ZQ%3D%3D?line=80'>81</a>\u001b[0m \u001b[39m# timestamp = datetime.now().strftime(\"%Y%m%d-%H%M%S\")\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell://wsl%2Bubuntu/home/pkhamchuai/codes/spppt-1/DL_template.ipynb#X42sdnNjb2RlLXJlbW90ZQ%3D%3D?line=81'>82</a>\u001b[0m metrics \u001b[39m=\u001b[39m test(model, model_params, timestamp)\n",
      "\u001b[1;32m/home/pkhamchuai/codes/spppt-1/DL_template.ipynb Cell 28\u001b[0m line \u001b[0;36m5\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu/home/pkhamchuai/codes/spppt-1/DL_template.ipynb#X42sdnNjb2RlLXJlbW90ZQ%3D%3D?line=50'>51</a>\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu/home/pkhamchuai/codes/spppt-1/DL_template.ipynb#X42sdnNjb2RlLXJlbW90ZQ%3D%3D?line=51'>52</a>\u001b[0m     plot_ \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell://wsl%2Bubuntu/home/pkhamchuai/codes/spppt-1/DL_template.ipynb#X42sdnNjb2RlLXJlbW90ZQ%3D%3D?line=53'>54</a>\u001b[0m results \u001b[39m=\u001b[39m DL_affine_plot(\u001b[39mf\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39m{\u001b[39;49;00mi\u001b[39m+\u001b[39;49m\u001b[39m1\u001b[39;49m\u001b[39m}\u001b[39;49;00m\u001b[39m\"\u001b[39;49m, output_dir,\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu/home/pkhamchuai/codes/spppt-1/DL_template.ipynb#X42sdnNjb2RlLXJlbW90ZQ%3D%3D?line=54'>55</a>\u001b[0m     \u001b[39mf\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39m{\u001b[39;49;00mi\u001b[39m}\u001b[39;49;00m\u001b[39m\"\u001b[39;49m, \u001b[39m\"\u001b[39;49m\u001b[39m_\u001b[39;49m\u001b[39m\"\u001b[39;49m, source_image[\u001b[39m0\u001b[39;49m, \u001b[39m0\u001b[39;49m, :, :]\u001b[39m.\u001b[39;49mcpu()\u001b[39m.\u001b[39;49mnumpy(), target_image[\u001b[39m0\u001b[39;49m, \u001b[39m0\u001b[39;49m, :, :]\u001b[39m.\u001b[39;49mcpu()\u001b[39m.\u001b[39;49mnumpy(), \\\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu/home/pkhamchuai/codes/spppt-1/DL_template.ipynb#X42sdnNjb2RlLXJlbW90ZQ%3D%3D?line=55'>56</a>\u001b[0m     transformed_source_affine[\u001b[39m0\u001b[39;49m, \u001b[39m0\u001b[39;49m, :, :]\u001b[39m.\u001b[39;49mcpu()\u001b[39m.\u001b[39;49mnumpy(), \\\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu/home/pkhamchuai/codes/spppt-1/DL_template.ipynb#X42sdnNjb2RlLXJlbW90ZQ%3D%3D?line=56'>57</a>\u001b[0m     points1, points2, points1_affine, desc1, desc2, \\\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu/home/pkhamchuai/codes/spppt-1/DL_template.ipynb#X42sdnNjb2RlLXJlbW90ZQ%3D%3D?line=57'>58</a>\u001b[0m         affine_params\u001b[39m=\u001b[39;49maffine_params_predicted, heatmap1\u001b[39m=\u001b[39;49mheatmap1, heatmap2\u001b[39m=\u001b[39;49mheatmap2, plot\u001b[39m=\u001b[39;49mplot_)\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu/home/pkhamchuai/codes/spppt-1/DL_template.ipynb#X42sdnNjb2RlLXJlbW90ZQ%3D%3D?line=59'>60</a>\u001b[0m \u001b[39m# calculate metrics\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu/home/pkhamchuai/codes/spppt-1/DL_template.ipynb#X42sdnNjb2RlLXJlbW90ZQ%3D%3D?line=60'>61</a>\u001b[0m \u001b[39m# matches1_transformed = results[0]\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu/home/pkhamchuai/codes/spppt-1/DL_template.ipynb#X42sdnNjb2RlLXJlbW90ZQ%3D%3D?line=61'>62</a>\u001b[0m mse_before \u001b[39m=\u001b[39m results[\u001b[39m1\u001b[39m]\n",
      "\u001b[0;31mTypeError\u001b[0m: DL_affine_plot() got an unexpected keyword argument 'affine_params'"
     ]
    }
   ],
   "source": [
    "def test(model, model_params, timestamp):\n",
    "    # Set model to training mode\n",
    "    model.eval()\n",
    "\n",
    "    # Create output directory\n",
    "    output_dir = f\"output/{model_params.get_model_code()}_{timestamp}_test\"\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    # Validate model\n",
    "    # validation_loss = 0.0\n",
    "\n",
    "    # create a csv file to store the metrics\n",
    "    csv_file = f\"{output_dir}/metrics.csv\"\n",
    "    with open(csv_file, 'w', newline='') as file:\n",
    "        writer = csv.writer(file)\n",
    "        # matches1_transformed.shape[-1], mse_before, mse12, tre_before, tre12, \\\n",
    "        # mse12_image, ssim12_image, \n",
    "        writer.writerow([\"index\", \"mse_before\", \"mse12\", \"tre_before\", \"tre12\", \"mse12_image_before\", \"mse12_image\", \"ssim12_image_before\", \"ssim12_image\"])\n",
    "\n",
    "    with torch.no_grad():\n",
    "        testbar = tqdm(test_dataset, desc=f'Testing:')\n",
    "        for i, data in enumerate(testbar, 0):\n",
    "            # Get images and affine parameters\n",
    "            if model_params.sup:\n",
    "                source_image, target_image, affine_params_true = data\n",
    "            else:\n",
    "                source_image, target_image = data\n",
    "            source_image = source_image.to(device)\n",
    "            target_image = target_image.to(device)\n",
    "\n",
    "            # Forward pass\n",
    "            outputs = model(source_image, target_image)\n",
    "            # for i in range(len(outputs)):\n",
    "            #     print(i, outputs[i].shape)\n",
    "            transformed_source_affine = outputs[0]\n",
    "            affine_params_predicted = outputs[1]\n",
    "            points1 = outputs[2]\n",
    "            points2 = outputs[3]\n",
    "            points1_affine = np.array(outputs[4])\n",
    "            try:\n",
    "                points1_affine = points1_affine.reshape(points1_affine.shape[2], points1_affine.shape[1])\n",
    "            except:\n",
    "                pass\n",
    "            desc1 = outputs[5]\n",
    "            desc2 = outputs[6]\n",
    "            heatmap1 = outputs[7]\n",
    "            heatmap2 = outputs[8]\n",
    "\n",
    "            if i < 50:\n",
    "                plot_ = True\n",
    "            else:\n",
    "                plot_ = False\n",
    "\n",
    "            results = DL_affine_plot(f\"{i+1}\", output_dir,\n",
    "                f\"{i}\", \"_\", source_image[0, 0, :, :].cpu().numpy(), target_image[0, 0, :, :].cpu().numpy(), \\\n",
    "                transformed_source_affine[0, 0, :, :].cpu().numpy(), \\\n",
    "                points1, points2, points1_affine, desc1, desc2, affine_params_true=affine_params_true,\n",
    "                affine_params_predict=affine_params_predicted, heatmap1=heatmap1, heatmap2=heatmap2, plot=plot_)\n",
    "\n",
    "            # calculate metrics\n",
    "            # matches1_transformed = results[0]\n",
    "            mse_before = results[1]\n",
    "            mse12 = results[2]\n",
    "            tre_before = results[3]\n",
    "            tre12 = results[4]\n",
    "            mse12_image_before = results[5]\n",
    "            mse12_image = results[6]\n",
    "            ssim12_image_before = results[7]\n",
    "            ssim12_image = results[8]\n",
    "\n",
    "            # write metrics to csv file\n",
    "            with open(csv_file, 'a', newline='') as file:\n",
    "                writer = csv.writer(file) # TODO: might need to export true & predicted affine parameters too\n",
    "                writer.writerow([i, mse_before, mse12, tre_before, tre12, mse12_image_before, mse12_image, ssim12_image_before, ssim12_image])\n",
    "\n",
    "    # delete all txt files in output_dir\n",
    "    for file in os.listdir(output_dir):\n",
    "        if file.endswith(\".txt\"):\n",
    "            os.remove(os.path.join(output_dir, file))\n",
    "\n",
    "# timestamp = datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "metrics = test(model, model_params, timestamp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spppt",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
