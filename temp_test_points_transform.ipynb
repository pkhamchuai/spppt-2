{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DVF choosing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda\n",
      "Device: cuda\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.functional import F\n",
    "from utils.utils0 import *\n",
    "from utils.utils1 import *\n",
    "from utils.utils1 import ModelParams, model_loader, print_summary#, test_repeat\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f'Device: {device}')\n",
    "\n",
    "mse = nn.MSELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model name:  dataset2_sup1_image1_points0_loss_image0\n",
      "Model code:  21100_0.001_0_10_1\n",
      "Model params:  {'dataset': 2, 'sup': 1, 'image': 1, 'points': 0, 'loss_image_case': 0, 'loss_image': MSELoss(), 'loss_affine': <utils.utils1.loss_affine object at 0x7f7388e2bd60>, 'learning_rate': 0.001, 'decay_rate': 0.96, 'start_epoch': 0, 'num_epochs': 10, 'batch_size': 1, 'model_name': 'dataset2_sup1_image1_points0_loss_image0'}\n"
     ]
    }
   ],
   "source": [
    "model_params = ModelParams(dataset=2, sup=1)\n",
    "test_dataset = datagen(model_params.dataset, False, model_params.sup)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# draw a pair of images from the test set\n",
    "A, B, affine_param, pt1, pt2, pt3 = list(test_dataset)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "A = torch.concat([A, B], dim=0)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1, 108, 2]), torch.Size([1, 108, 2]), torch.Size([1, 108, 2]))"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pt1.shape, pt2.shape, pt3.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_points_DVF(points, M, image):\n",
    "    # transform points using displacement field\n",
    "    # DVF.shape = (2, H, W)\n",
    "    # points.shape = (2, N)\n",
    "    displacement_field = torch.zeros(image.shape[-1], image.shape[-1])\n",
    "    DVF = transform_to_displacement_field(\n",
    "        displacement_field.view(1, 1, displacement_field.size(0), displacement_field.size(1)), \n",
    "        M.view(1, 2, 3))\n",
    "\n",
    "    # Reshape tensor points to have dimensions [2, N]\n",
    "    points = points.long()\n",
    "\n",
    "    # Use torch.gather to select values from A using indices from points\n",
    "    result = DVF[:, points[:, 0], points[:, 1]]\n",
    "\n",
    "    # Reshape result to have dimensions [2, N]\n",
    "    result = result.t()\n",
    "    # subtract the result from the original points\n",
    "    points = points.float()\n",
    "    result = torch.subtract(points, result)\n",
    "    return result\n",
    "\n",
    "# def transform_points_DVF(points, M, image):\n",
    "#     # transform points using displacement field\n",
    "#     # DVF.shape = (2, H, W)\n",
    "#     # points.shape = (2, N)\n",
    "#     displacement_field = torch.zeros(image.shape[-1], image.shape[-1])\n",
    "#     DVF = transform_to_displacement_field(\n",
    "#         displacement_field.view(1, 1, displacement_field.size(0), displacement_field.size(1)), \n",
    "#         M.clone().view(1, 2, 3))\n",
    "#     if isinstance(DVF, torch.Tensor):\n",
    "#         DVF = DVF.numpy()\n",
    "#     # loop through each point and apply the transformation\n",
    "#     for i in range(points.shape[1]):\n",
    "#         points[:, i] = points[:, i] - DVF[:, int(points[1, i]), int(points[0, i])]\n",
    "#     return points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 108])\n"
     ]
    }
   ],
   "source": [
    "transform_points = transform_points_DVF(pt1[0], affine_param[0], A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# scatter plot of the points pt1[0], pt2[0], transformed_points, \n",
    "\n",
    "plt.scatter(pt1[0][0], pt1[0][1], c='r')\n",
    "plt.scatter(pt2[0][0], pt2[0][1], c='b')\n",
    "plt.scatter(transform_points[0], transform_points[1], c='g')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.6353, 0.1961, 0.5686, 0.2588, 0.6941, 0.6902, 0.1725, 0.5255, 0.6863,\n",
      "         0.3020, 0.2039, 0.3765, 0.2588, 0.3098, 0.0902, 0.5608, 0.7569, 0.7412,\n",
      "         0.2118, 0.3647, 0.2863, 0.6863, 0.3765, 0.1333, 0.2353, 0.6980, 0.3451,\n",
      "         0.2627, 0.7255, 0.1020, 0.6549, 0.6745, 0.6549, 0.1843, 0.5294, 0.5255,\n",
      "         0.2824, 0.9922, 0.4627, 0.2902, 0.2471, 0.1137, 0.7020, 0.6000, 0.3490,\n",
      "         0.5176, 0.2588, 0.5490, 0.0549, 0.3216, 0.5490, 0.2353, 0.3373, 0.0941,\n",
      "         0.4196, 0.5843, 0.2667, 0.6902, 0.1725, 0.8275, 0.5490, 0.5176, 0.3725,\n",
      "         0.5020, 0.3922, 0.2235, 0.1412, 0.2824, 0.5373, 0.0941, 0.0706, 0.7020,\n",
      "         0.3608, 0.6118, 0.4353, 0.0627, 0.1020, 0.7843, 0.7412, 0.2549, 0.8118,\n",
      "         0.5216, 0.6314, 0.1255, 0.3529, 0.6157, 0.3412, 0.5098, 0.3882, 0.3686,\n",
      "         0.0784, 0.5176, 0.5843, 0.0471, 0.4980, 0.6980, 0.6039, 0.3373, 0.5020,\n",
      "         0.4627, 0.2510, 0.2000, 0.0863, 0.7412, 0.6000, 0.3137, 0.6902, 0.3529]])\n"
     ]
    }
   ],
   "source": [
    "# Reshape tensor points to have dimensions [2, N]\n",
    "points = pt1[0].long()\n",
    "\n",
    "# Use torch.gather to select values from A using indices from points\n",
    "result = A[:, points[:, 0], points[:, 1]]\n",
    "\n",
    "print(result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 20])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Affine transformation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Method 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AffineTransform(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(AffineTransform, self).__init__()\n",
    "\n",
    "    def forward(self, points, matrix):\n",
    "        points = points.T\n",
    "        # Add a row of ones to the input points for the affine transformation\n",
    "        ones = torch.ones(1, points.size(1), dtype=points.dtype, device=points.device)\n",
    "        points_homogeneous = torch.cat([points, ones], dim=0)\n",
    "\n",
    "        # Apply the affine transformation\n",
    "        # print dtype of matrix and points_homogeneous\n",
    "        transformed_points = torch.mm(matrix, points_homogeneous.float())\n",
    "\n",
    "        return transformed_points[:2, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "affine_layer = AffineTransform()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "affine_param = torch.tensor([[1.0, 0.0, 0.0],\n",
    "                                [0.0, 1.0, 0.0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[141., 175.],\n",
       "        [196.,  75.],\n",
       "        [249.,   0.],\n",
       "        [  9.,  36.],\n",
       "        [142.,  90.],\n",
       "        [ 64.,   0.],\n",
       "        [136., 213.],\n",
       "        [ 48., 122.],\n",
       "        [ 76., 243.],\n",
       "        [148., 242.],\n",
       "        [ 53., 208.],\n",
       "        [231.,  69.],\n",
       "        [ 10., 200.],\n",
       "        [  0., 148.],\n",
       "        [ 76., 161.],\n",
       "        [151., 189.],\n",
       "        [129., 164.],\n",
       "        [ 95., 168.],\n",
       "        [201.,  10.],\n",
       "        [167.,  97.]])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transformed_points = affine_layer(points, affine_param)\n",
    "transformed_points.T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Method 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_to_displacement_field(tensor, tensor_transform, device='cpu'):\n",
    "    \"\"\"\n",
    "    Transforms a tensor using an affine transformation matrix and returns the corresponding displacement field.\n",
    "\n",
    "    Args:\n",
    "        tensor (torch.Tensor): The input tensor to transform, with shape (batch_size, channels, height, width).\n",
    "        tensor_transform (torch.Tensor): The affine transformation matrix, with shape (batch_size, 2, 3).\n",
    "        device (str, optional): The device to use for the computation (default: 'cpu').\n",
    "\n",
    "    Returns:\n",
    "        torch.Tensor: The displacement field tensor, with shape (2, height, width).\n",
    "    \"\"\"\n",
    "    # function code here\n",
    "    y_size, x_size = tensor.size(2), tensor.size(3)\n",
    "    deformation_field = F.affine_grid(tensor_transform, tensor.size(), align_corners=False)\n",
    "    gy, gx = torch.meshgrid(torch.arange(y_size), torch.arange(x_size))\n",
    "    gy = gy.type(torch.FloatTensor).to(device)\n",
    "    gx = gx.type(torch.FloatTensor).to(device)\n",
    "    grid_x = (gx / (x_size - 1) - 0.5) * 2\n",
    "    grid_y = (gy / (y_size - 1) - 0.5) * 2\n",
    "    u_x = deformation_field[0, :, :, 0] - grid_x\n",
    "    u_y = deformation_field[0, :, :, 1] - grid_y\n",
    "    u_x = u_x / 2 * (x_size - 1)\n",
    "    u_y = u_y / 2 * (y_size - 1)\n",
    "    displacement_field = torch.cat((u_x.view(1, y_size, x_size), u_y.view(1, y_size, x_size)), dim=0)\n",
    "    return displacement_field"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_points_DVF(points, M, image):\n",
    "    # transform points using displacement field\n",
    "    # DVF.shape = (2, H, W)\n",
    "    # points.shape = (2, N)\n",
    "    displacement_field = torch.zeros(image.shape[-1], image.shape[-1])\n",
    "    DVF = transform_to_displacement_field(\n",
    "        displacement_field.view(1, 1, displacement_field.size(0), displacement_field.size(1)), \n",
    "        M.view(1, 2, 3))\n",
    "\n",
    "    # Reshape tensor points to have dimensions [2, N]\n",
    "    points = points.t().long()\n",
    "\n",
    "    # Use torch.gather to select values from A using indices from points\n",
    "    result = DVF[:, points[:, 0], points[:, 1]]\n",
    "\n",
    "    # Reshape result to have dimensions [2, N]\n",
    "    result = result.t()\n",
    "    # subtract the result from the original points\n",
    "    points = points.float()\n",
    "    result = torch.subtract(points, result)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1.4119e+02,  1.7505e+02],\n",
       "        [ 1.9579e+02,  7.5268e+01],\n",
       "        [ 2.4850e+02,  4.7461e-01],\n",
       "        [ 8.6426e+00,  3.5537e+01],\n",
       "        [ 1.4185e+02,  9.0057e+01],\n",
       "        [ 6.3502e+01, -2.4805e-01],\n",
       "        [ 1.3633e+02,  2.1303e+02],\n",
       "        [ 4.7979e+01,  1.2169e+02],\n",
       "        [ 7.6451e+01,  2.4280e+02],\n",
       "        [ 1.4845e+02,  2.4208e+02],\n",
       "        [ 5.3314e+01,  2.0771e+02],\n",
       "        [ 2.3077e+02,  6.9404e+01],\n",
       "        [ 1.0283e+01,  1.9954e+02],\n",
       "        [ 8.0092e-02,  1.4750e+02],\n",
       "        [ 7.6131e+01,  1.6080e+02],\n",
       "        [ 1.5124e+02,  1.8909e+02],\n",
       "        [ 1.2914e+02,  1.6401e+02],\n",
       "        [ 9.5158e+01,  1.6787e+02],\n",
       "        [ 2.0054e+02,  1.0287e+01],\n",
       "        [ 1.6688e+02,  9.7154e+01]])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transformed_points = transform_points_DVF(points.T, affine_param, A)\n",
    "transformed_points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spppt",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
