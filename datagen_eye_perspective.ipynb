{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda\n",
      "(5040, 9)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>source</th>\n",
       "      <th>Source ROI</th>\n",
       "      <th>target</th>\n",
       "      <th>Target ROI</th>\n",
       "      <th>training</th>\n",
       "      <th>Warped target images</th>\n",
       "      <th>Warped target ROI</th>\n",
       "      <th>Execution time</th>\n",
       "      <th>Directory</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2011248_20161215__L_b2.jpg</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2011248_20161215__L_b1.jpg</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Dataset/Dataset-processed/15-12-2559/2011248/Lb</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2011248_20161215__L_b2.jpg</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2011248_20161215__L_b3.jpg</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Dataset/Dataset-processed/15-12-2559/2011248/Lb</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2011248_20161215__L_b1.jpg</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2011248_20161215__L_b3.jpg</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Dataset/Dataset-processed/15-12-2559/2011248/Lb</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2011248_20161215__L_c2.jpg</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2011248_20161215__L_c1.jpg</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Dataset/Dataset-processed/15-12-2559/2011248/Lc</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2011248_20161215__L_c2.jpg</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2011248_20161215__L_c3.jpg</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Dataset/Dataset-processed/15-12-2559/2011248/Lc</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       source  Source ROI                      target  \\\n",
       "0  2011248_20161215__L_b2.jpg         NaN  2011248_20161215__L_b1.jpg   \n",
       "1  2011248_20161215__L_b2.jpg         NaN  2011248_20161215__L_b3.jpg   \n",
       "2  2011248_20161215__L_b1.jpg         NaN  2011248_20161215__L_b3.jpg   \n",
       "3  2011248_20161215__L_c2.jpg         NaN  2011248_20161215__L_c1.jpg   \n",
       "4  2011248_20161215__L_c2.jpg         NaN  2011248_20161215__L_c3.jpg   \n",
       "\n",
       "   Target ROI  training  Warped target images  Warped target ROI  \\\n",
       "0         NaN         1                   NaN                NaN   \n",
       "1         NaN         1                   NaN                NaN   \n",
       "2         NaN         1                   NaN                NaN   \n",
       "3         NaN         0                   NaN                NaN   \n",
       "4         NaN         0                   NaN                NaN   \n",
       "\n",
       "   Execution time                                        Directory  \n",
       "0             NaN  Dataset/Dataset-processed/15-12-2559/2011248/Lb  \n",
       "1             NaN  Dataset/Dataset-processed/15-12-2559/2011248/Lb  \n",
       "2             NaN  Dataset/Dataset-processed/15-12-2559/2011248/Lb  \n",
       "3             NaN  Dataset/Dataset-processed/15-12-2559/2011248/Lc  \n",
       "4             NaN  Dataset/Dataset-processed/15-12-2559/2011248/Lc  "
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# read the file 'dataset_reg_pair_filled.csv' and generate synthetic data\n",
    "# first read the file, then make a list of the source training images\n",
    "# then for each image, generate 10 synthetic images with random affine transformation\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "import os\n",
    "import random\n",
    "import csv\n",
    "from PIL import Image\n",
    "import shutil\n",
    "\n",
    "import torch\n",
    "\n",
    "from utils.utils0 import tensor_affine_transform, transform_to_displacement_field\n",
    "from utils.utils1 import transform_points_DVF, transform_points_from_DVF_unbatched, transform_points_from_DVF\n",
    "from utils.SuperPoint import SuperPointFrontend, PointTracker\n",
    "superpoint = SuperPointFrontend('utils/superpoint_v1.pth', nms_dist=4,\n",
    "                          conf_thresh=0.015, nn_thresh=0.7, cuda=True)\n",
    "\n",
    "# read the file\n",
    "df = pd.read_csv('Dataset/dataset_reg_pair_filled.csv')\n",
    "print(df.shape)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_143606/1623562252.py:7: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_train['image_path'] = df_train['Directory'] + '/' + df_train['source']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>source</th>\n",
       "      <th>Source ROI</th>\n",
       "      <th>target</th>\n",
       "      <th>Target ROI</th>\n",
       "      <th>training</th>\n",
       "      <th>Warped target images</th>\n",
       "      <th>Warped target ROI</th>\n",
       "      <th>Execution time</th>\n",
       "      <th>Directory</th>\n",
       "      <th>image_path</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2011248_20161215__L_c2.jpg</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2011248_20161215__L_c1.jpg</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Dataset/Dataset-processed/15-12-2559/2011248/Lc</td>\n",
       "      <td>Dataset/Dataset-processed/15-12-2559/2011248/L...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2011248_20161215__L_c2.jpg</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2011248_20161215__L_c3.jpg</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Dataset/Dataset-processed/15-12-2559/2011248/Lc</td>\n",
       "      <td>Dataset/Dataset-processed/15-12-2559/2011248/L...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2011248_20161215__L_c1.jpg</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2011248_20161215__L_c3.jpg</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Dataset/Dataset-processed/15-12-2559/2011248/Lc</td>\n",
       "      <td>Dataset/Dataset-processed/15-12-2559/2011248/L...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2011248_20161215__R_b2.jpg</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2011248_20161215__R_b3.jpg</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Dataset/Dataset-processed/15-12-2559/2011248/Rb</td>\n",
       "      <td>Dataset/Dataset-processed/15-12-2559/2011248/R...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2011248_20161215__R_b2.jpg</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2011248_20161215__R_b1.jpg</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Dataset/Dataset-processed/15-12-2559/2011248/Rb</td>\n",
       "      <td>Dataset/Dataset-processed/15-12-2559/2011248/R...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       source  Source ROI                      target  \\\n",
       "3  2011248_20161215__L_c2.jpg         NaN  2011248_20161215__L_c1.jpg   \n",
       "4  2011248_20161215__L_c2.jpg         NaN  2011248_20161215__L_c3.jpg   \n",
       "5  2011248_20161215__L_c1.jpg         NaN  2011248_20161215__L_c3.jpg   \n",
       "6  2011248_20161215__R_b2.jpg         NaN  2011248_20161215__R_b3.jpg   \n",
       "7  2011248_20161215__R_b2.jpg         NaN  2011248_20161215__R_b1.jpg   \n",
       "\n",
       "   Target ROI  training  Warped target images  Warped target ROI  \\\n",
       "3         NaN         0                   NaN                NaN   \n",
       "4         NaN         0                   NaN                NaN   \n",
       "5         NaN         0                   NaN                NaN   \n",
       "6         NaN         0                   NaN                NaN   \n",
       "7         NaN         0                   NaN                NaN   \n",
       "\n",
       "   Execution time                                        Directory  \\\n",
       "3             NaN  Dataset/Dataset-processed/15-12-2559/2011248/Lc   \n",
       "4             NaN  Dataset/Dataset-processed/15-12-2559/2011248/Lc   \n",
       "5             NaN  Dataset/Dataset-processed/15-12-2559/2011248/Lc   \n",
       "6             NaN  Dataset/Dataset-processed/15-12-2559/2011248/Rb   \n",
       "7             NaN  Dataset/Dataset-processed/15-12-2559/2011248/Rb   \n",
       "\n",
       "                                          image_path  \n",
       "3  Dataset/Dataset-processed/15-12-2559/2011248/L...  \n",
       "4  Dataset/Dataset-processed/15-12-2559/2011248/L...  \n",
       "5  Dataset/Dataset-processed/15-12-2559/2011248/L...  \n",
       "6  Dataset/Dataset-processed/15-12-2559/2011248/R...  \n",
       "7  Dataset/Dataset-processed/15-12-2559/2011248/R...  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# make a list of the unique source training images that has 'training' = 0\n",
    "# each image path consists of image directory, image name\n",
    "\n",
    "df_train = df[df['training'] == 0]\n",
    "\n",
    "# create a new df consists of image directory and image name concatenated\n",
    "df_train['image_path'] = df_train['Directory'] + '/' + df_train['source']\n",
    "df_train.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4132, 10)\n",
      "5\n",
      "Dataset/Dataset-processed/15-12-2559/2011248/Lc/2011248_20161215__L_c2.jpg\n"
     ]
    }
   ],
   "source": [
    "print(df_train.shape)\n",
    "# make a list of the unique values in the column 'image_path'\n",
    "image_list = df_train['image_path'].unique()[:5]\n",
    "print(len(image_list))\n",
    "print(image_list[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_translation = 0.1  # Minimum translation in pixels\n",
    "max_translation = 0.2  # Maximum translation in pixels\n",
    "max_rotation = 20  # Maximum rotation in degrees\n",
    "max_shear = 10  # Maximum shear in degrees\n",
    "min_scale = 0.85  # Minimum scale factor\n",
    "max_scale = 1.15  # Maximum scale factor\n",
    "max_perspective = 0.1  # Maximum perspective distortion\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_train.to_csv('Dataset/synth_eye_train.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_test.to_csv('Dataset/synth_eye_test.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: a fuction for warping image using perspective transformation\n",
    "def warp_image_perspective_cv2(image, M):\n",
    "    # image: a numpy array of shape (H, W, 3)\n",
    "    # M: a numpy array of shape (3, 3)\n",
    "    # return: a numpy array of shape (H, W, 3)\n",
    "    H, W, _ = image.shape\n",
    "    warped_image = cv2.warpPerspective(image, M, (W, H))\n",
    "    return warped_image\n",
    "\n",
    "def warp_image_perspective_pytorch(image, M):\n",
    "    '''\n",
    "    - Grid Generation: We generate a grid of coordinates (x, y) for the image.\n",
    "    - Homography Application: We apply the homography matrix M to these coordinates to get transformed coordinates.\n",
    "    - Grid Sampling: We then use grid_sample to sample the image at these transformed coordinates.\n",
    "    '''\n",
    "\n",
    "    H, W = image.shape\n",
    "    M = torch.from_numpy(M).float()\n",
    "\n",
    "    # Generate a grid of coordinates (x, y) for the image\n",
    "    y, x = torch.meshgrid(torch.arange(H), torch.arange(W))\n",
    "    ones = torch.ones_like(x)\n",
    "    grid = torch.stack([x, y, ones], dim=-1).float()  # Shape (H, W, 3)\n",
    "    \n",
    "    # Flatten the grid to (H*W, 3)\n",
    "    grid_flat = grid.view(-1, 3).t()  # Shape (3, H*W)\n",
    "    \n",
    "    # Apply the homography transformation\n",
    "    transformed_grid = M @ grid_flat  # Shape (3, H*W)\n",
    "    \n",
    "    # Convert from homogeneous to Cartesian coordinates\n",
    "    transformed_grid = transformed_grid / transformed_grid[2, :]\n",
    "    transformed_grid = transformed_grid[:2, :].t()  # Shape (H*W, 2)\n",
    "    \n",
    "    # Reshape the transformed coordinates to the original image shape\n",
    "    transformed_grid = transformed_grid.view(H, W, 2)\n",
    "    \n",
    "    # Normalize coordinates to be in range [-1, 1] for grid_sample\n",
    "    transformed_grid[..., 0] = (transformed_grid[..., 0] / (W - 1)) * 2 - 1\n",
    "    transformed_grid[..., 1] = (transformed_grid[..., 1] / (H - 1)) * 2 - 1\n",
    "    \n",
    "    # Add batch dimension and channel dimension, permute to (_, H, W, 1)\n",
    "    transformed_grid = transformed_grid.unsqueeze(0)\n",
    "    \n",
    "    if len(image.shape) == 2:\n",
    "        image = torch.from_numpy(image).float().unsqueeze(0).unsqueeze(0)\n",
    "    # image = image.permute(2, 0, 1).unsqueeze(0)\n",
    "    \n",
    "    # Sample the image using the transformed grid\n",
    "    warped_image = torch.nn.functional.grid_sample(image, transformed_grid, align_corners=False)\n",
    "    \n",
    "    # Remove batch and channel dimensions and convert to numpy\n",
    "    # warped_image = warped_image.squeeze(0).squeeze(0).numpy()\n",
    "    \n",
    "    print(warped_image.shape, transformed_grid.shape)\n",
    "    return warped_image, transformed_grid"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Apply all transformations to the eye images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_affine_transformed_images_multiple(img_list, csv_file, output_dir, num_images=5, modify=False):\n",
    "    # delete all files and subdirectories in the output directory\n",
    "    shutil.rmtree(output_dir)\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    # create a list to store different point locations\n",
    "    points_list = []\n",
    "\n",
    "    # Initialize the CSV file with a header\n",
    "    with open(csv_file, 'w', newline='') as csvfile:\n",
    "        writer = csv.writer(csvfile)\n",
    "        # write the header, perspective parameters, image path, and keypoints\n",
    "        writer.writerow([\"source\", \"target\", \"M01\", \"M02\", \"M03\", \"M04\", \"M05\",\n",
    "            \"M06\", \"M07\", \"M08\", \"M09\",\n",
    "            \"image_path\", \"keypoints\"])\n",
    "\n",
    "    # Loop over the images, read the image, \n",
    "    # apply affine transformation and save it\n",
    "    for i, img_path in enumerate(img_list):\n",
    "        # if i > len(img_list)/2:\n",
    "        #     break\n",
    "        # Read the image as grayscale using cv2\n",
    "        image = cv2.imread(img_path, cv2.IMREAD_GRAYSCALE)\n",
    "\n",
    "        # Save original image\n",
    "        original_image_path = os.path.join(output_dir, f\"img_{i}_original.png\")\n",
    "\n",
    "        # take 90% of the image\n",
    "        image_base = image[int(image.shape[0]*0.1):int(image.shape[0]*0.9), \n",
    "                        int(image.shape[1]*0.1):int(image.shape[1]*0.9)]\n",
    "        # resize image to 256x256\n",
    "        image_base = cv2.resize(image_base, (256, 256))\n",
    "\n",
    "        cv2.imwrite(original_image_path, image_base + np.random.uniform(-0.01, 0.01, image_base.shape))\n",
    "\n",
    "        # Convert the transformed image to a numpy array\n",
    "        # img_transformed = np.array(img_transformed)\n",
    "        image_base = np.array(Image.fromarray(image_base).convert('L'))\n",
    "\n",
    "        tracker = PointTracker(5, nn_thresh=0.7)\n",
    "        points1, desc1, _ = superpoint(image_base.astype(np.float32)/255)\n",
    "\n",
    "        for j in range(num_images):\n",
    "            # random (2x3) affine transformation matrix\n",
    "            #M = np.array([[1.0, 0.0, np.random()], [0.0, 1.0, 0.0]])\n",
    "            if j == num_images-1:\n",
    "                # pass\n",
    "                img_transformed = image_base.copy()\n",
    "                M = np.array([[1.0, 0.0, 0.0], [0.0, 1.0, 0.0], [0.0, 0.0, 1.0]])\n",
    "                points2 = points1.copy()\n",
    "                desc2 = desc1.copy()\n",
    "                \n",
    "                img_transformed = cv2.resize(img_transformed, (256, 256))\n",
    "                # convert to grayscale\n",
    "                img_transformed_BW = np.array(Image.fromarray(img_transformed).convert('L'))\n",
    "\n",
    "                # # TODO: save heatmaps for other version of network\n",
    "                points2, desc2, _ = superpoint(img_transformed_BW.astype(np.float32)/255)\n",
    "                matches = tracker.nn_match_two_way(desc1, desc2, nn_thresh=0.7)\n",
    "\n",
    "                # # print(desc1.shape, desc2.shape)\n",
    "                matches1 = points1[:2, matches[0, :].astype(int)]\n",
    "                # # matches1 = matches1.T[None, :, :]\n",
    "                matches2 = points2[:2, matches[1, :].astype(int)]\n",
    "                \n",
    "                # transform the points using the displacement field\n",
    "                # print(torch.tensor(M)[None, :, :].shape, torch.tensor(image)[None, None, :, :].shape)\n",
    "                # print(torch.tensor(M).shape, torch.tensor(image).shape, torch.tensor(matches1.copy()).unsqueeze(-1).view(2, -1, 1).shape)\n",
    "                matches1_transformed_DVF = matches1.copy()\n",
    "                # print(f'Img {i}, diff: {matches1_transformed_DVF[:, 0] - matches2[:, 0]}')\n",
    "                # points_list.append(matches1_transformed_DVF[:, 0] - matches2[:, 0])\n",
    "\n",
    "                # add some noise to the transformed image and save it\n",
    "                img_transformed = img_transformed + np.random.uniform(-0.01, 0.01, img_transformed.shape)\n",
    "                if modify: # if modify is True, then add some intensity change to the transformed image\n",
    "                    img_transformed = img_transformed + np.random.normal(1, 0.1, 1)\n",
    "\n",
    "\n",
    "                transformed_image_path = os.path.join(output_dir, f\"img_{i}_transformed_{j}.png\")\n",
    "                cv2.imwrite(transformed_image_path, img_transformed)\n",
    "\n",
    "                # create a dataframe with the matches\n",
    "                # print(matches1.shape, matches2.shape, matches1_transformed_DVF.shape)\n",
    "                if len(matches1_transformed_DVF.shape) == 3:\n",
    "                    matches1_transformed_DVF = matches1_transformed_DVF.squeeze(-1)\n",
    "                df = pd.DataFrame({'x1': matches1[0, :], 'y1': matches1[1, :],\n",
    "                                'x2': matches2[0, :], 'y2': matches2[1, :],\n",
    "                                'x2_': matches1_transformed_DVF[0, :], 'y2_': matches1_transformed_DVF[1, :]})\n",
    "                save_name = os.path.join(output_dir, f\"img_{i}_{j}_keypoints.csv\")\n",
    "                df.to_csv(save_name, index=False)\n",
    "\n",
    "                with open(csv_file, 'a', newline='') as csvfile:\n",
    "                    writer = csv.writer(csvfile)\n",
    "                    writer.writerow([original_image_path, transformed_image_path, \n",
    "                                    M[0, 0], M[0, 1], M[0, 2], M[1, 0], M[1, 1], M[1, 2], \n",
    "                                    M[2, 0], M[2, 1], M[2, 2], img_path, save_name])\n",
    "            \n",
    "            else:\n",
    "                rand_angle = np.random.uniform(0, max_rotation)/180*np.pi\n",
    "                rand_range = 1.2\n",
    "                scaling = [np.random.uniform(1, rand_range), np.random.uniform(1, rand_range)]\n",
    "                # rand_angle = 10/180*np.pi\n",
    "                translate_range = 0.1\n",
    "                translate = [np.random.uniform(-translate_range, translate_range), np.random.uniform(-translate_range, translate_range)]\n",
    "                \n",
    "                # test_random = [1.1, 1.1]\n",
    "                # M = np.array([[1.0 + test_random[0], 0.0, 0.0], [0.0, 1.0 + test_random[1], 0.0]])\n",
    "                scale_power = [[-1, -1], [1, 1], [-1, 1], [1, -1]]\n",
    "                rotation_direction = [-1, 1]\n",
    "                # perspective\n",
    "                rand_range = 0.001\n",
    "                perspective = [np.random.uniform(-rand_range, rand_range), np.random.uniform(-rand_range, rand_range)]\n",
    "\n",
    "                for k in range(4):\n",
    "                    for l in range(2):\n",
    "                        power = scale_power[k]\n",
    "                        sign = rotation_direction[l]\n",
    "                        \n",
    "                        # Combined transformation matrix for perspective, rotation, scaling, and translation\n",
    "                        # M = np.array([[sign*scaling[0]**power[0]*np.cos(rand_angle), sign*scaling[1]**power[1]*np.sin(rand_angle), translate[0]],\n",
    "                        #             [-sign*scaling[0]**power[0]*np.sin(rand_angle), sign*scaling[1]**power[1]*np.cos(rand_angle), translate[1]],\n",
    "                        #             [perspective[0], perspective[1], 1.0]])\n",
    "                        # M = np.array([[np.cos(rand_angle)*(scaling[0]**power[0]), -sign*np.sin(rand_angle), translate[0]],\n",
    "                        #             [sign*np.sin(rand_angle), np.cos(rand_angle)*(scaling[1]**power[1]), translate[1]],\n",
    "                        #             [perspective[0], perspective[1], 1.0]])\n",
    "                        print(f'Perspective: {perspective}')\n",
    "                        M = np.array([[1.0, 0.0, 0.0], [0.0, 1.0, 0.0], [perspective[0], perspective[1], 1.0]])\n",
    "                        \n",
    "                        # M = np.array([[0.8, 0.0, 0.0], \n",
    "                        #               [0.0, 0.8, 0.0]])\n",
    "\n",
    "                        img_transformed, DVF = warp_image_perspective_pytorch(image_base, M)\n",
    "                        img_transformed = img_transformed.squeeze(0).squeeze(0).numpy()\n",
    "                        # img_transformed = img_transformed[int(image.shape[0]*0.1):int(image.shape[0]*0.9), \n",
    "                        #                                 int(image.shape[1]*0.1):int(image.shape[1]*0.9)]\n",
    "\n",
    "                        # resize image to 256x256\n",
    "                        img_transformed = cv2.resize(img_transformed, (256, 256))\n",
    "                        # convert to grayscale\n",
    "                        img_transformed_BW = np.array(Image.fromarray(img_transformed).convert('L'))\n",
    "\n",
    "                        # --------------- to here\n",
    "                        # add some noise to the transformed image and save it\n",
    "                        img_transformed = img_transformed + np.random.uniform(-0.01, 0.01, img_transformed.shape)\n",
    "                        if modify: # if modify is True, then add some intensity change to the transformed image\n",
    "                            img_transformed = img_transformed + np.random.normal(1, 0.1, 1)\n",
    "\n",
    "                        transformed_image_path = os.path.join(output_dir, f\"img_{i}_transformed_{j}_{k}_{l}.png\")\n",
    "                        cv2.imwrite(transformed_image_path, img_transformed)\n",
    "                        # ---------------\n",
    "\n",
    "                        # # TODO: save heatmaps for other version of network\n",
    "                        points2, desc2, _ = superpoint(img_transformed_BW.astype(np.float32)/255)\n",
    "                        # print(points2.shape, points2)\n",
    "                        print(desc1.shape, desc2.shape)\n",
    "                        matches = tracker.nn_match_two_way(desc1, desc2, nn_thresh=0.7)\n",
    "\n",
    "                        # # print(desc1.shape, desc2.shape)\n",
    "                        matches1 = points1[:2, matches[0, :].astype(int)]\n",
    "                        # # matches1 = matches1.T[None, :, :]\n",
    "                        matches2 = points2[:2, matches[1, :].astype(int)]\n",
    "                        print(torch.tensor(matches1.copy()).unsqueeze(-1).view(2, -1, 1).shape, DVF.shape)\n",
    "\n",
    "                        # transform the points using the displacement field\n",
    "                        matches1_transformed_DVF = transform_points_from_DVF(torch.tensor(matches1.copy()).unsqueeze(-1).view(2, -1, 1), \n",
    "                                DVF, torch.tensor(img_transformed_BW).unsqueeze(0).unsqueeze(0))\n",
    "                        # print(f'Img {i}, diff: {matches1_transformed_DVF[:, 0] - matches2[:, 0]}')\n",
    "                        # points_list.append(matches1_transformed_DVF[:, 0] - matches2[:, 0])\n",
    "\n",
    "                        # --------------- move from here ^^up\n",
    "                        # ---------------\n",
    "\n",
    "                        # create a dataframe with the matches\n",
    "                        # print(matches1.shape, matches2.shape, matches1_transformed_DVF.shape)\n",
    "                        if len(matches1_transformed_DVF.shape) == 3:\n",
    "                            matches1_transformed_DVF = matches1_transformed_DVF.squeeze(-1)\n",
    "                        df = pd.DataFrame({'x1': matches1[0, :], 'y1': matches1[1, :],\n",
    "                                        'x2': matches2[0, :], 'y2': matches2[1, :],\n",
    "                                        'x2_': matches1_transformed_DVF[0, :], 'y2_': matches1_transformed_DVF[1, :]})\n",
    "                        save_name = os.path.join(output_dir, f\"img_{i}_{j}_{k}_{l}_keypoints.csv\")\n",
    "                        df.to_csv(save_name, index=False)\n",
    "\n",
    "                        with open(csv_file, 'a', newline='') as csvfile:\n",
    "                            writer = csv.writer(csvfile)\n",
    "                            writer.writerow([original_image_path, transformed_image_path, \n",
    "                                            M[0, 0], M[0, 1], M[0, 2], M[1, 0], M[1, 1], M[1, 2], \n",
    "                                            M[2, 0], M[2, 1], M[2, 2], img_path, save_name])\n",
    "\n",
    "    print(f\"\\nGenerated {(i+1)*(num_images)} images\")\n",
    "    # print mean absolute error of the points\n",
    "    # print('MAE point location error:', np.mean(np.abs(np.array(points_list))))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Perspective: [0.00017164176132949396, -0.0005073062872683412]\n",
      "torch.Size([1, 1, 256, 256]) torch.Size([1, 256, 256, 2])\n",
      "(256, 102) (256, 97)\n",
      "torch.Size([2, 80, 1]) torch.Size([1, 256, 256, 2])\n",
      "DVF shape: (1, 256, 256, 2), points shape: (2, 80, 1)\n",
      "[[ 52.]\n",
      " [179.]]\n",
      "[[-0.5557833  0.5291306]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/pakpoom/codes/spppt-2/.venv/lib/python3.10/site-packages/torch/functional.py:504: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at ../aten/src/ATen/native/TensorShape.cpp:3526.)\n",
      "  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "could not broadcast input array from shape (2,2) into shape (2,1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 9\u001b[0m\n\u001b[1;32m      3\u001b[0m os\u001b[38;5;241m.\u001b[39mmakedirs(output_dir, exist_ok\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m# plot_dir = \"Dataset/synthetic_eye_dataset_train/plot\"\u001b[39;00m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m# os.makedirs(plot_dir, exist_ok=True)\u001b[39;00m\n\u001b[1;32m      7\u001b[0m \n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m# generate synthetic images for each source training image\u001b[39;00m\n\u001b[0;32m----> 9\u001b[0m \u001b[43mgenerate_affine_transformed_images_multiple\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage_list\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mDataset/synth_eye_perspetive_easy.csv\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_dir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_images\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodify\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[8], line 165\u001b[0m, in \u001b[0;36mgenerate_affine_transformed_images_multiple\u001b[0;34m(img_list, csv_file, output_dir, num_images, modify)\u001b[0m\n\u001b[1;32m    162\u001b[0m \u001b[38;5;28mprint\u001b[39m(torch\u001b[38;5;241m.\u001b[39mtensor(matches1\u001b[38;5;241m.\u001b[39mcopy())\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mshape, DVF\u001b[38;5;241m.\u001b[39mshape)\n\u001b[1;32m    164\u001b[0m \u001b[38;5;66;03m# transform the points using the displacement field\u001b[39;00m\n\u001b[0;32m--> 165\u001b[0m matches1_transformed_DVF \u001b[38;5;241m=\u001b[39m \u001b[43mtransform_points_from_DVF\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmatches1\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43munsqueeze\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mview\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m    166\u001b[0m \u001b[43m        \u001b[49m\u001b[43mDVF\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg_transformed_BW\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43munsqueeze\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43munsqueeze\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    167\u001b[0m \u001b[38;5;66;03m# print(f'Img {i}, diff: {matches1_transformed_DVF[:, 0] - matches2[:, 0]}')\u001b[39;00m\n\u001b[1;32m    168\u001b[0m \u001b[38;5;66;03m# points_list.append(matches1_transformed_DVF[:, 0] - matches2[:, 0])\u001b[39;00m\n\u001b[1;32m    169\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    173\u001b[0m \u001b[38;5;66;03m# create a dataframe with the matches\u001b[39;00m\n\u001b[1;32m    174\u001b[0m \u001b[38;5;66;03m# print(matches1.shape, matches2.shape, matches1_transformed_DVF.shape)\u001b[39;00m\n\u001b[1;32m    175\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(matches1_transformed_DVF\u001b[38;5;241m.\u001b[39mshape) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m3\u001b[39m:\n",
      "File \u001b[0;32m~/codes/spppt-2/utils/utils1.py:792\u001b[0m, in \u001b[0;36mtransform_points_from_DVF\u001b[0;34m(points_, DVF, image)\u001b[0m\n\u001b[1;32m    787\u001b[0m \u001b[38;5;66;03m# print(\"points_ shape:\", points_.shape, \"M shape:\", M.shape, \"image shape:\", image.shape)\u001b[39;00m\n\u001b[1;32m    788\u001b[0m \u001b[38;5;66;03m# points = [transform_points_DVF_unbatched(points_[:, :, b], M[b, :, :], image[b, :, :, :]) for b in range(B)]\u001b[39;00m\n\u001b[1;32m    789\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m b \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(B):\n\u001b[1;32m    790\u001b[0m     \u001b[38;5;66;03m# print(\"points_ shape:\", points_[:, :, b].shape, \"M shape:\", M[b].shape, \"image shape:\", image[b].shape)\u001b[39;00m\n\u001b[1;32m    791\u001b[0m     points_[:, :, b] \u001b[38;5;241m=\u001b[39m \\\n\u001b[0;32m--> 792\u001b[0m         \u001b[43mtransform_points_from_DVF_unbatched\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpoints_\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mb\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mview\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mN\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m    793\u001b[0m \u001b[43m                                    \u001b[49m\u001b[43mDVF\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m    794\u001b[0m \u001b[43m                                    \u001b[49m\u001b[43mimage\u001b[49m\u001b[43m[\u001b[49m\u001b[43mb\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mview\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mimage\u001b[49m\u001b[43m[\u001b[49m\u001b[43mb\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msize\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mimage\u001b[49m\u001b[43m[\u001b[49m\u001b[43mb\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msize\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    795\u001b[0m \u001b[38;5;66;03m# print(points_.shape)\u001b[39;00m\n\u001b[1;32m    796\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m points_\n",
      "File \u001b[0;32m~/codes/spppt-2/utils/utils1.py:749\u001b[0m, in \u001b[0;36mtransform_points_from_DVF_unbatched\u001b[0;34m(points_, DVF, image)\u001b[0m\n\u001b[1;32m    747\u001b[0m     \u001b[38;5;28mprint\u001b[39m(points[:, i])\n\u001b[1;32m    748\u001b[0m     \u001b[38;5;28mprint\u001b[39m(DVF[:, \u001b[38;5;28mint\u001b[39m(points[\u001b[38;5;241m1\u001b[39m, i]), \u001b[38;5;28mint\u001b[39m(points[\u001b[38;5;241m0\u001b[39m, i])])\n\u001b[0;32m--> 749\u001b[0m     \u001b[43mpoints\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;241m=\u001b[39m points[:, i] \u001b[38;5;241m-\u001b[39m DVF[:, \u001b[38;5;28mint\u001b[39m(points[\u001b[38;5;241m1\u001b[39m, i]), \u001b[38;5;28mint\u001b[39m(points[\u001b[38;5;241m0\u001b[39m, i])]\n\u001b[1;32m    750\u001b[0m     \u001b[38;5;66;03m# print(points[:, i])\u001b[39;00m\n\u001b[1;32m    751\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mIndexError\u001b[39;00m:\n\u001b[1;32m    752\u001b[0m     \u001b[38;5;66;03m# if both points are outside the image\u001b[39;00m\n\u001b[1;32m    753\u001b[0m     \u001b[38;5;66;03m# find the nearest point inside the image and apply the transformation\u001b[39;00m\n",
      "\u001b[0;31mValueError\u001b[0m: could not broadcast input array from shape (2,2) into shape (2,1)"
     ]
    }
   ],
   "source": [
    "# Define parameters\n",
    "output_dir = \"Dataset/synth_eye_perspetive_easy\"  # Output directory\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# plot_dir = \"Dataset/synthetic_eye_dataset_train/plot\"\n",
    "# os.makedirs(plot_dir, exist_ok=True)\n",
    "\n",
    "# generate synthetic images for each source training image\n",
    "generate_affine_transformed_images_multiple(image_list,\n",
    "    'Dataset/synth_eye_perspetive_easy.csv', output_dir, num_images=3, modify=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Define parameters\n",
    "# image_size = (512, 512)  # Size of the images\n",
    "# output_dir = \"Dataset/synthetic_eye_dataset_train_multiple\"  # Output directory\n",
    "# os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# # plot_dir = \"Dataset/synthetic_eye_dataset_train_multiple/plot\"\n",
    "# # os.makedirs(plot_dir, exist_ok=True)\n",
    "\n",
    "\n",
    "# # generate synthetic images for each source training image\n",
    "# generate_affine_transformed_images_multiple(image_list,'dataset_eye_synth_train_multiple.csv', output_dir, num_images=2, modify=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_12187/686737539.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_test['image_path'] = df_test['Directory'] + '/' + df_test['source']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Generated 100 images\n"
     ]
    }
   ],
   "source": [
    "# do the same for the test images\n",
    "df_test = df[df['training'] == 1]\n",
    "df_test['image_path'] = df_test['Directory'] + '/' + df_test['source']\n",
    "image_list_test = df_test['image_path'].unique()[:50]\n",
    "\n",
    "# Define parameters\n",
    "output_dir = \"Dataset/synth_eye_mix_test\"  # Output directory\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# plot_dir = \"Dataset/synthetic_eye_dataset_test/plot\"\n",
    "# os.makedirs(plot_dir, exist_ok=True)\n",
    "\n",
    "# generate synthetic images for each source test image\n",
    "generate_affine_transformed_images_multiple(image_list_test, 'Dataset/synth_eye_mix_test.csv', \n",
    "                                            output_dir, df_test_csv, num_images=2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Define parameters\n",
    "# image_size = (512, 512)  # Size of the images\n",
    "# output_dir = \"Dataset/synthetic_eye_dataset_test_multiple\"  # Output directory\n",
    "# os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# # plot_dir = \"Dataset/synthetic_eye_dataset_test/plot\"\n",
    "# # os.makedirs(plot_dir, exist_ok=True)\n",
    "\n",
    "# # generate synthetic images for each source test image\n",
    "# generate_affine_transformed_images_multiple(image_list_test, 'dataset_eye_synth_test_scaling.csv', output_dir, num_images=1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spppt",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
