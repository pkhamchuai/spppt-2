{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5040, 9)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>source</th>\n",
       "      <th>Source ROI</th>\n",
       "      <th>target</th>\n",
       "      <th>Target ROI</th>\n",
       "      <th>training</th>\n",
       "      <th>Warped target images</th>\n",
       "      <th>Warped target ROI</th>\n",
       "      <th>Execution time</th>\n",
       "      <th>Directory</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2011248_20161215__L_b2.jpg</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2011248_20161215__L_b1.jpg</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Dataset/Dataset-processed/15-12-2559/2011248/Lb</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2011248_20161215__L_b2.jpg</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2011248_20161215__L_b3.jpg</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Dataset/Dataset-processed/15-12-2559/2011248/Lb</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2011248_20161215__L_b1.jpg</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2011248_20161215__L_b3.jpg</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Dataset/Dataset-processed/15-12-2559/2011248/Lb</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2011248_20161215__L_c2.jpg</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2011248_20161215__L_c1.jpg</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Dataset/Dataset-processed/15-12-2559/2011248/Lc</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2011248_20161215__L_c2.jpg</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2011248_20161215__L_c3.jpg</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Dataset/Dataset-processed/15-12-2559/2011248/Lc</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       source  Source ROI                      target  \\\n",
       "0  2011248_20161215__L_b2.jpg         NaN  2011248_20161215__L_b1.jpg   \n",
       "1  2011248_20161215__L_b2.jpg         NaN  2011248_20161215__L_b3.jpg   \n",
       "2  2011248_20161215__L_b1.jpg         NaN  2011248_20161215__L_b3.jpg   \n",
       "3  2011248_20161215__L_c2.jpg         NaN  2011248_20161215__L_c1.jpg   \n",
       "4  2011248_20161215__L_c2.jpg         NaN  2011248_20161215__L_c3.jpg   \n",
       "\n",
       "   Target ROI  training  Warped target images  Warped target ROI  \\\n",
       "0         NaN         1                   NaN                NaN   \n",
       "1         NaN         1                   NaN                NaN   \n",
       "2         NaN         1                   NaN                NaN   \n",
       "3         NaN         0                   NaN                NaN   \n",
       "4         NaN         0                   NaN                NaN   \n",
       "\n",
       "   Execution time                                        Directory  \n",
       "0             NaN  Dataset/Dataset-processed/15-12-2559/2011248/Lb  \n",
       "1             NaN  Dataset/Dataset-processed/15-12-2559/2011248/Lb  \n",
       "2             NaN  Dataset/Dataset-processed/15-12-2559/2011248/Lb  \n",
       "3             NaN  Dataset/Dataset-processed/15-12-2559/2011248/Lc  \n",
       "4             NaN  Dataset/Dataset-processed/15-12-2559/2011248/Lc  "
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# read the file 'dataset_reg_pair_filled.csv' and generate synthetic data\n",
    "# first read the file, then make a list of the source training images\n",
    "# then for each image, generate 10 synthetic images with random affine transformation\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "import os\n",
    "import random\n",
    "import csv\n",
    "from PIL import Image\n",
    "import shutil\n",
    "\n",
    "import torch\n",
    "import torchvision.transforms.functional as F\n",
    "from PIL import Image\n",
    "\n",
    "from utils.utils0 import tensor_affine_transform, transform_to_displacement_field\n",
    "from utils.utils1 import transform_points_DVF, transform_points_from_DVF_unbatched, transform_points_from_DVF, plot_grid\n",
    "from utils.SuperPoint import SuperPointFrontend, PointTracker\n",
    "superpoint = SuperPointFrontend('utils/superpoint_v1.pth', nms_dist=4,\n",
    "                          conf_thresh=0.015, nn_thresh=0.7, cuda=True)\n",
    "\n",
    "# read the file\n",
    "df = pd.read_csv('Dataset/dataset_reg_pair_filled.csv')\n",
    "print(df.shape)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_177603/1623562252.py:7: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_train['image_path'] = df_train['Directory'] + '/' + df_train['source']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>source</th>\n",
       "      <th>Source ROI</th>\n",
       "      <th>target</th>\n",
       "      <th>Target ROI</th>\n",
       "      <th>training</th>\n",
       "      <th>Warped target images</th>\n",
       "      <th>Warped target ROI</th>\n",
       "      <th>Execution time</th>\n",
       "      <th>Directory</th>\n",
       "      <th>image_path</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2011248_20161215__L_c2.jpg</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2011248_20161215__L_c1.jpg</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Dataset/Dataset-processed/15-12-2559/2011248/Lc</td>\n",
       "      <td>Dataset/Dataset-processed/15-12-2559/2011248/L...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2011248_20161215__L_c2.jpg</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2011248_20161215__L_c3.jpg</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Dataset/Dataset-processed/15-12-2559/2011248/Lc</td>\n",
       "      <td>Dataset/Dataset-processed/15-12-2559/2011248/L...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2011248_20161215__L_c1.jpg</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2011248_20161215__L_c3.jpg</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Dataset/Dataset-processed/15-12-2559/2011248/Lc</td>\n",
       "      <td>Dataset/Dataset-processed/15-12-2559/2011248/L...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2011248_20161215__R_b2.jpg</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2011248_20161215__R_b3.jpg</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Dataset/Dataset-processed/15-12-2559/2011248/Rb</td>\n",
       "      <td>Dataset/Dataset-processed/15-12-2559/2011248/R...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2011248_20161215__R_b2.jpg</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2011248_20161215__R_b1.jpg</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Dataset/Dataset-processed/15-12-2559/2011248/Rb</td>\n",
       "      <td>Dataset/Dataset-processed/15-12-2559/2011248/R...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       source  Source ROI                      target  \\\n",
       "3  2011248_20161215__L_c2.jpg         NaN  2011248_20161215__L_c1.jpg   \n",
       "4  2011248_20161215__L_c2.jpg         NaN  2011248_20161215__L_c3.jpg   \n",
       "5  2011248_20161215__L_c1.jpg         NaN  2011248_20161215__L_c3.jpg   \n",
       "6  2011248_20161215__R_b2.jpg         NaN  2011248_20161215__R_b3.jpg   \n",
       "7  2011248_20161215__R_b2.jpg         NaN  2011248_20161215__R_b1.jpg   \n",
       "\n",
       "   Target ROI  training  Warped target images  Warped target ROI  \\\n",
       "3         NaN         0                   NaN                NaN   \n",
       "4         NaN         0                   NaN                NaN   \n",
       "5         NaN         0                   NaN                NaN   \n",
       "6         NaN         0                   NaN                NaN   \n",
       "7         NaN         0                   NaN                NaN   \n",
       "\n",
       "   Execution time                                        Directory  \\\n",
       "3             NaN  Dataset/Dataset-processed/15-12-2559/2011248/Lc   \n",
       "4             NaN  Dataset/Dataset-processed/15-12-2559/2011248/Lc   \n",
       "5             NaN  Dataset/Dataset-processed/15-12-2559/2011248/Lc   \n",
       "6             NaN  Dataset/Dataset-processed/15-12-2559/2011248/Rb   \n",
       "7             NaN  Dataset/Dataset-processed/15-12-2559/2011248/Rb   \n",
       "\n",
       "                                          image_path  \n",
       "3  Dataset/Dataset-processed/15-12-2559/2011248/L...  \n",
       "4  Dataset/Dataset-processed/15-12-2559/2011248/L...  \n",
       "5  Dataset/Dataset-processed/15-12-2559/2011248/L...  \n",
       "6  Dataset/Dataset-processed/15-12-2559/2011248/R...  \n",
       "7  Dataset/Dataset-processed/15-12-2559/2011248/R...  "
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# make a list of the unique source training images that has 'training' = 0\n",
    "# each image path consists of image directory, image name\n",
    "\n",
    "df_train = df[df['training'] == 0]\n",
    "\n",
    "# create a new df consists of image directory and image name concatenated\n",
    "df_train['image_path'] = df_train['Directory'] + '/' + df_train['source']\n",
    "df_train.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4132, 10)\n",
      "50\n",
      "Dataset/Dataset-processed/15-12-2559/2011248/Lc/2011248_20161215__L_c2.jpg\n"
     ]
    }
   ],
   "source": [
    "print(df_train.shape)\n",
    "# make a list of the unique values in the column 'image_path'\n",
    "image_list = df_train['image_path'].unique()[:50]\n",
    "print(len(image_list))\n",
    "print(image_list[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_translation = 0.1  # Minimum translation in pixels\n",
    "max_translation = 0.2  # Maximum translation in pixels\n",
    "max_rotation = 20  # Maximum rotation in degrees\n",
    "max_shear = 10  # Maximum shear in degrees\n",
    "min_scale = 0.85  # Minimum scale factor\n",
    "max_scale = 1.15  # Maximum scale factor\n",
    "max_perspective = 0.1  # Maximum perspective distortion\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_train.to_csv('Dataset/synth_eye_train.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_test.to_csv('Dataset/synth_eye_test.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: a fuction for warping image using perspective transformation\n",
    "def warp_image_perspective_cv2(image, M):\n",
    "    # image: a numpy array of shape (H, W, 3)\n",
    "    # M: a numpy array of shape (3, 3)\n",
    "    # return: a numpy array of shape (H, W, 3)\n",
    "    H, W, _ = image.shape\n",
    "    warped_image = cv2.warpPerspective(image, M, (W, H))\n",
    "    return warped_image\n",
    "\n",
    "def warp_image_perspective_pytorch(image, M):\n",
    "    '''\n",
    "    - Grid Generation: We generate a grid of coordinates (x, y) for the image.\n",
    "    - Homography Application: We apply the homography matrix M to these coordinates to get transformed coordinates.\n",
    "    - Grid Sampling: We then use grid_sample to sample the image at these transformed coordinates.\n",
    "    '''\n",
    "\n",
    "    H, W = image.shape\n",
    "    M = torch.from_numpy(M).float()\n",
    "\n",
    "    # Generate a grid of coordinates (x, y) for the image\n",
    "    y, x = torch.meshgrid(torch.arange(H), torch.arange(W))\n",
    "    ones = torch.ones_like(x)\n",
    "    \n",
    "    x_transformed = (M[0, 0]*x + M[0, 1]*x + M[0, 1]*x)/(M[2, 0]*x + M[2, 1]*x + 1)\n",
    "    y_transformed = (M[1, 0]*y + M[1, 1]*y + M[1, 1]*y)/(M[2, 0]*y + M[2, 1]*y + 1)\n",
    "    transformed_grid = torch.stack([x, y], dim=-1).float()\n",
    "\n",
    "    # grid = torch.stack([x, y, ones], dim=-1).float()  # Shape (H, W, 3)\n",
    "    \n",
    "    # # Flatten the grid to (H*W, 3)\n",
    "    # grid_flat = grid.view(-1, 3).t()  # Shape (3, H*W)\n",
    "    \n",
    "    # # Apply the homography transformation\n",
    "    # # transformed_grid = M @ grid_flat  # Shape (3, H*W)\n",
    "\n",
    "    \n",
    "    # # Convert from homogeneous to Cartesian coordinates\n",
    "    # transformed_grid = transformed_grid / transformed_grid[2, :]\n",
    "    # transformed_grid = transformed_grid[:2, :].t()  # Shape (H*W, 2)\n",
    "    \n",
    "    # Reshape the transformed coordinates to the original image shape\n",
    "    transformed_grid_ = transformed_grid.view(H, W, 2).clone()\n",
    "    transformed_grid = transformed_grid.view(H, W, 2)\n",
    "    \n",
    "    # Normalize coordinates to be in range [-1, 1] for grid_sample\n",
    "    transformed_grid[..., 0] = (transformed_grid[..., 0] / (W - 1)) * 2 - 1\n",
    "    transformed_grid[..., 1] = (transformed_grid[..., 1] / (H - 1)) * 2 - 1\n",
    "    \n",
    "    # Add batch dimension and channel dimension, permute to (_, H, W, 1)\n",
    "    transformed_grid = transformed_grid.unsqueeze(0)\n",
    "    \n",
    "    if len(image.shape) == 2:\n",
    "        image = torch.from_numpy(image).float().unsqueeze(0).unsqueeze(0)\n",
    "    # image = image.permute(2, 0, 1).unsqueeze(0)\n",
    "    \n",
    "    # Sample the image using the transformed grid\n",
    "    warped_image = torch.nn.functional.grid_sample(image, transformed_grid, align_corners=False)\n",
    "    \n",
    "    # Remove batch and channel dimensions and convert to numpy\n",
    "    # warped_image = warped_image.squeeze(0).squeeze(0).numpy()\n",
    "    \n",
    "    return warped_image, transformed_grid_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Apply all transformations to the eye images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Version 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def calculate_transformed_points(width, height, affine_params, perspective_params):\n",
    "#     \"\"\"\n",
    "#     Calculate the transformed points based on affine and perspective parameters.\n",
    "#     :param width: Image width\n",
    "#     :param height: Image height\n",
    "#     :param affine_params: A list of 6 affine transformation parameters [a, b, c, d, e, f]\n",
    "#     :param perspective_params: A list of 2 perspective transformation parameters [p1, p2]\n",
    "#     :return: Start points and transformed end points\n",
    "#     \"\"\"\n",
    "#     # Original corner points of the image\n",
    "#     startpoints = np.array([[0, 0], [width, 0], [width, height], [0, height]], dtype=np.float32)\n",
    "\n",
    "#     # Create the affine transformation matrix\n",
    "#     a, b, c, d, e, f = affine_params\n",
    "#     M_affine = np.array([\n",
    "#         [a, b, c],\n",
    "#         [d, e, f],\n",
    "#         [0, 0, 1]\n",
    "#     ], dtype=np.float32)\n",
    "\n",
    "#     # Create the perspective transformation matrix\n",
    "#     p1, p2 = perspective_params\n",
    "#     M_perspective = np.array([\n",
    "#         [1, 0, 0],\n",
    "#         [0, 1, 0],\n",
    "#         [p1, p2, 1]\n",
    "#     ], dtype=np.float32)\n",
    "\n",
    "#     # Combine the affine and perspective transformations\n",
    "#     M_combined = np.dot(M_perspective, M_affine)\n",
    "\n",
    "#     # Transform the corner points using the combined matrix\n",
    "#     endpoints = []\n",
    "#     for point in startpoints:\n",
    "#         transformed_point = np.dot(M_combined, [point[0], point[1], 1])\n",
    "#         transformed_point /= transformed_point[2]  # Normalize the homogeneous coordinate\n",
    "#         endpoints.append(transformed_point[:2])\n",
    "    \n",
    "#     return startpoints.tolist(), endpoints, M_combined\n",
    "\n",
    "# def apply_perspective_transform(image, startpoints, endpoints):\n",
    "#     \"\"\"\n",
    "#     Apply the perspective transformation using the calculated start and end points.\n",
    "#     \"\"\"\n",
    "#     # if the image is not a PIL image, convert it to a PIL image\n",
    "#     if not isinstance(image, Image.Image):\n",
    "#         image = Image.fromarray(image)\n",
    "#     transformed_image = F.perspective(image, startpoints, endpoints, interpolation=Image.BICUBIC)\n",
    "#     return transformed_image\n",
    "\n",
    "# # Load your image\n",
    "# image = Image.open('path_to_your_image.jpg')\n",
    "\n",
    "# # Define the 6 affine parameters (rotation, scaling, translation, etc.)\n",
    "# affine_params = [1, 0, 0, 0, 1, 0]  # Identity transform for simplicity (no affine change)\n",
    "\n",
    "# # Define the 2 perspective parameters\n",
    "# perspective_params = [0.001, 0.001]  # Small perspective distortion\n",
    "\n",
    "# # Calculate the transformed points\n",
    "# width, height = image.size\n",
    "# startpoints, endpoints = calculate_transformed_points(width, height, affine_params, perspective_params)\n",
    "\n",
    "# # Apply the perspective transformation\n",
    "# warped_image = apply_perspective_transform(image, startpoints, endpoints)\n",
    "\n",
    "# # Display the transformed image\n",
    "# warped_image.show()\n",
    "\n",
    "# # Print the start and end points\n",
    "# print(\"Start Points:\\n\", startpoints)\n",
    "# print(\"End Points:\\n\", endpoints)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Version 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision.transforms.functional as F\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "\n",
    "def calculate_endpoints(width, height, params):\n",
    "    \"\"\"\n",
    "    Calculate the four endpoints for the perspective transformation given\n",
    "    affine and perspective parameters.\n",
    "    \"\"\"\n",
    "    h11, h12, h13, h21, h22, h23, h31, h32, _ = params\n",
    "\n",
    "    # Corners of the original image\n",
    "    topleft = np.array([0, 0, 1])\n",
    "    topright = np.array([width, 0, 1])\n",
    "    bottomright = np.array([width, height, 1])\n",
    "    bottomleft = np.array([0, height, 1])\n",
    "\n",
    "    # Create the perspective transformation matrix H\n",
    "    H = np.array([\n",
    "        [h11, h12, h13],\n",
    "        [h21, h22, h23],\n",
    "        [h31, h32, 1]\n",
    "    ], dtype=np.float32)\n",
    "\n",
    "    # Apply the transformation to each corner\n",
    "    topleft_transformed = np.dot(H, topleft)\n",
    "    topright_transformed = np.dot(H, topright)\n",
    "    bottomright_transformed = np.dot(H, bottomright)\n",
    "    bottomleft_transformed = np.dot(H, bottomleft)\n",
    "\n",
    "    # Normalize to Cartesian coordinates\n",
    "    topleft_transformed /= topleft_transformed[2]\n",
    "    topright_transformed /= topright_transformed[2]\n",
    "    bottomright_transformed /= bottomright_transformed[2]\n",
    "    bottomleft_transformed /= bottomleft_transformed[2]\n",
    "\n",
    "    # Extract (x, y) coordinates\n",
    "    startpoints = [[0, 0], [width, 0], [width, height], [0, height]]\n",
    "    endpoints = [\n",
    "        topleft_transformed[:2].tolist(),\n",
    "        topright_transformed[:2].tolist(),\n",
    "        bottomright_transformed[:2].tolist(),\n",
    "        bottomleft_transformed[:2].tolist()\n",
    "    ]\n",
    "\n",
    "    return startpoints, endpoints\n",
    "\n",
    "def get_random_perspective_params(width, height, distortion_scale):\n",
    "    \"\"\"\n",
    "    Generate random offsets for the four corners to create a perspective warp.\n",
    "    \"\"\"\n",
    "    topleft = (random.uniform(0, distortion_scale) * width, random.uniform(0, distortion_scale) * height)\n",
    "    topright = (random.uniform(1 - distortion_scale, 1) * width, random.uniform(0, distortion_scale) * height)\n",
    "    botright = (random.uniform(1 - distortion_scale, 1) * width, random.uniform(1 - distortion_scale, 1) * height)\n",
    "    botleft = (random.uniform(0, distortion_scale) * width, random.uniform(1 - distortion_scale, 1) * height)\n",
    "\n",
    "    startpoints = [[0, 0], [width, 0], [width, height], [0, height]]\n",
    "    endpoints = [topleft, topright, botright, botleft]\n",
    "    return startpoints, endpoints\n",
    "\n",
    "def calculate_perspective_matrix(startpoints, endpoints):\n",
    "    \"\"\"\n",
    "    Calculate the perspective transformation matrix from start and end points.\n",
    "    \"\"\"\n",
    "    startpoints = torch.tensor(startpoints, dtype=torch.float32)\n",
    "    endpoints = torch.tensor(endpoints, dtype=torch.float32)\n",
    "    \n",
    "    # Solve the linear system to find the perspective transformation matrix\n",
    "    A = []\n",
    "    for (x, y), (u, v) in zip(startpoints, endpoints):\n",
    "        A.append([-x, -y, -1, 0, 0, 0, x*u, y*u, u])\n",
    "        A.append([0, 0, 0, -x, -y, -1, x*v, y*v, v])\n",
    "    A = torch.tensor(A, dtype=torch.float32)\n",
    "    _, _, V = torch.svd(A)\n",
    "    H = V[:, -1].reshape(3, 3)\n",
    "    return H\n",
    "\n",
    "# def apply_perspective_transform(image, perspective_matrix):\n",
    "#     \"\"\"\n",
    "#     Apply the perspective transformation using the provided matrix.\n",
    "#     \"\"\"\n",
    "#     width, height = image.shape[1], image.shape[0]\n",
    "#     # Create a meshgrid for the image coordinates\n",
    "#     y, x = torch.meshgrid(torch.arange(height), torch.arange(width))\n",
    "#     xy1 = torch.stack([x.flatten(), y.flatten(), torch.ones_like(x.flatten())], dim=0)  # Shape (3, H*W)\n",
    "\n",
    "#     # Apply the perspective transformation\n",
    "#     transformed_xy1 = torch.matmul(perspective_matrix, xy1)\n",
    "#     transformed_xy1 = transformed_xy1[:2] / transformed_xy1[2]  # Normalize by the third coordinate\n",
    "#     transformed_xy1 = transformed_xy1.view(2, height, width).permute(1, 2, 0)  # Shape (H, W, 2)\n",
    "\n",
    "#     # Normalize grid to [-1, 1]\n",
    "#     transformed_xy1[..., 0] = (transformed_xy1[..., 0] / (width - 1)) * 2 - 1\n",
    "#     transformed_xy1[..., 1] = (transformed_xy1[..., 1] / (height - 1)) * 2 - 1\n",
    "\n",
    "#     # Apply the grid sample\n",
    "#     grid = transformed_xy1.unsqueeze(0)\n",
    "#     image_tensor = F.to_tensor(image).unsqueeze(0)\n",
    "#     transformed_image_tensor = F.grid_sample(image_tensor, grid, align_corners=False)\n",
    "#     transformed_image = F.to_pil_image(transformed_image_tensor.squeeze(0))\n",
    "#     return transformed_image\n",
    "\n",
    "def apply_perspective_transform(image, startpoints, endpoints):\n",
    "    \"\"\"\n",
    "    Apply a perspective transformation using calculated start and end points.\n",
    "    \"\"\"\n",
    "    # if the image is not a PIL image, convert it to a PIL image\n",
    "    if not isinstance(image, Image.Image):\n",
    "        image = Image.fromarray(image)\n",
    "\n",
    "    image = F.perspective(image, startpoints, endpoints, interpolation=Image.BICUBIC)\n",
    "    \n",
    "    # Calculate the minimum x and y coordinates\n",
    "    min_x = max(0, endpoints[0][0], endpoints[3][0])\n",
    "    min_y = max(0, endpoints[0][1], endpoints[1][1])\n",
    "    \n",
    "    # Calculate the width and height of the cropped image\n",
    "    width = min(256, endpoints[1][0], endpoints[2][0])\n",
    "    height = min(256, endpoints[2][1], endpoints[3][1])\n",
    "    \n",
    "    # print(f\"crop: {min_x}, {min_y}, {width}, {height}\")\n",
    "    # Crop the image\n",
    "    image = image.crop((min_x, min_y, min_x + width, min_y + height))\n",
    "    # print(image.size)\n",
    "    \n",
    "    return image\n",
    "\n",
    "def transform_points(points, H):\n",
    "    \"\"\"\n",
    "    Transform a single point using a homography matrix.\n",
    "    \"\"\"\n",
    "    # if points are not in homogeneous form, convert them\n",
    "    # print(points.shape)\n",
    "    if len(points) == 2:\n",
    "        points = np.array([points[0], points[1], np.ones_like(points[0])], dtype=np.float32)\n",
    "    transformed_point = np.dot(H, points)\n",
    "    transformed_point /= transformed_point[2]\n",
    "    return transformed_point[:2]\n",
    "\n",
    "# Load your image\n",
    "# image = Image.open('path_to_your_image.jpg')\n",
    "\n",
    "# # Define the 8 random parameters for the homography matrix\n",
    "# params = np.random.uniform(-1, 1, size=8)\n",
    "\n",
    "# # Calculate the start and end points\n",
    "# width, height = image.size\n",
    "# startpoints, endpoints = calculate_endpoints(width, height, params)\n",
    "\n",
    "# # Apply the perspective transformation\n",
    "# warped_image = apply_perspective_transform(image, startpoints, endpoints)\n",
    "\n",
    "# # Display the transformed image\n",
    "# warped_image.show()\n",
    "\n",
    "# # Print the start and end points\n",
    "# print(\"Start Points:\\n\", startpoints)\n",
    "# print(\"End Points:\\n\", endpoints)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transformation function codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_affine_transformed_images_multiple(img_list, csv_file, output_dir, num_images=5, modify=False):\n",
    "    # delete all files and subdirectories in the output directory\n",
    "    shutil.rmtree(output_dir)\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    # create a list to store different point locations\n",
    "    points_list = []\n",
    "\n",
    "    # Initialize the CSV file with a header\n",
    "    with open(csv_file, 'w', newline='') as csvfile:\n",
    "        writer = csv.writer(csvfile)\n",
    "        # write the header, perspective parameters, image path, and keypoints\n",
    "        writer.writerow([\"source\", \"target\", \"M01\", \"M02\", \"M03\", \"M04\", \"M05\",\n",
    "            \"M06\", \"M07\", \"M08\", \"M09\",\n",
    "            \"image_path\", \"keypoints\"])\n",
    "\n",
    "    # Loop over the images, read the image, \n",
    "    # apply affine transformation and save it\n",
    "    for i, img_path in enumerate(img_list):\n",
    "        # if i > len(img_list)/2:\n",
    "        #     break\n",
    "        # Read the image as grayscale using cv2\n",
    "        image = cv2.imread(img_path, cv2.IMREAD_GRAYSCALE)\n",
    "\n",
    "        # Save original image\n",
    "        original_image_path = os.path.join(output_dir, f\"img_{i}_original.png\")\n",
    "\n",
    "        # take 90% of the image\n",
    "        image_base = image[int(image.shape[0]*0.1):int(image.shape[0]*0.9), \n",
    "                        int(image.shape[1]*0.1):int(image.shape[1]*0.9)]\n",
    "        # resize image to 256x256\n",
    "        image_base = cv2.resize(image_base, (256, 256))\n",
    "\n",
    "        cv2.imwrite(original_image_path, image_base + np.random.uniform(-0.01, 0.01, image_base.shape))\n",
    "\n",
    "        # Convert the transformed image to a numpy array\n",
    "        # img_transformed = np.array(img_transformed)\n",
    "        image_base = np.array(Image.fromarray(image_base).convert('L'))\n",
    "\n",
    "        tracker = PointTracker(5, nn_thresh=0.7)\n",
    "        points1, desc1, _ = superpoint(image_base.astype(np.float32)/255)\n",
    "\n",
    "        for j in range(num_images):\n",
    "            # random (2x3) affine transformation matrix\n",
    "            #M = np.array([[1.0, 0.0, np.random()], [0.0, 1.0, 0.0]])\n",
    "            if j == num_images-1:\n",
    "                # pass\n",
    "                img_transformed = image_base.copy()\n",
    "                M = np.array([[1.0, 0.0, 0.0], [0.0, 1.0, 0.0], [0.0, 0.0, 1.0]])\n",
    "                points2 = points1.copy()\n",
    "                desc2 = desc1.copy()\n",
    "                \n",
    "                img_transformed = cv2.resize(img_transformed, (256, 256))\n",
    "                # convert to grayscale\n",
    "                img_transformed_BW = np.array(Image.fromarray(img_transformed).convert('L'))\n",
    "\n",
    "                # # TODO: save heatmaps for other version of network\n",
    "                points2, desc2, _ = superpoint(img_transformed_BW.astype(np.float32)/255)\n",
    "                matches = tracker.nn_match_two_way(desc1, desc2, nn_thresh=0.7)\n",
    "\n",
    "                # # print(desc1.shape, desc2.shape)\n",
    "                matches1 = points1[:2, matches[0, :].astype(int)]\n",
    "                # # matches1 = matches1.T[None, :, :]\n",
    "                matches2 = points2[:2, matches[1, :].astype(int)]\n",
    "                \n",
    "                # transform the points using the displacement field\n",
    "                # print(torch.tensor(M)[None, :, :].shape, torch.tensor(image)[None, None, :, :].shape)\n",
    "                # print(torch.tensor(M).shape, torch.tensor(image).shape, torch.tensor(matches1.copy()).unsqueeze(-1).view(2, -1, 1).shape)\n",
    "                matches1_transformed_DVF = matches1.copy()\n",
    "                # print(f'Img {i}, diff: {matches1_transformed_DVF[:, 0] - matches2[:, 0]}')\n",
    "                # points_list.append(matches1_transformed_DVF[:, 0] - matches2[:, 0])\n",
    "\n",
    "                # add some noise to the transformed image and save it\n",
    "                img_transformed = img_transformed + np.random.uniform(-0.01, 0.01, img_transformed.shape)\n",
    "                if modify: # if modify is True, then add some intensity change to the transformed image\n",
    "                    img_transformed = img_transformed + np.random.normal(1, 0.1, 1)\n",
    "\n",
    "\n",
    "                transformed_image_path = os.path.join(output_dir, f\"img_{i}_transformed_{j}.png\")\n",
    "                cv2.imwrite(transformed_image_path, img_transformed)\n",
    "\n",
    "                # create a dataframe with the matches\n",
    "                # print(matches1.shape, matches2.shape, matches1_transformed_DVF.shape)\n",
    "                if len(matches1_transformed_DVF.shape) == 3:\n",
    "                    matches1_transformed_DVF = matches1_transformed_DVF.squeeze(-1)\n",
    "                df = pd.DataFrame({'x1': matches1[0, :], 'y1': matches1[1, :],\n",
    "                                'x2': matches2[0, :], 'y2': matches2[1, :],\n",
    "                                'x2_': matches1_transformed_DVF[0, :], 'y2_': matches1_transformed_DVF[1, :]})\n",
    "                save_name = os.path.join(output_dir, f\"img_{i}_{j}_keypoints.csv\")\n",
    "                df.to_csv(save_name, index=False)\n",
    "\n",
    "                with open(csv_file, 'a', newline='') as csvfile:\n",
    "                    writer = csv.writer(csvfile)\n",
    "                    writer.writerow([original_image_path, transformed_image_path, \n",
    "                                    M[0, 0], M[0, 1], M[0, 2], M[1, 0], M[1, 1], M[1, 2], \n",
    "                                    M[2, 0], M[2, 1], M[2, 2], img_path, save_name])\n",
    "            \n",
    "            else:\n",
    "                rand_angle = np.random.uniform(0, max_rotation)/180*np.pi\n",
    "                rand_range = 1.2\n",
    "                scaling = [np.random.uniform(1, rand_range), np.random.uniform(1, rand_range)]\n",
    "                # rand_angle = 10/180*np.pi\n",
    "                translate_range = 0.1\n",
    "                translate = [np.random.uniform(-translate_range, translate_range), np.random.uniform(-translate_range, translate_range)]\n",
    "                \n",
    "                # test_random = [1.1, 1.1]\n",
    "                # M = np.array([[1.0 + test_random[0], 0.0, 0.0], [0.0, 1.0 + test_random[1], 0.0]])\n",
    "                scale_power = [[-1, -1], [1, 1], [-1, 1], [1, -1]]\n",
    "                rotation_direction = [-1, 1]\n",
    "                # perspective\n",
    "                rand_range = 0.001\n",
    "                \n",
    "                # perspective = [0.0007316110989513938, 0.0009501206585347142]\n",
    "                # perspective = [0.0, 0]\n",
    "\n",
    "                for k in range(4):\n",
    "                    for l in range(2):\n",
    "                        # power = scale_power[k]\n",
    "                        # sign = rotation_direction[l]\n",
    "                        # perspective = [np.random.uniform(-rand_range, rand_range), np.random.uniform(-rand_range, rand_range)]\n",
    "                        # perspective = [0.0007316110989513938, 0.0]\n",
    "                        # Combined transformation matrix for perspective, rotation, scaling, and translation\n",
    "                        # M = np.array([[sign*scaling[0]**power[0]*np.cos(rand_angle), sign*scaling[1]**power[1]*np.sin(rand_angle), translate[0]],\n",
    "                        #             [-sign*scaling[0]**power[0]*np.sin(rand_angle), sign*scaling[1]**power[1]*np.cos(rand_angle), translate[1]],\n",
    "                        #             [perspective[0], perspective[1], 1.0]])\n",
    "                        # M = np.array([[np.cos(rand_angle)*(scaling[0]**power[0]), -sign*np.sin(rand_angle), translate[0]],\n",
    "                        #             [sign*np.sin(rand_angle), np.cos(rand_angle)*(scaling[1]**power[1]), translate[1]],\n",
    "                        #             [perspective[0], perspective[1], 1.0]])\n",
    "                        \n",
    "                        # print(f'Perspective: {perspective}')\n",
    "                        # M = np.array([[1.0, 0.0, 0.0], [0.0, 1.0, 0.0], [perspective[0], perspective[1], 1.0]])\n",
    "\n",
    "                        # Calculate the transformed points\n",
    "                        width, height = image_base.shape\n",
    "                        # startpoints, endpoints = calculate_endpoints(width, height, M.flatten())\n",
    "                        \n",
    "                        # Get the perspective parameters\n",
    "                        distortion_scale = 0.1\n",
    "                        startpoints, endpoints = get_random_perspective_params(width, height, distortion_scale)\n",
    "                        M = calculate_perspective_matrix(startpoints, endpoints)\n",
    "                        # Apply the perspective transformation\n",
    "                        img_transformed = apply_perspective_transform(image_base, startpoints, endpoints)\n",
    "\n",
    "                        print(f'M: {M}')\n",
    "                        print(f'Startpoints: {startpoints}')\n",
    "                        print(f'Endpoints: {endpoints}\\n')\n",
    "\n",
    "                        # Apply the perspective transformation\n",
    "                        # img_transformed = apply_perspective_transform(image_base, startpoints, endpoints)\n",
    "\n",
    "                        # img_transformed, DVF = warp_image_perspective_pytorch(image_base, M)\n",
    "                        # img_transformed = img_transformed.squeeze(0).squeeze(0).numpy()\n",
    "                        # img_transformed = img_transformed[int(image.shape[0]*0.1):int(image.shape[0]*0.9), \n",
    "                        #                                 int(image.shape[1]*0.1):int(image.shape[1]*0.9)]\n",
    "\n",
    "                        # resize image to 256x256\n",
    "                        img_transformed = cv2.resize(np.array(img_transformed), (256, 256))\n",
    "                        # convert to grayscale\n",
    "                        img_transformed_BW = np.array(Image.fromarray(img_transformed).convert('L'))\n",
    "\n",
    "                        # --------------- to here\n",
    "                        # add some noise to the transformed image and save it\n",
    "                        img_transformed = img_transformed + np.random.uniform(-0.01, 0.01, img_transformed.shape)\n",
    "                        if modify: # if modify is True, then add some intensity change to the transformed image\n",
    "                            img_transformed = img_transformed + np.random.normal(1, 0.1, 1)\n",
    "\n",
    "                        # subplots: image base, transformed image, DVF\n",
    "                        # fig, ax = plt.subplots(1, 2, figsize=(10, 5))\n",
    "                        # ax[0].imshow(image_base, cmap='gray')\n",
    "                        # ax[0].set_title('Base Image')\n",
    "                        # ax[1].imshow(img_transformed, cmap='gray')\n",
    "                        # ax[1].set_title('Transformed Image (PyTorch)')\n",
    "\n",
    "\n",
    "                        transformed_image_path = os.path.join(output_dir, f\"img_{i}_transformed_{j}_{k}_{l}.png\")\n",
    "                        cv2.imwrite(transformed_image_path, img_transformed)\n",
    "                        # ---------------\n",
    "\n",
    "                        # # TODO: save heatmaps for other version of network\n",
    "                        points2, desc2, _ = superpoint(img_transformed_BW.astype(np.float32)/255)\n",
    "                        # print(points2.shape, points2)\n",
    "                        # print(desc1.shape, desc2.shape)\n",
    "                        matches = tracker.nn_match_two_way(desc1, desc2, nn_thresh=0.7)\n",
    "\n",
    "                        # # print(desc1.shape, desc2.shape)\n",
    "                        matches1 = points1[:2, matches[0, :].astype(int)]\n",
    "                        matches1_transformed = transform_points(matches1, M)\n",
    "                        # # matches1 = matches1.T[None, :, :]\n",
    "                        matches2 = points2[:2, matches[1, :].astype(int)]\n",
    "                        # print(torch.tensor(matches1.copy()).unsqueeze(-1).view(2, -1, 1).shape, DVF.shape)\n",
    "\n",
    "                        # transform the points using the displacement field\n",
    "                        # matches1_transformed = transform_points_from_DVF(torch.tensor(matches1.copy()).unsqueeze(-1).view(2, -1, 1), \n",
    "                        #         DVF, torch.tensor(img_transformed_BW).unsqueeze(0).unsqueeze(0))\n",
    "                        # print(f'Img {i}, diff: {matches1_transformed[:, 0] - matches2[:, 0]}')\n",
    "                        # points_list.append(matches1_transformed[:, 0] - matches2[:, 0])\n",
    "\n",
    "                        # --------------- move from here ^^up\n",
    "                        # ---------------\n",
    "\n",
    "                        # create a dataframe with the matches\n",
    "                        # print(matches1.shape, matches2.shape, matches1_transformed_DVF.shape)\n",
    "                        if len(matches1_transformed.shape) == 3:\n",
    "                            matches1_transformed = matches1_transformed.squeeze(-1)\n",
    "                        df = pd.DataFrame({'x1': matches1[0, :], 'y1': matches1[1, :],\n",
    "                                        'x2': matches2[0, :], 'y2': matches2[1, :],\n",
    "                                        'x2_': matches1_transformed[0, :], 'y2_': matches1_transformed[1, :]})\n",
    "                        save_name = os.path.join(output_dir, f\"img_{i}_{j}_{k}_{l}_keypoints.csv\")\n",
    "                        df.to_csv(save_name, index=False)\n",
    "\n",
    "                        with open(csv_file, 'a', newline='') as csvfile:\n",
    "                            writer = csv.writer(csvfile)\n",
    "                            writer.writerow([original_image_path, transformed_image_path, \n",
    "                                            M[0, 0], M[0, 1], M[0, 2], M[1, 0], M[1, 1], M[1, 2], \n",
    "                                            M[2, 0], M[2, 1], M[2, 2], img_path, save_name])\n",
    "\n",
    "    print(f\"\\nGenerated {(i+1)*(num_images)} images\")\n",
    "    # print mean absolute error of the points\n",
    "    # print('MAE point location error:', np.mean(np.abs(np.array(points_list))))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Define parameters\n",
    "# output_dir = \"Dataset/synth_eye_perspetive_easy\"  # Output directory\n",
    "# os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# # plot_dir = \"Dataset/synthetic_eye_dataset_train/plot\"\n",
    "# # os.makedirs(plot_dir, exist_ok=True)\n",
    "\n",
    "# # generate synthetic images for each source training image\n",
    "# generate_affine_transformed_images_multiple(image_list,\n",
    "#     'Dataset/synth_eye_perspetive_easy.csv', output_dir, num_images=2, modify=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Define parameters\n",
    "# image_size = (512, 512)  # Size of the images\n",
    "# output_dir = \"Dataset/synthetic_eye_dataset_train_multiple\"  # Output directory\n",
    "# os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# # plot_dir = \"Dataset/synthetic_eye_dataset_train_multiple/plot\"\n",
    "# # os.makedirs(plot_dir, exist_ok=True)\n",
    "\n",
    "\n",
    "# # generate synthetic images for each source training image\n",
    "# generate_affine_transformed_images_multiple(image_list,\n",
    "#       'dataset_eye_synth_train_multiple.csv', output_dir, num_images=2, modify=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "M: tensor([[-3.3878e-04, -3.1276e-03,  7.6813e-01],\n",
      "        [ 2.6558e-03, -6.2866e-04, -6.4029e-01],\n",
      "        [ 1.1282e-05, -1.3224e-05,  7.1675e-06]])\n",
      "Startpoints: [[0, 0], [256, 0], [256, 256], [0, 256]]\n",
      "Endpoints: [(10.616478352291859, 12.739014825497433), (236.4960178877051, 13.72877778264174), (243.2920786847212, 247.736385202523), (9.611912298597417, 238.17467705097587)]\n",
      "\n",
      "M: tensor([[ 3.6974e-03, -5.3447e-04,  1.3129e-01],\n",
      "        [ 3.9890e-03,  3.5490e-03, -9.9132e-01],\n",
      "        [ 1.7131e-05, -1.3144e-06, -1.1747e-05]])\n",
      "Startpoints: [[0, 0], [256, 0], [256, 256], [0, 256]]\n",
      "Endpoints: [(21.667294455512323, 2.8829330284871393), (246.35385521007012, 6.822270572262838), (233.11684699485897, 232.4200492491177), (15.876460954196489, 236.93331931739448)]\n",
      "\n",
      "M: tensor([[ 1.5610e-03, -2.1993e-03,  5.1200e-01],\n",
      "        [ 3.5096e-03,  1.1678e-03, -8.5897e-01],\n",
      "        [ 1.4370e-05, -9.1017e-06,  4.5896e-05]])\n",
      "Startpoints: [[0, 0], [256, 0], [256, 256], [0, 256]]\n",
      "Endpoints: [(14.270853089152315, 8.488313262258705), (244.45292286227811, 10.783537520178369), (250.0324621397945, 242.6876360579067), (22.346777331426555, 244.6720798765127)]\n",
      "\n",
      "M: tensor([[-8.2667e-04,  2.6894e-03, -6.2403e-01],\n",
      "        [-3.3117e-03, -2.8758e-04,  7.8139e-01],\n",
      "        [-1.4077e-05,  1.1850e-05, -1.3791e-05]])\n",
      "Startpoints: [[0, 0], [256, 0], [256, 256], [0, 256]]\n",
      "Endpoints: [(23.525916619094943, 18.752794328572982), (230.44629632273228, 18.17507985584471), (251.80571865035498, 239.7610196799737), (21.416276298159843, 233.6592403044778)]\n",
      "\n",
      "M: tensor([[ 1.7315e-03, -1.8968e-03,  4.7779e-01],\n",
      "        [ 3.6254e-03,  1.4857e-03, -8.7846e-01],\n",
      "        [ 1.4725e-05, -7.7034e-06, -1.6049e-05]])\n",
      "Startpoints: [[0, 0], [256, 0], [256, 256], [0, 256]]\n",
      "Endpoints: [(20.018012362481485, 10.905608077015067), (245.59273556686628, 13.252317384202298), (244.2961132105283, 241.58389174925983), (3.9222278949474885, 250.92184326343045)]\n",
      "\n",
      "M: tensor([[ 1.2043e-03, -2.4603e-03,  5.8900e-01],\n",
      "        [ 3.4262e-03,  7.8175e-04, -8.0812e-01],\n",
      "        [ 1.4072e-05, -9.8681e-06,  2.9512e-05]])\n",
      "Startpoints: [[0, 0], [256, 0], [256, 256], [0, 256]]\n",
      "Endpoints: [(7.136121240836329, 5.197617583512602), (246.15127427636722, 18.900607851806836), (241.9916123666274, 243.36922254449973), (16.31094553145345, 242.17749441866167)]\n",
      "\n",
      "M: tensor([[ 1.3784e-03,  3.4031e-03, -8.6641e-01],\n",
      "        [-2.1109e-03,  1.4357e-03,  4.9932e-01],\n",
      "        [-8.3449e-06,  1.3828e-05,  1.2547e-05]])\n",
      "Startpoints: [[0, 0], [256, 0], [256, 256], [0, 256]]\n",
      "Endpoints: [(10.09512074176957, 17.506288809863804), (241.0892564619859, 19.168843720997213), (252.5027841076743, 230.51473382534166), (1.3358165423374146, 243.53168601577954)]\n",
      "\n",
      "M: tensor([[-2.4085e-03, -3.7448e-03,  9.5558e-01],\n",
      "        [ 1.1952e-03, -2.3895e-03, -2.9470e-01],\n",
      "        [ 5.1402e-06, -1.5325e-05,  8.4043e-06]])\n",
      "Startpoints: [[0, 0], [256, 0], [256, 256], [0, 256]]\n",
      "Endpoints: [(6.66007984901992, 21.582206246978714), (255.03177950118035, 8.36435700205297), (238.38321215169194, 230.8921899100113), (0.7834130031150238, 231.25595822708615)]\n",
      "\n",
      "M: tensor([[-1.2374e-03, -3.7004e-03,  8.6798e-01],\n",
      "        [ 1.9859e-03, -1.5091e-03, -4.9658e-01],\n",
      "        [ 8.6972e-06, -1.4835e-05, -2.5265e-05]])\n",
      "Startpoints: [[0, 0], [256, 0], [256, 256], [0, 256]]\n",
      "Endpoints: [(11.314559651256179, 19.75650799156289), (250.06575491357955, 5.3935487552481165), (248.2598396033247, 234.52686868145634), (20.69920116746597, 230.69463493174246)]\n",
      "\n",
      "M: tensor([[-4.6425e-05, -3.2439e-03,  7.5325e-01],\n",
      "        [ 2.7992e-03, -5.7734e-04, -6.5772e-01],\n",
      "        [ 1.1931e-05, -1.3354e-05, -4.1313e-06]])\n",
      "Startpoints: [[0, 0], [256, 0], [256, 256], [0, 256]]\n",
      "Endpoints: [(18.849751821305002, 21.572282307373655), (242.58300939584484, 19.356371158647544), (242.81903928257276, 240.86900853334012), (22.579210638700445, 234.89790910920405)]\n",
      "\n",
      "M: tensor([[-2.3430e-03,  1.4879e-03, -3.6981e-01],\n",
      "        [-3.9198e-03, -2.1969e-03,  9.2909e-01],\n",
      "        [-1.5456e-05,  5.9279e-06, -5.7712e-05]])\n",
      "Startpoints: [[0, 0], [256, 0], [256, 256], [0, 256]]\n",
      "Endpoints: [(15.463723580985135, 6.139940281209348), (241.48339406711904, 18.497830694741538), (235.63372380835165, 255.13344860195633), (7.593410794602637, 251.10036470402298)]\n",
      "\n",
      "M: tensor([[ 2.2993e-04, -2.9610e-03,  6.9549e-01],\n",
      "        [ 3.0225e-03, -2.2894e-04, -7.1852e-01],\n",
      "        [ 1.1878e-05, -1.1934e-05,  3.1292e-07]])\n",
      "Startpoints: [[0, 0], [256, 0], [256, 256], [0, 256]]\n",
      "Endpoints: [(17.873864501672312, 17.298367497012226), (247.4498496323321, 18.03394860840459), (238.55549039688773, 248.8657134632841), (20.520450328004614, 253.76762609389067)]\n",
      "\n",
      "M: tensor([[-1.0708e-03, -3.2451e-03,  8.2355e-01],\n",
      "        [ 2.3160e-03, -1.1966e-03, -5.6722e-01],\n",
      "        [ 9.1286e-06, -1.3682e-05, -1.0211e-05]])\n",
      "Startpoints: [[0, 0], [256, 0], [256, 256], [0, 256]]\n",
      "Endpoints: [(17.551604454333447, 25.465886217625012), (236.19062214731795, 11.020583670942765), (239.06405423673453, 238.9739590884098), (2.045849419665771, 248.64485916093074)]\n",
      "\n",
      "M: tensor([[ 1.4926e-03,  3.7625e-03, -8.6717e-01],\n",
      "        [-2.0548e-03,  1.9601e-03,  4.9799e-01],\n",
      "        [-7.4573e-06,  1.5282e-05,  2.3715e-05]])\n",
      "Startpoints: [[0, 0], [256, 0], [256, 256], [0, 256]]\n",
      "Endpoints: [(11.692137036695712, 20.33852063835981), (255.68351905020185, 14.770277614956848), (236.0424403918093, 233.53990028452188), (24.360213407271516, 253.16042081294424)]\n",
      "\n",
      "M: tensor([[-5.9361e-04,  2.5141e-03, -6.0679e-01],\n",
      "        [-3.1613e-03, -4.2206e-04,  7.9485e-01],\n",
      "        [-1.2418e-05,  1.0464e-05,  1.1362e-05]])\n",
      "Startpoints: [[0, 0], [256, 0], [256, 256], [0, 256]]\n",
      "Endpoints: [(12.070163490957382, 9.223337826336047), (238.82575759186943, 4.472747015734706), (234.3946821695275, 251.02905110534726), (13.668148769663606, 254.44384142248595)]\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_177603/418320699.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_test['image_path'] = df_test['Directory'] + '/' + df_test['source']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "M: tensor([[ 2.9002e-03,  3.8958e-03, -9.5689e-01],\n",
      "        [-1.1395e-03,  2.9713e-03,  2.9039e-01],\n",
      "        [-3.8785e-06,  1.5842e-05,  7.6085e-05]])\n",
      "Startpoints: [[0, 0], [256, 0], [256, 256], [0, 256]]\n",
      "Endpoints: [(2.535931204558196, 8.334777041129868), (232.94739232259116, 1.14071972713287), (249.6866990182062, 241.8916518071859), (9.774435021692938, 254.04274369192305)]\n",
      "\n",
      "\n",
      "Generated 4 images\n",
      "\n",
      "Generated 4 images\n"
     ]
    }
   ],
   "source": [
    "# do the same for the test images\n",
    "df_test = df[df['training'] == 1]\n",
    "df_test['image_path'] = df_test['Directory'] + '/' + df_test['source']\n",
    "image_list_test = df_test['image_path'].unique()[:2]\n",
    "\n",
    "# Define parameters\n",
    "output_dir = \"Dataset/synth_eye_perspetive_test_\"  # Output directory\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# plot_dir = \"Dataset/synthetic_eye_dataset_test/plot\"\n",
    "# os.makedirs(plot_dir, exist_ok=True)\n",
    "\n",
    "# generate synthetic images for each source test image\n",
    "generate_affine_transformed_images_multiple(image_list_test, \n",
    "    'Dataset/synth_eye_perspetive_test_.csv', \n",
    "    output_dir, num_images=2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Define parameters\n",
    "# image_size = (512, 512)  # Size of the images\n",
    "# output_dir = \"Dataset/synthetic_eye_dataset_test_multiple\"  # Output directory\n",
    "# os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# # plot_dir = \"Dataset/synthetic_eye_dataset_test/plot\"\n",
    "# # os.makedirs(plot_dir, exist_ok=True)\n",
    "\n",
    "# # generate synthetic images for each source test image\n",
    "# generate_affine_transformed_images_multiple(image_list_test, 'dataset_eye_synth_test_scaling.csv', output_dir, num_images=1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spppt",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
