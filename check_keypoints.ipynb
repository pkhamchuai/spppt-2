{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check if the keypoints are duplicated"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\\\wsl.localhost\\Ubuntu\\home\\pkhamchuai\\codes\\spppt-2\\output\\SIFT\\SIFT_20100_0.0001_0_1_1_20240705-150230_BFMatcher_RANSAC_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda\n"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "import glob\n",
    "import numpy as np\n",
    "import os\n",
    "import time\n",
    "import cv2\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import math\n",
    "from skimage.metrics import structural_similarity as ssim\n",
    "from skimage.measure import ransac\n",
    "from skimage.transform import FundamentalMatrixTransform, AffineTransform\n",
    "# Suppress the specific warning\n",
    "import warnings\n",
    "import csv\n",
    "import sys\n",
    "from IPython.utils.capture import capture_output\n",
    "from datetime import datetime\n",
    "\n",
    "# Stub to warn about opencv version.\n",
    "if int(cv2.__version__[0]) < 3: # pragma: no cover\n",
    "  print('Warning: OpenCV 3 is not installed')\n",
    "\n",
    "# Jet colormap for visualization.\n",
    "myjet = np.array([[0.        , 0.        , 0.5       ],\n",
    "                  [0.        , 0.        , 0.99910873],\n",
    "                  [0.        , 0.37843137, 1.        ],\n",
    "                  [0.        , 0.83333333, 1.        ],\n",
    "                  [0.30044276, 1.        , 0.66729918],\n",
    "                  [0.66729918, 1.        , 0.30044276],\n",
    "                  [1.        , 0.90123457, 0.        ],\n",
    "                  [1.        , 0.48002905, 0.        ],\n",
    "                  [0.99910873, 0.07334786, 0.        ],\n",
    "                  [0.5       , 0.        , 0.        ]])\n",
    "\n",
    "from utils.SuperPoint import SuperPointFrontend, PointTracker, load_image\n",
    "from utils.datagen import datagen\n",
    "from utils.utils0 import *\n",
    "from utils.utils1 import *\n",
    "from utils.utils1 import ModelParams, print_summary, DL_affine_plot\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f'Device: {device}')\n",
    "\n",
    "image_size = 256\n",
    "\n",
    "def process_image(image):\n",
    "    # squeeze dimensions 0 and 1\n",
    "    image = image.squeeze(0).squeeze(0)\n",
    "    # convert to numpy array\n",
    "    image = image.cpu().numpy()\n",
    "    # normalize image to range 0 to 1\n",
    "    image = (image/np.max(image)).astype('float32')\n",
    "    return image\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run(model_params, method1='BFMatcher', method2='RANSAC', plot=1):\n",
    "    \n",
    "    test_dataset = datagen(model_params.dataset, False, model_params.sup)\n",
    "\n",
    "    # Create output directory\n",
    "    output_dir = f\"output/{args.model}_{model_params.get_model_code()}_{timestamp}_{method1}_{method2}_test\"\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    metrics = []\n",
    "    # create a csv file to store the metrics\n",
    "    csv_file = f\"{output_dir}/metrics.csv\"\n",
    "\n",
    "    num_failed = 0\n",
    "\n",
    "    testbar = tqdm(test_dataset, desc=f'Testing:')\n",
    "    for i, data in enumerate(testbar, 0):\n",
    "        if i != 90:\n",
    "            continue\n",
    "        elif i == 90:\n",
    "            # Get images and affine parameters\n",
    "            source_image, target_image, affine_params_true, points1, points2, points1_2_true = data\n",
    "            points1 = points1.squeeze(0).cpu().numpy()\n",
    "            points2 = points2.squeeze(0).cpu().numpy()\n",
    "                \n",
    "            # process images\n",
    "            source_image = process_image(source_image)\n",
    "            target_image = process_image(target_image)\n",
    "            # print(f\"source_image: {source_image.shape}\")\n",
    "            # print(f\"target_image: {target_image.shape}\")\n",
    "\n",
    "            # Extract keypoints and descriptors using SIFT\n",
    "            sift = cv2.SIFT_create()\n",
    "            kp1, desc1 = sift.detectAndCompute(source_image, None)\n",
    "            kp2, desc2 = sift.detectAndCompute(target_image, None)\n",
    "\n",
    "            # print(desc1.shape)\n",
    "            # print(desc2.shape)\n",
    "\n",
    "            # pad the smaller desc to the same size with zeros\n",
    "            # if desc1.shape[0] < desc2.shape[0]:\n",
    "            #     desc1 = np.pad(desc1, ((0, desc2.shape[0] - desc1.shape[0]), (0, 0)), mode='constant')\n",
    "            #     kp1 = kp1 + tuple([cv2.KeyPoint(x=kp1[0].pt[0], y=kp1[0].pt[1], size=0)])\n",
    "            # elif desc1.shape[0] > desc2.shape[0]:\n",
    "            #     desc2 = np.pad(desc2, ((0, desc1.shape[0] - desc2.shape[0]), (0, 0)), mode='constant')\n",
    "            #     kp2 = kp2 + tuple([cv2.KeyPoint(x=kp2[0].pt[0], y=kp2[0].pt[1], size=0)])\n",
    "\n",
    "            if method1 == 'BFMatcher':\n",
    "                # Match keypoints using nearest neighbor search\n",
    "                bf = cv2.BFMatcher()\n",
    "                matches = bf.knnMatch(desc1, desc2, k=2)\n",
    "\n",
    "                good = []\n",
    "                try:\n",
    "                    for m,n in matches:\n",
    "                        if m.distance < 0.75*n.distance:\n",
    "                            good.append([m])\n",
    "                except ValueError:\n",
    "                    good = []\n",
    "\n",
    "                # img3 = cv2.drawMatchesKnn(source_image, kp1, target_image, kp2,good,None,flags=cv2.DrawMatchesFlags_NOT_DRAW_SINGLE_POINTS)\n",
    "                # plt.imshow(img3), plt.show()\n",
    "\n",
    "                matches = np.array([m for m in matches])\n",
    "                matches1 = np.float32([kp1[m[0].queryIdx].pt for m in good]).reshape(-1, 2)\n",
    "                matches2 = np.float32([kp2[m[0].trainIdx].pt for m in good]).reshape(-1, 2)\n",
    "                # print(f\"matches1: {matches1}\")\n",
    "                # print(f\"matches2: {matches2}\")\n",
    "\n",
    "                # tracker = PointTracker(2, nn_thresh=0.7)\n",
    "                # matches = tracker.ransac(desc1, desc2, matches)\n",
    "\n",
    "                # print(f\"pair: {i+1}, matches: {matches.shape}\")\n",
    "\n",
    "            elif method1 == 'FLANN':\n",
    "                FLANN_INDEX_KDTREE = 1\n",
    "                index_params = dict(algorithm = FLANN_INDEX_KDTREE, trees = 5)\n",
    "                search_params = dict(checks = 50)\n",
    "                \n",
    "                flann = cv2.FlannBasedMatcher(index_params, search_params)\n",
    "                \n",
    "                matches = flann.knnMatch(desc1,desc2,k=2)\n",
    "\n",
    "                # Apply ratio test to filter out ambiguous matches\n",
    "                good_matches = []\n",
    "                for m, n in matches:\n",
    "                    if m.distance < 0.75 * n.distance:\n",
    "                        good_matches.append(m)\n",
    "\n",
    "                # Apply RANSAC to filter out outliers\n",
    "                matches1 = np.float32([kp1[m.queryIdx].pt for m in good_matches]).reshape(-1, 2)\n",
    "                matches2 = np.float32([kp2[m.trainIdx].pt for m in good_matches]).reshape(-1, 2)\n",
    "\n",
    "                # # Need to draw only good matches, so create a mask\n",
    "                # matchesMask = [[0,0] for i in range(len(matches))]\n",
    "                \n",
    "                # # ratio test as per Lowe's paper\n",
    "                # for i,(m,n) in enumerate(matches):\n",
    "                #     if m.distance < 0.*n.distance:\n",
    "                #         matchesMask[i]=[1,0]\n",
    "\n",
    "                # draw_params = dict(matchColor = (0,255,0),\n",
    "                #     singlePointColor = (255,0,0),\n",
    "                #     matchesMask = matchesMask,\n",
    "                #     flags = cv2.DrawMatchesFlags_DEFAULT)\n",
    "                \n",
    "                # img3 = cv2.drawMatchesKnn(source_image, kp1, target_image, kp2, matches, None, **draw_params)\n",
    "                # plt.imshow(img3), plt.show()\n",
    "            \n",
    "            try:\n",
    "                # M, mask = cv2.findHomography(matches1, matches2, cv2.RANSAC, 5.0)\n",
    "                # print(f\"M: {M}\")\n",
    "                # affine_transform1 = M[:2, :]\n",
    "                if method2 == 'RANSAC':\n",
    "                    affine_transform1, _ = cv2.estimateAffinePartial2D(matches1, matches2, method=cv2.RANSAC)\n",
    "                elif method2 == 'LMEDS':\n",
    "                    affine_transform1, _ = cv2.estimateAffinePartial2D(matches1, matches2, method=cv2.LMEDS)\n",
    "                points1_transformed = cv2.transform(points1[None, :, :], affine_transform1)\n",
    "                # print(f\"matches1_transformed: {matches1_transformed.shape}\")\n",
    "                try:\n",
    "                    points1_transformed = points1_transformed[0]\n",
    "                except TypeError:\n",
    "                    pass\n",
    "                # transform image 1 and 2 using the affine transform matrix\n",
    "                transformed_source_affine = cv2.warpAffine(source_image, affine_transform1, (256, 256))\n",
    "                text = \"success\"\n",
    "\n",
    "                if i < 100 and plot == 1:\n",
    "                    plot_ = 1\n",
    "                elif i < 100 and plot == 2:\n",
    "                    plot_ = 2\n",
    "                    # do the bitwise not operation to get the inverse of the image\n",
    "                    source_image = cv2.bitwise_not(source_image)\n",
    "                    target_image = cv2.bitwise_not(target_image)\n",
    "                    transformed_source_affine = cv2.bitwise_not(transformed_source_affine)\n",
    "                else:\n",
    "                    plot_ = 0\n",
    "\n",
    "                try:\n",
    "                    matches1_2 = cv2.transform(matches1[None, :, :], affine_transform1)[0]\n",
    "                    matches1, matches2, matches1_2 = matches1.T, matches2.T, matches1_2.T\n",
    "                except:\n",
    "                    matches1, matches2, matches1_2 = [], [], []\n",
    "                    text = \"failed\"\n",
    "                    plot_ = plot\n",
    "\n",
    "                # print(f\"matcches1: {matches1.shape}\")\n",
    "                # print(f\"matches2: {matches2.shape}\")\n",
    "                # print(f\"matches1_2: {matches1_2.shape}\")\n",
    "                results = DL_affine_plot(f\"test\", output_dir,\n",
    "                    f\"{i}\", text, source_image, target_image, \\\n",
    "                    transformed_source_affine, \\\n",
    "                    matches1, matches2, matches1_2, desc1, desc2,\n",
    "                    affine_params_true=affine_params_true,\n",
    "                    affine_params_predict=np.round(affine_transform1, 3), \n",
    "                    heatmap1=None, heatmap2=None, plot=plot_)\n",
    "\n",
    "            except cv2.error:\n",
    "                # print(f\"Error: {i}\")\n",
    "                # break\n",
    "                affine_transform1 = np.array([[[1, 0, 0], [0, 1, 0]]])\n",
    "                points1_transformed = points1\n",
    "                transformed_source_affine = source_image\n",
    "                text = \"failed\"\n",
    "                num_failed += 1\n",
    "                # continue\n",
    "\n",
    "                if i < 100 and plot == 1:\n",
    "                    plot_ = 1\n",
    "                elif i < 100 and plot == 2:\n",
    "                    plot_ = 2\n",
    "                    # do the bitwise not operation to get the inverse of the image\n",
    "                    source_image = cv2.bitwise_not(source_image)\n",
    "                    target_image = cv2.bitwise_not(target_image)\n",
    "                    transformed_source_affine = cv2.bitwise_not(transformed_source_affine)\n",
    "                else:\n",
    "                    plot_ = 0\n",
    "\n",
    "                try:\n",
    "                    matches1_2 = cv2.transform(matches1[None, :, :], affine_transform1)[0]\n",
    "                    matches1, matches2, matches1_2 = matches1.T, matches2.T, matches1_2.T\n",
    "                except:\n",
    "                    matches1, matches2, matches1_2 = [], [], []\n",
    "                    text = \"failed\"\n",
    "                    plot_ = plot\n",
    "\n",
    "                results = DL_affine_plot(f\"test\", output_dir,\n",
    "                    f\"{i}\", text, source_image, target_image, \\\n",
    "                    transformed_source_affine, \\\n",
    "                    matches1, matches2, matches1_2, desc1, desc2,\n",
    "                    affine_params_true=affine_params_true,\n",
    "                    affine_params_predict=np.round(affine_transform1, 3), \n",
    "                    heatmap1=None, heatmap2=None, plot=plot_)\n",
    "\n",
    "            try:\n",
    "                points1, points2, points1_transformed = points1.T, points2.T, points1_transformed.T\n",
    "            except:\n",
    "                points1, points2, points1_transformed = [], [], []\n",
    "\n",
    "            # print(f\"points1: {points1.shape}\")\n",
    "            # print(f\"points2: {points2.shape}\")\n",
    "            # print(f\"points1_transformed: {points1_transformed.shape}\")\n",
    "            \n",
    "            results = DL_affine_plot(f\"test\", output_dir,\n",
    "                    f\"{i}\", text, source_image, target_image, \\\n",
    "                    transformed_source_affine, \\\n",
    "                    points1, points2, points1_transformed, desc1, desc2, \n",
    "                    affine_params_true=affine_params_true,\n",
    "                    affine_params_predict=np.round(affine_transform1, 3), \n",
    "                    heatmap1=None, heatmap2=None, plot=False)\n",
    "\n",
    "            # calculate metrics\n",
    "            # matches1_transformed = results[0]\n",
    "            mse_before = results[1]\n",
    "            mse12 = results[2]\n",
    "            tre_before = results[3]\n",
    "            tre12 = results[4]\n",
    "            mse12_image_before = results[5]\n",
    "            mse12_image = results[6]\n",
    "            ssim12_image_before = results[7]\n",
    "            ssim12_image = results[8]\n",
    "\n",
    "    return source_image, target_image, transformed_source_affine, matches1, matches2, matches1_transformed\n",
    "            \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model name:  dataset2_sup1_image1_points0_loss_image0\n",
      "Model code:  21100_0.0001_0_1_1\n",
      "Model params:  {'dataset': 2, 'sup': 1, 'image': 1, 'points': 0, 'loss_image_case': 0, 'loss_image': MSELoss(), 'loss_affine': <utils.utils1.loss_affine object at 0x7f1acb35e9e0>, 'learning_rate': 0.0001, 'decay_rate': 0.96, 'start_epoch': 0, 'num_epochs': 1, 'batch_size': 1, 'model_name': 'dataset2_sup1_image1_points0_loss_image0'}\n",
      "\n",
      "Model name:  dataset2_sup1_image1_points0_loss_image0\n",
      "Model code:  21100_0.0001_0_1_1\n",
      "Dataset used:  Synthetic eye scaling\n",
      "Supervised or unsupervised model:  Supervised\n",
      "Loss image type:  Image used\n",
      "Points used:  Points not used\n",
      "Loss function case:  0\n",
      "Loss function for image:  MSELoss()\n",
      "Loss function for affine:  <utils.utils1.loss_affine object at 0x7f1acb35e9e0>\n",
      "Learning rate:  0.0001\n",
      "Decay rate:  0.96\n",
      "Start epoch:  0\n",
      "Number of epochs:  1\n",
      "Batch size:  1\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing::   0%|          | 0/100 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing::  90%|█████████ | 90/100 [00:00<00:00, 188.26it/s]\n"
     ]
    },
    {
     "ename": "error",
     "evalue": "OpenCV(4.6.0) /io/opencv/modules/features2d/src/sift.dispatch.cpp:477: error: (-5:Bad argument) image is empty or has incorrect depth (!=CV_8U) in function 'detectAndCompute'\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31merror\u001b[0m                                     Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[31], line 34\u001b[0m\n\u001b[1;32m     31\u001b[0m timestamp \u001b[38;5;241m=\u001b[39m datetime\u001b[38;5;241m.\u001b[39mnow()\u001b[38;5;241m.\u001b[39mstrftime(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mY\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mm\u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m-\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mH\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mM\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mS\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     33\u001b[0m \u001b[38;5;66;03m# run the function\u001b[39;00m\n\u001b[0;32m---> 34\u001b[0m source_image, target_image, transformed_source_affine, matches1, matches2, matches1_transformed \u001b[38;5;241m=\u001b[39m \u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_params\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmethod2\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mRANSAC\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mplot\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     36\u001b[0m \u001b[38;5;66;03m# plot the images\u001b[39;00m\n\u001b[1;32m     37\u001b[0m fig, ax \u001b[38;5;241m=\u001b[39m plt\u001b[38;5;241m.\u001b[39msubplots(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m3\u001b[39m, figsize\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m15\u001b[39m, \u001b[38;5;241m5\u001b[39m))\n",
      "Cell \u001b[0;32mIn[30], line 33\u001b[0m, in \u001b[0;36mrun\u001b[0;34m(model_params, method1, method2, plot)\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[38;5;66;03m# print(f\"source_image: {source_image.shape}\")\u001b[39;00m\n\u001b[1;32m     29\u001b[0m \u001b[38;5;66;03m# print(f\"target_image: {target_image.shape}\")\u001b[39;00m\n\u001b[1;32m     30\u001b[0m \n\u001b[1;32m     31\u001b[0m \u001b[38;5;66;03m# Extract keypoints and descriptors using SIFT\u001b[39;00m\n\u001b[1;32m     32\u001b[0m sift \u001b[38;5;241m=\u001b[39m cv2\u001b[38;5;241m.\u001b[39mSIFT_create()\n\u001b[0;32m---> 33\u001b[0m kp1, desc1 \u001b[38;5;241m=\u001b[39m \u001b[43msift\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdetectAndCompute\u001b[49m\u001b[43m(\u001b[49m\u001b[43msource_image\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m     34\u001b[0m kp2, desc2 \u001b[38;5;241m=\u001b[39m sift\u001b[38;5;241m.\u001b[39mdetectAndCompute(target_image, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m     36\u001b[0m \u001b[38;5;66;03m# print(desc1.shape)\u001b[39;00m\n\u001b[1;32m     37\u001b[0m \u001b[38;5;66;03m# print(desc2.shape)\u001b[39;00m\n\u001b[1;32m     38\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[38;5;66;03m#     desc2 = np.pad(desc2, ((0, desc1.shape[0] - desc2.shape[0]), (0, 0)), mode='constant')\u001b[39;00m\n\u001b[1;32m     45\u001b[0m \u001b[38;5;66;03m#     kp2 = kp2 + tuple([cv2.KeyPoint(x=kp2[0].pt[0], y=kp2[0].pt[1], size=0)])\u001b[39;00m\n",
      "\u001b[0;31merror\u001b[0m: OpenCV(4.6.0) /io/opencv/modules/features2d/src/sift.dispatch.cpp:477: error: (-5:Bad argument) image is empty or has incorrect depth (!=CV_8U) in function 'detectAndCompute'\n"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "import sys\n",
    "\n",
    "# Create a list of command-line arguments\n",
    "# sys.argv = ['python', 'run_SIFT.py', '--model', 'SIFT', \n",
    "#             '--sup', '0', '--dataset', str(i), '--plot', '1',\n",
    "#             '--method1', 'BFMatcher', '--method2', 'RANSAC',\n",
    "#             ]\n",
    "\n",
    "class Args:\n",
    "    def __init__(self):\n",
    "        self.dataset = 2\n",
    "        self.sup = 1\n",
    "        self.image = 1\n",
    "        self.loss_image = 0\n",
    "        self.num_epochs = 1\n",
    "        self.learning_rate = 1e-4\n",
    "        self.decay_rate = 0.96\n",
    "        self.model = 'SIFT'\n",
    "        self.model_path = None\n",
    "        self.plot = 1\n",
    "        self.method = 'RANSAC'\n",
    "\n",
    "args = Args()\n",
    "\n",
    "model_params = ModelParams(dataset=args.dataset, sup=args.sup, image=args.image,\n",
    "                           loss_image=args.loss_image, num_epochs=args.num_epochs,\n",
    "                           learning_rate=args.learning_rate, decay_rate=args.decay_rate)\n",
    "model_params.print_explanation()\n",
    "\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "\n",
    "# run the function\n",
    "source_image, target_image, transformed_source_affine, matches1, matches2, matches1_transformed = run(model_params, method2='RANSAC', plot=1)\n",
    "\n",
    "# plot the images\n",
    "fig, ax = plt.subplots(1, 3, figsize=(15, 5))\n",
    "ax[0].imshow(source_image, cmap='gray')\n",
    "ax[0].scatter(matches1[0], matches1[1], c='r', s=5)\n",
    "ax[0].set_title('Source Image')\n",
    "ax[1].imshow(target_image, cmap='gray')\n",
    "ax[1].scatter(matches2[0], matches2[1], c='r', s=5)\n",
    "ax[1].set_title('Target Image')\n",
    "ax[2].imshow(transformed_source_affine, cmap='gray')\n",
    "ax[2].scatter(matches1_transformed[0], matches1_transformed[1], c='r', s=5)\n",
    "ax[2].set_title('Transformed Source Image')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "108"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(matches1[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
